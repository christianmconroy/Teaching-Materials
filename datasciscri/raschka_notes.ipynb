{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2 \n",
    "# Sebastian Raschka and Vahid Mirjalili"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch. 1 Intro\n",
    "\n",
    "- “Unsupervised dimensionality reduction is a commonly used approach in feature preprocessing to remove noise from data, which can also degrade the predictive performance of certain algorithms, and compress the data onto a smaller dimensional subspace while retaining most of the relevant information.”\n",
    "\n",
    "#### Preprocessing \n",
    "- “Some of the selected features may be highly correlated and therefore redundant to a certain degree. In those cases, dimensionality reduction techniques are useful for compressing the features onto a lower dimensional subspace. Reducing the dimensionality of our feature space has the advantage that less storage space is required, and the learning algorithm can run much faster”\n",
    "    - “In certain cases, dimensionality reduction can also improve the predictive performance of a model if the dataset contains a large number of irrelevant features (or noise); that is, if the dataset has a low signal-to-noise ratio”\n",
    "- “Finally, we also cannot expect that the default parameters of the different learning algorithms provided by software libraries are optimal for our specific problem task. Therefore, we will make frequent use of hyperparameter optimization techniques that help us to fine-tune the performance of our model in later chapters.”\n",
    "\n",
    "### Ch. 2: Training Simple Machine Learning Algorithms for Classification\n",
    "\n",
    "- We will start by implementing a perceptron step by step in Python and training it to classify different flower species in the Iris dataset.\n",
    "- Thus, Rosenblatt's initial perceptron rule is fairly simple, and the perceptron algorithm can be summarized by the following steps:\n",
    "    - Initialize the weights to 0 or small random numbers.\n",
    "    - For each training example, x(i) :\n",
    "        - Compute the output value, y hat.\n",
    "        - Update the weights.”\n",
    "\n",
    "- It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small\n",
    "    - “The preceding diagram illustrates how the perceptron receives the inputs of an example, x, and combines them with the weights, w, to compute the net input. The net input is then passed on to the threshold function, which generates a binary output of –1 or +1—the predicted class label of the example. During the learning phase, this output is used to calculate the error of the prediction and update the weights.”\n",
    "    - “It is important to keep in mind that we don't initialize the weights to zero because the learning rate,  (eta), only has an effect on the classification outcome if the weights are initialized to non-zero values.” “If all the weights are initialized to zero, the learning rate parameter, eta, affects only the scale of the weight vector, not the direction.”\n",
    "    - “After the weights have been initialized, the fit method loops over all individual examples in the training dataset and updates the weights according to the perceptron learning rule”\n",
    "    - “Rosenblatt proved mathematically that the perceptron learning rule converges if the two classes can be separated by a linear hyperplane. However, if the classes cannot be separated perfectly by such a linear decision boundary, the weights will never stop updating unless we set a maximum number of epochs”\n",
    "\n",
    "- “The key difference between the Adaline rule (also known as the Widrow-Hoff rule) and Rosenblatt's perceptron is that the weights are updated based on a linear activation function rather than a unit step function like in the perceptron.”\n",
    "    - “While the linear activation function is used for learning the weights, we still use a threshold function to make the final prediction”\n",
    "    - “As the illustration indicates, the Adaline algorithm compares the true class labels with the linear activation function's continuous valued output to compute the model error and update the weights. In contrast, the perceptron compares the true class labels to the predicted class labels.”\n",
    "\n",
    "- One of the key ingredients of supervised machine learning algorithms is a defined objective function that is to be optimized during the learning process\n",
    "    - “This objective function is often a cost function that we want to minimize”\n",
    "    - “In the case of Adaline, we can define the cost function, J, to learn the weights as the sum of squared errors (SSE) between the calculated outcome and the true class label”\n",
    "    - “Another nice property of this cost function is that it is convex; thus, we can use a very simple yet powerful optimization algorithm called gradient descent to find the weights that minimize our cost function to classify the examples in the Iris dataset”\n",
    "    - “As illustrated in the following figure, we can describe the main idea behind gradient descent as climbing down a hill until a local or global cost minimum is reached. In each iteration, we take a step in the opposite direction of the gradient, where the step size is determined by the value of the learning rate, as well as the slope of the gradient”\n",
    "    - “To compute the gradient of the cost function, we need to compute the partial derivative of the cost function with respect to each weight \n",
    "    - “the weight update is calculated based on all examples in the training dataset (instead of updating the weights incrementally after each training example), which is why this approach is also referred to as batch gradient descent.”\n",
    "    - “Instead of updating the weights after evaluating each individual training example, as in the perceptron, we calculate the gradient based on the whole training dataset”\n",
    "    \n",
    "- “what could happen if we choose a learning rate that is too large”\n",
    "    - “Instead of minimizing the cost function, the error becomes larger in every epoch, because we overshoot the global minimum.”\n",
    "    - “On the other hand, we can see that the cost decreases on the right plot, but the chosen learning rate, , is so small that the algorithm would require a very large number of epochs to converge to the global cost minimum”\n",
    "    \n",
    "- “Gradient descent is one of the many algorithms that benefit from feature scaling”\n",
    "    - “standardization, which gives our data the properties of a standard normal distribution: zero-mean and unit variance. \n",
    "    - This normalization procedure helps gradient descent learning to converge more quickly; however, it does not make the original dataset normally distributed. \n",
    "    - Standardization shifts the mean of each feature so that it is centered at zero and each feature has a standard deviation of 1 (unit variance) ”\n",
    "    - “One of the reasons why standardization helps with gradient descent learning is that the optimizer has to go through fewer steps to find a good or optimal solution (the global cost minimum)”\n",
    "\n",
    "\n",
    "- “Large-scale machine learning and stochastic gradient descent”\n",
    "    - “In the previous section, we learned how to minimize a cost function by taking a step in the opposite direction of a cost gradient that is calculated from the whole training dataset; this is why this approach is sometimes also referred to as batch gradient descent.”\n",
    "    - Now imaginae that we have a very large dataset with millions of data points - “Running batch gradient descent can be computationally quite costly in such scenarios, since we need to reevaluate the whole training dataset each time that we take one step toward the global minimum.”\n",
    "    - “A popular alternative to the batch gradient descent algorithm is stochastic gradient descent (SGD), which is sometimes also called iterative or online gradient descent. Instead of updating the weights based on the sum of the accumulated errors over all training exam, “we update the weights incrementally for each training example”\n",
    "        - “Although SGD can be considered as an approximation of gradient descent, it typically reaches convergence much faster because of the more frequent weight updates”\n",
    "        - “Since each gradient is calculated based on a single training example, the error surface is noisier than in gradient descent, which can also have the advantage that SGD can escape shallow local minima more readily “if we are working with nonlinear cost functions”\n",
    "        - “To obtain satisfying results via SGD, it is important to present training data in a random order; also, we want to shuffle the training dataset for every epoch to prevent cycles.”\n",
    "    - “A compromise between batch gradient descent and SGD is so-called mini-batch learning”\n",
    "        - “Mini-batch learning can be understood as applying batch gradient descent to smaller subsets of the training data, for example, 32 training examples at a time. ”\n",
    "        - “The advantage over batch gradient descent is that convergence is reached faster via mini-batches because of the more frequent weight updates.\n",
    "        - Furthermore, mini-batch learning allows us to replace the for loop over the training examples in SGD with vectorized operations leveraging concepts from linear algebra (for example, implementing a weighted sum via a dot product), which can further improve the computational efficiency of our learning algorithm.”\n",
    "\n",
    "\n",
    "- “We will see that a logistic regression model is closely related to Adaline, with the only difference being its activation and cost function”\n",
    "\n",
    "### Ch. 3: A Tour of Machine Learning Classifiers Using scikit-learn\n",
    "\n",
    "The five main steps that are involved in training a supervised machine learning algorithm can be summarized as follows:\n",
    "1. Selecting features and collecting labeled training examples.\n",
    "2. Choosing a performance metric.\n",
    "3. Choosing a classifier and optimization algorithm.\n",
    "4. Evaluating the performance of the model.\n",
    "5. Tuning the algorithm.\n",
    "\n",
    "- So to put in context of neural networks, the difference between linear and logistic regression is one of activation functions: linear vs sigmoid\n",
    "\n",
    "- Training a logistic regression model with scikit-learn\n",
    "    - “If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.”\n",
    "        - “In the context of machine learning models, variance measures the consistency (or variability) of the model prediction for classifying a particular example if we retrain the model multiple times, for example, on different subsets of the training dataset.”\n",
    "        - “In contrast, bias measures how far off the predictions are from the correct values in general if we rebuild the model multiple times on different training datasets; bias is the measure of the systematic error that is not due to randomness.”\n",
    "    - “One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization. ”\n",
    "        - “Regularization is a very useful method for handling collinearity (high correlation among features), filtering out noise from data, and eventually preventing overfitting.”\n",
    "        - “The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values. \n",
    "        - The most common form of regularization is so-called L2 regularization (sometimes also called L2 shrinkage or weight decay)”\n",
    "        - “Regularization is another reason why feature scaling such as standardization is important. For regularization to work properly, we need to ensure that all our features are on comparable scales.”\n",
    "        - “The parameter, C, that is implemented for the LogisticRegression class in scikit-learn comes from a convention in support vector machines\n",
    "            - Consequently, decreasing the value of the inverse regularization parameter, C, means that we are increasing the regularization strength”\n",
    "\n",
    "- Maximum Margin Classification with Support Vector Machines (SVMs)\n",
    "    - “The margin is defined as the distance between the separating hyperplane (decision boundary) and the training examples that are closest to this hyperplane, which are the so-called support vectors”\n",
    "        - “The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error, whereas models with small margins are more prone to overfitting”\n",
    "        - Soft Margin Classification \n",
    "            - “The motivation for introducing the slack variable was that the linear constraints need to be relaxed for nonlinearly separable data to allow the convergence of the optimization in the presence of misclassifications, under appropriate cost penalization.”\n",
    "            - “Via the variable, C, we can then control the penalty for misclassification. Large values of C correspond to large error penalties, whereas we are less strict about misclassification errors if we choose smaller values for C. We can then use the C parameter to control the width of the margin and therefore tune the bias-variance tradeoff” [Large C is narrow hyperplane and small C is wider hyperplane]\n",
    "                - “This concept is related to regularization, which we discussed in the previous section in the context of regularized regression, where decreasing the value of C increases the bias and lowers the variance of the model”\n",
    "    - “In practical classification tasks, linear logistic regression and linear SVMs often yield very similar results. \n",
    "        - Logistic regression tries to maximize the conditional likelihoods of the training data, which makes it more prone to outliers than SVMs, which mostly care about the points that are closest to the decision boundary (support vectors)”\n",
    "            - “On the other hand, logistic regression has the advantage that it is a simpler model and can be implemented more easily. Furthermore, logistic regression models can be easily updated, which is attractive when working with streaming data.\n",
    "    - “Another reason why SVMs enjoy high popularity among machine learning practitioners is that they can be easily kernelized to solve nonlinear classification problems”\n",
    "        - “The basic idea behind kernel methods to deal with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function, , where the data becomes linearly separable”\n",
    "        - “Then, we could use the same mapping function, , to transform new, unseen data to classify it using the linear SVM model”\n",
    "        - “However, one problem with this mapping approach is that the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data. This is where the so-called kernel trick comes into play.”\n",
    "            - “One of the most widely used kernels is the radial basis function (RBF) kernel, which can simply be called the Gaussian kernel”\n",
    "            - “Roughly speaking, the term \"kernel\" can be interpreted as a similarity function between a pair of examples.”\n",
    "\n",
    "- Decision Tree Learning \n",
    "    - “Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG)”\n",
    "    - “ In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure”\n",
    "    - “we typically want to prune the tree by setting a limit for the maximal depth of the tree”\n",
    "    - “As we can see, the information gain is simply the difference between the impurity of the parent node and the sum of the child node impurities—the lower the impurities of the child nodes, “the larger the information gain”\n",
    "    - “The three impurity measures or splitting criteria that are commonly used in binary decision trees are Gini impurity (), entropy (), and the classification error (). ”\n",
    "    - “Decision trees can build complex decision boundaries by dividing the feature space into rectangles. However, we have to be careful since the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting.”\n",
    "    - “Although feature scaling may be desired for visualization purposes, note that feature scaling is not a requirement for decision tree algorithms.”\n",
    "\n",
    "- Combining multiple decision trees via random forests\n",
    "    - “The idea behind a random forest is to average multiple (deep) decision trees that individually suffer from high variance to build a more robust model that has a better generalization performance and is less susceptible to overfitting”\n",
    "    - “The random forest algorithm can be summarized in four simple steps:\n",
    "        - 1. Draw a random bootstrap sample of size n (randomly choose n examples from the training dataset with replacement).\n",
    "        - 2. Grow a decision tree from the bootstrap sample. At each node:\n",
    "            - Randomly select d features without replacement.\n",
    "            - Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.\n",
    "        - 3. Repeat the steps 1-2 k times.\n",
    "        - 4. Aggregate the prediction by each tree to assign the class label by majority vote.”\n",
    "    - “Although random forests don't offer the same level of interpretability as decision trees, a big advantage of random forests is that we don't have to worry so much about choosing good hyperparameter values”\n",
    "    - “The only parameter that we really need to care about in practice is the number of trees, k, (step 3) that we choose for the random forest. Typically, the larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost”\n",
    "    - “Via the sample size, n, of the bootstrap sample, we control the bias-variance tradeoff of the random forest”\n",
    "        - “shrinking the size of the bootstrap samples may increase the randomness of the random forest, and it can help to reduce the effect of overfitting”\n",
    "        - “However, smaller bootstrap samples typically result in a lower overall performance of the random forest, and a small gap between training and test performance, but a low test performance overall”\n",
    "        - “Conversely, increasing the size of the bootstrap sample may increase the degree of overfitting”\n",
    "        - “In most implementations, including the RandomForestClassifier implementation in scikit-learn, the size of the bootstrap sample is chosen to be equal to the number of training examples in the original training dataset, which usually provides a good bias-variance tradeoff”\n",
    "\n",
    "- “K-nearest neighbors – a lazy learning algorithm”\n",
    "    - “Machine learning algorithms can be grouped into parametric and nonparametric models”\n",
    "        - “Using parametric models, we estimate parameters from the training dataset to learn a function that can classify new data points without requiring the original training dataset anymore”\n",
    "            - “Typical examples of parametric models are the perceptron, logistic regression, and the linear SVM”\n",
    "            - “In contrast, nonparametric models can't be characterized by a fixed set of parameters, and the number of parameters grows with the training data. Two examples of nonparametric models that we have seen so far are the decision tree classifier/random forest and the kernel SVM.”\n",
    "                - “KNN belongs to a subcategory of nonparametric models that is described as instance-based learning”\n",
    "                - “Models based on instance-based learning are characterized by memorizing the training dataset, and lazy learning is a special case of instance-based learning that is associated with no (zero) cost during the learning process.”\n",
    "    - “The KNN algorithm itself is fairly straightforward and can be summarized by the following steps:\n",
    "        1. Choose the number of k and a distance metric.\n",
    "        2. Find the k-nearest neighbors of the data record that we want to classify.\n",
    "        3. Assign the class label by majority vote.”\n",
    "    - “The main advantage of such a memory-based approach is that the classifier immediately adapts as we collect new training data. However, the downside is that the computational complexity for classifying new examples grows linearly with the number of examples in the training dataset in the worst-case scenario—unless the dataset has very few dimensions (features) and the algorithm has been implemented using efficient data structures such as k-d trees”\n",
    "    - “in models where regularization is not applicable, such as decision trees and KNN, we can use feature selection and dimensionality reduction techniques to help us to avoid the curse of dimensionality”\n",
    "\n",
    "### Ch. 4: “Building Good Training Datasets – Data Preprocessing”\n",
    "- “Identifying missing values in tabular data”\n",
    "    - “Although scikit-learn was originally developed for working with NumPy arrays only, it can sometimes be more convenient to preprocess data using pandas' DataFrame. Nowadays, most scikit-learn functions support DataFrame objects as inputs, but since NumPy array handling is more mature in the scikit-learn API, it is recommended to use NumPy arrays when possible. Note that you can always access the underlying NumPy array of a DataFrame via the values attribute before you feed it into a scikit-learn estimator”\n",
    "\n",
    "- “The SimpleImputer class belongs to the so-called transformer classes in scikit-learn, which are used for data transformation. \n",
    "    - The two essential methods of those estimators are fit and transform. The fit method is used to learn the parameters from the training data, and the transform method uses those parameters to transform the data. Any data array that is to be transformed needs to have the same number of features as the data array that was used to fit the model.”\n",
    "\n",
    "- “The classifiers that we used in Chapter 3, A Tour of Machine Learning Classifiers Using scikit-learn, belong to the so-called estimators in scikit-learn, with an API that is conceptually very similar to the transformer class. \n",
    "    - Estimators have a predict method but can also have a transform method, as you will see later in this chapter. ”\n",
    "\n",
    "- Handling categorical data \n",
    "    - “Although most estimators for classification in scikit-learn convert class labels to integers internally, it is considered good practice to “provide class labels as integer arrays to avoid technical glitches\n",
    "    - One-hot encoding \n",
    "\n",
    "- Feature Scaling \n",
    "    - “Decision trees and random forests are two of the very few machine learning algorithms where we don't need to worry about feature scaling”\n",
    "    - “Now, there are two common approaches to bringing different features onto the same scale: normalization and standardization”\n",
    "        - “Most often, normalization refers to the rescaling of the features to a range of [0, 1], which is a special case of min-max scaling”\n",
    "        - “standardization can be more practical for many machine learning algorithms, especially for optimization algorithms such as gradient descent”\n",
    "            - “The reason is that many linear models, such as the logistic regression and SVM initialize the weights to 0 or small random values close to 0”\n",
    "            - “Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns have the same parameters as a standard normal distribution (zero mean and unit variance), which makes it easier to learn the weights”\n",
    "            - “Furthermore, standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values”\n",
    "            \n",
    "- Regularization \n",
    "    - “L2 regularization is one approach to reduce the complexity of a model by penalizing large individual weights”\n",
    "        - By “increasing the regularization strength via the regularization parameter, , we shrink the weights toward zero and decrease the dependence of our model on the training data”\n",
    "        - “For example, if we increase the regularization parameter towards infinity, the weight coefficients will become effectively zero”\n",
    "    - “Another approach to reduce the model complexity is the related L1 regularization”\n",
    "        - “In contrast to L2 regularization, L1 regularization usually yields sparse feature vectors and most feature weights will be zero”\n",
    "        - “Sparsity can be useful in practice if we have a high-dimensional dataset with many features that are irrelevant, especially in cases where we have more irrelevant dimensions than training examples”\n",
    "        - So L1 better for feature selection (LASSO)\n",
    "        \n",
    "- Sequential Feature Selection\n",
    "    - “There are two main categories of dimensionality reduction techniques: feature selection and feature extraction”\n",
    "    - “Via feature selection, we select a subset of the original features, whereas in feature extraction, we derive information from the feature set to construct a new feature subspace.”\n",
    "    - “Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k<d”\n",
    "        - “The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem, to improve computational efficiency, or to reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization”\n",
    "        - “A classic sequential feature selection algorithm is sequential backward selection (SBS), which aims to reduce the dimensionality of the initial feature subspace with a minimum decay in the performance of the classifier to improve upon computational efficiency”\n",
    "            - “The idea behind the SBS algorithm is quite simple: SBS sequentially removes features from the full feature subset until the new feature subspace contains the desired number of features”\n",
    "            - “The criterion calculated by the criterion function can simply be the difference in performance of the classifier before and after the removal of a particular feature”\n",
    "\n",
    "- “Assessing feature importance with random forests” (Add to Paper)\n",
    "    - “Using a random forest, we can measure the feature importance as the averaged impurity decrease computed from all decision trees in the forest, without making any assumptions about whether our data is linearly separable or not. ”\n",
    "    - “By executing the following code, we will now train a forest of 500 trees on the Wine dataset and rank the 13 features by their respective importance measures”\n",
    "    - “we don't need to use standardized or normalized features in tree-based models”\n",
    "    - “However, as far as interpretability is concerned, the random forest technique comes with an important gotcha that is worth mentioning. If two or more features are highly correlated, one feature may be ranked very highly while the information on the other feature(s) may not be fully captured.”\n",
    "        - “On the other hand, we don't need to be concerned about this problem if we are merely interested in the predictive performance of a model rather than the interpretation of feature importance values”\n",
    "\n",
    "### Ch. 5: “Compressing Data via Dimensionality Reduction”\n",
    "- “In this chapter, we will cover the following topics:\n",
    "    - Principal component analysis (PCA) for unsupervised data compression\n",
    "    - Linear discriminant analysis (LDA) as a supervised dimensionality reduction technique for maximizing class separability\n",
    "    - Nonlinear dimensionality reduction via kernel principal component analysis (KPCA)”\n",
    "- “The difference between feature selection and feature extraction is that while we maintain the original features when we use feature selection algorithms, such as sequential backward selection, we use “feature extraction to transform or project the data onto a new feature space”\n",
    "    - “In practice, feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm, but can also improve the predictive performance by reducing the curse of dimensionality—especially if we are working with non-regularized models”\n",
    "    \n",
    "- PCA\n",
    "    - “PCA helps us to identify patterns in data based on the correlation between features. “In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one”\n",
    "    - “The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other”\n",
    "    - “Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features.”\n",
    "    - “PCA is an unsupervised method, which means that information about the class labels is ignored. Whereas a random forest uses the class membership information to compute the node impurities, variance measures the spread of values along a feature axis.”\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "    - “Both PCA and LDA are linear transformation techniques that can be used to reduce the number of dimensions in a dataset; the former is an unsupervised algorithm, whereas the latter is supervised”\n",
    "    - “Before we dive into the code implementation, let's briefly summarize the main steps that are required to perform LDA:\n",
    "        1. Standardize the d-dimensional dataset (d is the number of features).\n",
    "        2. For each class, compute the d-dimensional mean vector.\n",
    "        3. Construct the between-class scatter matrix, S B, and the within-class scatter matrix, S W.\n",
    "        4. Compute the eigenvectors and corresponding eigenvalues of the matrix, SW-1 S B .\n",
    "        5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "        6. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a dxk-dimensional transformation matrix, W; the eigenvectors are the columns of this matrix.\n",
    "        7. Project the examples onto the new feature subspace using the transformation matrix, W.”\n",
    "        \n",
    "- “if we are dealing with nonlinear problems, which we may encounter rather frequently in real-world applications, linear transformation techniques for dimensionality reduction, such as PCA and LDA, may not be the best choice”\n",
    "    - “we will take a look at a kernelized version of PCA, or KPCA, which relates to the concepts of kernel SVM”\n",
    "    - “Using KPCA, we will learn how to transform data that is not linearly separable onto a new, lower-dimensional subspace that is suitable for linear classifiers”\n",
    "    - “In other words, we perform a nonlinear mapping via KPCA that transforms the data onto a higher-dimensional space. We then use standard PCA in this higher-dimensional space to project the data back onto a lower-dimensional space where the examples can be separated by a linear classifier (under the condition that the examples can be separated by density in the input space). \n",
    "        - However, one downside of this approach is that it is computationally very expensive, and this is where we use the kernel trick. Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space.”\n",
    "    - “In other words, what we obtain after KPCA are the examples already projected onto the respective components, rather than constructing a transformation matrix as in the standard PCA approach. \n",
    "        - Basically, the kernel function (or simply kernel) can be understood as a function that calculates a dot product between two vectors—a measure of similarity.”\n",
    "    - Most commonly used kernels: Polynomial, hyperbolic tangent, radial basis function (RBF)\n",
    "\n",
    "- Projecting Data Points \n",
    "    - “In the two previous example applications of KPCA, the half-moon shapes and the concentric circles, we projected a single dataset onto a new feature. In real applications, however, we may have more than one dataset that we want to transform, for example, training and test data, and typically also new examples we will collect after the model building and evaluation. In this section, you will learn how to project data points that were not part of the training dataset.”\n",
    "    - “As you will remember from the standard PCA approach at the beginning of this chapter, we project data by calculating the dot product between a transformation matrix and the input examples; the columns of the projection matrix are the top k eigenvectors (v) that we obtained from the covariance matrix”\n",
    "    - “However, it is worth noting that KPCA, in contrast to standard PCA, is a memory-based method, which means that we have to reuse the original training dataset each time to project new examples.”\n",
    "\n",
    "- “In this chapter, you learned about three different, fundamental dimensionality reduction techniques for feature extraction: standard PCA, LDA, and KPCA.\n",
    "    - Using PCA, we projected data onto a lower-dimensional subspace to maximize the variance along the orthogonal feature axes, while ignoring the class labels. \n",
    "    - LDA, in contrast to PCA, is a technique for supervised dimensionality reduction, which means that it considers class information in the training dataset to attempt to maximize the class-separability in a linear feature space.\n",
    "    - Lastly, you learned about a nonlinear feature extractor, KPCA. Using the kernel trick and a temporary projection into a higher-dimensional feature space, you were ultimately able to compress datasets consisting of nonlinear features onto a lower-dimensional subspace where the classes became linearly separable.”\n",
    "    \n",
    "### Ch. 6: “Learning Best Practices for Model Evaluation and Hyperparameter Tuning”\n",
    "- “In this section, you will learn about an extremely handy tool, the Pipeline class in scikit-learn”\n",
    "    - “It allows us to fit a model including an arbitrary number of transformation steps and apply it to make predictions about new data.”\n",
    "\n",
    "- Hold Out and Cross-Fold Validation \n",
    "    - “However, if we reuse the same test dataset over and over again during model selection, it will become part of our training data and thus the model will be more likely to overfit”\n",
    "        - “A better way of using the holdout method for model selection is to separate the data into three parts: a training dataset, a validation dataset, and a test dataset.”\n",
    "        - “A disadvantage of the holdout method is that the performance estimate may be very sensitive to how we partition the training dataset into the training and validation subsets; the estimate will vary for different examples of the data”\n",
    "        \n",
    "    - “In k-fold cross-validation, we randomly split the training dataset into k folds without replacement, where k – 1 folds are used for the model training, and one fold is used for performance evaluation”\n",
    "        - “We then calculate the average performance of the models based on the different, independent test folds to obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method”\n",
    "        - “Typically, we use k-fold cross-validation for model tuning, that is, finding the optimal hyperparameter values that yield a satisfying generalization performance, which is estimated from evaluating the model performance on the test folds”\n",
    "        - “Since k-fold cross-validation is a resampling technique without replacement, the advantage of this approach is that each example will be used for training and validation (as part of a test fold) exactly once, which yields a lower-variance estimate of the model performance than the holdout method.”\n",
    "        - “A good standard value for k in k-fold cross-validation is 10, as empirical evidence shows”\n",
    "            - “However, if we are working with relatively small training sets, it can be useful to increase the number of folds. If we increase the value of k, more training data will be used in each iteration, which results in a lower pessimistic bias toward estimating the generalization performance by averaging the individual model estimates. However, large values of k will also increase the runtime of the cross-validation algorithm and yield estimates with higher variance, since the training folds will be more similar to each other”\n",
    "    - “A slight improvement over the standard k-fold cross-validation approach is stratified k-fold cross-validation, which can yield better bias and variance estimates, especially in cases of unequal class proportions”\n",
    "        - “In stratified cross-validation, the class label proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset”\n",
    "\n",
    "- “Debugging algorithms with learning and validation curves”\n",
    "    - “If a model is too complex for a given training dataset—there are too many degrees of freedom or parameters in this model—the model tends to overfit the training data and does not generalize well to unseen data”\n",
    "    - “By plotting the model training and validation accuracies as functions of the training dataset size, we can easily detect whether the model suffers from high variance or high bias, and whether the collection of more data could help to address this problem”\n",
    "\n",
    "- Fine Tuning and Grid Search \n",
    "    - “In machine learning, we have two types of parameters: those that are learned from the training data, for example, the weights in logistic regression, and the parameters of a learning algorithm that are optimized separately. The latter are the tuning parameters (or hyperparameters) of a model, for example, the regularization parameter in logistic regression or the depth parameter of a decision tree.”\n",
    "    - “Randomized search usually performs about as well as grid search but is much more cost- and time-effective. In particular, if we only sample 60 parameter combinations via randomized search, we already have a 95 percent probability of obtaining solutions within 5 percent of the optimal performance”\n",
    "    - “Using the RandomizedSearchCV class in scikit-learn, we can draw random parameter combinations from sampling distributions with a specified budget.”\n",
    "\n",
    "- “Using k-fold cross-validation in combination with grid search is a useful approach for fine-tuning the performance of a machine learning model by varying its hyperparameter values,”\n",
    "    - “If we want to select among different machine learning algorithms, though, another recommended approach is nested cross-validation”\n",
    "    - “In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model using k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance”\n",
    "    \n",
    "- “In the previous sections and chapters, we evaluated different machine learning models using the prediction accuracy, which is a useful metric with which to quantify the performance of a model in general. However, there are several other performance metrics that can be used to measure a model's relevance, such as precision, recall, and the F1 score.”\n",
    "- “A confusion matrix is simply a square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions of a classifier, as shown in the following figure”\n",
    "\n",
    "- “Both the prediction error (ERR) and accuracy (ACC) provide general information about how many examples are misclassified. The error can be understood as the sum of all false predictions divided by the number of total predictions, and the accuracy is calculated as the sum of correct predictions divided by the total number of predictions”\n",
    "    - ERR: FP + FN / FP + FN + TP + TN\n",
    "    - ACC: TP + TN / FP + FN + TP + TN = 1 - ERR\n",
    "    - FPR: FP/FP + TN\n",
    "    - TPR: TP/FN+TP\n",
    "    - PRE: TP/TP+FP\n",
    "    - REC = TPR = TP/FN+TP\n",
    "    - F1: 2(PRE x REC / PRE + REC)\n",
    "    \n",
    "- ROC\n",
    "    - “Receiver operating characteristic (ROC) graphs are useful tools to select models for classification based on their performance with respect to the FPR and TPR, which are computed by shifting the decision threshold of the classifier”\n",
    "    - “A perfect classifier would fall into the top-left corner of the graph with a TPR of 1 and an FPR of 0”\n",
    "\n",
    "- Dealing with Class Imbalance \n",
    "    - “One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class. ”\n",
    "    - “Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training examples”\n",
    "    - “Another technique for dealing with class imbalance is the generation of synthetic training examples, which is beyond the scope of this book. Probably the most widely used algorithm for synthetic training data generation is Synthetic Minority Over-sampling Technique (SMOTE)”\n",
    "    \n",
    "### Ch. 7: “Combining Different Models for Ensemble Learning”\n",
    "- “We will learn how to do the following:\n",
    "    1. Make predictions based on majority voting\n",
    "    2. Use bagging to reduce overfitting by drawing random combinations of the training dataset with repetition\n",
    "    3. Apply boosting to build powerful models from weak learners that learn from their mistakes”\n",
    "\n",
    "- Learning with Ensembles\n",
    "     - “The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone”\n",
    "     - “In this chapter, we will focus on the most popular ensemble methods that use the majority voting principle. Majority voting simply means that we select the class label that has been predicted by the majority of classifiers, that is, received more than 50 percent of the votes”\n",
    "     - “Using the training dataset, we start by training m different classifiers (). Depending on the technique, the ensemble can be built from different classification algorithms, for example, decision trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same base classification algorithm, fitting different subsets of the training dataset”\n",
    "\n",
    "- “Combining classifiers via majority vote”\n",
    "    - “The majority vote approach we implemented in this section is not to be confused with stacking. The stacking algorithm can be understood as a two-level ensemble, where the first level consists of individual classifiers that feed their predictions to the second level, where another classifier (typically logistic regression) is fit to the level-one classifier predictions to make the final predictions”\n",
    "\n",
    "- “Bagging – building an ensemble of classifiers from bootstrap samples”\n",
    "    - “Bagging is an ensemble learning technique that is closely related to the MajorityVoteClassifier that we implemented in the previous section. However, instead of using the same training dataset to fit the individual classifiers in the ensemble, we draw bootstrap samples (random samples with replacement) from the initial training dataset, which is why bagging is also known as bootstrap aggregating.”\n",
    "    - “In practice, more complex classification tasks and a dataset's high dimensionality can easily lead to overfitting in single decision trees, and this is where the bagging algorithm can really play to its strengths”\n",
    "    \n",
    "- “Leveraging weak learners via adaptive boosting”\n",
    "    - “In this last section about ensemble methods, we will discuss boosting, with a special focus on its most common implementation: Adaptive Boosting (AdaBoost).”\n",
    "    - “In boosting, the ensemble consists of very simple base classifiers, also often referred to as weak learners, which often only have a slight performance advantage over random guessing—a typical example of a weak learner is a decision tree stump”\n",
    "        - The key concept behind boosting is to focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples to improve the performance of the ensemble.”\n",
    "    - “In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement; the original boosting procedure can be summarized in the following four key steps:\n",
    "        1. Draw a random subset (sample) of training examples, d1, without replacement from the training dataset, D, to train a weak learner, C1.\n",
    "        2. Draw a second random training subset, d2, without replacement from the training dataset and add 50 percent of the examples that were previously misclassified to train a weak learner, C2.\n",
    "        3. Find the training examples, d3, in the training dataset, D, which C1 and C2 disagree upon, to train a third weak learner, C3.\n",
    "        4. Combine the weak learners C1, C2, and  C3 via majority voting.”\n",
    "    - “As discussed by Leo Breiman (Bias, variance, and arcing classifiers, L. Breiman, 1996), boosting can lead to a decrease in bias as well as variance compared to bagging models”\n",
    "        - “In practice, however, boosting algorithms such as AdaBoost are also known for their high variance, that is, the tendency to overfit the training data ”\n",
    "        - “In contrast to the original boosting procedure described here, AdaBoost uses the complete training dataset to train the weak learners, where the training examples are reweighted in each iteration to build a strong classifier that learns from the mistakes of the previous weak learners in the ensemble.”\n",
    "\n",
    "- “As concluding remarks about ensemble techniques, it is worth noting that ensemble learning increases the computational complexity compared to individual classifiers. In practice, we need to think carefully about whether we want to pay the price of increased computational costs for an often relatively modest improvement in predictive performance.”\n",
    "\n",
    "- “Another popular variant of boosting is gradient boosting”\n",
    "    - “AdaBoost and gradient boosting share the main overall concept: boosting weak learners (such as decision tree stumps) to strong learners. The two approaches, adaptive and gradient boosting, differ mainly with regard to how the weights are updated and how the (weak) classifiers are combined.”\n",
    "    - “GradientBoostingClassifier implementation in scikit-learn, scikit-learn now also includes a substantially faster version of gradient boosting in version 0.21, HistGradientBoostingClassifier, which is even faster than XGBoost”\n",
    "\n",
    "### Ch. 8: Applying Machine Learning to Sentiment Analysis\n",
    "- “The topics that we will cover in the following sections include the following:\n",
    "    - Cleaning and preparing text data\n",
    "    - Building feature vectors from text documents\n",
    "    - Training a machine learning model to classify positive and negative movie reviews\n",
    "    - Working with large text datasets using out-of-core learning\n",
    "    - Inferring topics from document collections for categorization”\n",
    "\n",
    "- Preprocessing \n",
    "    - “While stemming can create non-real words, such as 'thu' (from 'thus'), as shown in the previous example, a technique called lemmatization aims to obtain the canonical (grammatically correct) forms of individual words—the so-called lemmas.”\n",
    "    - “However, lemmatization is computationally more difficult and expensive compared to stemming and, in practice, it has been observed that stemming and lemmatization have little impact on the performance of text classification”\n",
    "    - “Since not everyone has access to supercomputer facilities, we will now apply a technique called out-of-core learning, which allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of a dataset.”\n",
    "    \n",
    "- “The word2vec algorithm is an unsupervised learning algorithm based on neural networks that attempts to automatically learn the relationship between words. The idea behind word2vec is to put words that have similar meanings into similar clusters, and via clever vector-spacing, the model can reproduce certain words using simple vector math, for example, king – man + woman = queen.”\n",
    "\n",
    "- “Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:\n",
    "    - A document-to-topic matrix\n",
    "    - A word-to-topic matrix”\n",
    "\n",
    "### Ch. 9: “Embedding a Machine Learning Model into a Web Application\n",
    "- “One option for model persistence is Python's in-built pickle module which allows us to serialize and deserialize Python object structures to compact bytecode so that we can save our classifier in its current state and reload it if we want to classify new, unlabeled examples, without needing the model to learn from the training data all over again”\n",
    "    - “Please note that unpickling data from an untrusted source can be a potential security risk, since the pickle module is not secured against malicious code. ”\n",
    "\n",
    "- “Our logistic regression model contains several NumPy arrays, such as the weight vector, and a more efficient way to serialize NumPy arrays is to use the alternative joblib library. ”\n",
    "- “In this section, we will set up a simple SQLite database to collect optional feedback about the predictions from users of the web application. We can use this feedback to update our classification model. SQLite is an open source SQL database engine that doesn't require a separate server to operate, which makes it ideal for smaller projects and simple web applications.”\n",
    "\n",
    "- “ PythonAnywhere web hosting service, which specializes in the hosting of Python web applications and makes it extremely simple and hassle-free”\n",
    "\n",
    "### Ch. 10: “Predicting Continuous Target Variables with Regression Analysis”\n",
    "- “Note that in contrast to common belief, training a linear regression model does not require that the explanatory or target variables are normally distributed. The normality assumption is only a requirement for certain statistics and hypothesis tests that are beyond the scope of this book”\n",
    "- “We can show that the covariance between a pair of standardized features is, in fact, equal to their linear correlation coefficient. ”\n",
    "- “Essentially, OLS regression can be understood as Adaline without the unit step function so that we obtain continuous target values instead of the class labels -1 and 1”\n",
    "\n",
    "- “As an alternative to throwing out outliers, we will look at a robust method of regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers.\n",
    "    - “We can summarize the iterative RANSAC algorithm as follows:\n",
    "        1. Select a random number of examples to be inliers and fit the model.\n",
    "        2. Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers.\n",
    "        3. Refit the model using all inliers.\n",
    "        4. Estimate the error of the fitted model versus the inliers.\n",
    "        5. Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations were reached; go back to step 1 otherwise.”\n",
    "\n",
    "- “Another useful quantitative measure of a model's performance is the so-called mean squared error (MSE), which is simply the averaged value of the SSE cost that we minimized to fit the linear regression model. The MSE is useful for comparing different regression models or for tuning their parameters via grid search and cross-validation, as it normalizes the SSE by the sample size”\n",
    "    - “Thus, it may sometimes be more useful to report the coefficient of determination (), which can be understood as a standardized version of the MSE, for better interpretability of the model's performance.”\n",
    "        - “If , the model fits the data perfectly with a corresponding MSE = 0.”\n",
    "        \n",
    "- “The most popular approaches to regularized linear regression are the so-called Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net.”\n",
    "    - “Ridge Regression is an L2 penalized model where we simply add the squared sum of the weights to our least-squares cost function”\n",
    "        - “By increasing the value of hyperparameter , we increase the regularization strength and thereby shrink the weights of our model.”\n",
    "    - “An alternative approach that can lead to sparse models is LASSO. Depending on the regularization strength, certain weights can become zero, which also makes LASSO useful as a supervised feature selection technique”\n",
    "        - “Here, the L1 penalty for LASSO is defined as the sum of the absolute magnitudes of the model weights”\n",
    "        - “However, a limitation of LASSO is that it selects at most n features if m > n, where n is the number of training examples. This may be undesirable in certain applications of feature selection. In practice, however, this property of LASSO is often an advantage because it avoids saturated models. \n",
    "            - Saturation of a model occurs if the number of training examples is equal to the number of features, which is a form of overparameterization. As a consequence, a saturated model can always fit the training data perfectly but is merely a form of interpolation and thus is not expected to generalize well”\n",
    "    - “A compromise between Ridge Regression and LASSO is elastic net, which has an L1 penalty to generate sparsity and an L2 penalty such that it can be used for selecting more than n features if m > n”\n",
    "    \n",
    "- “In the previous sections, we assumed a linear relationship between explanatory and response variables. One way to account for the violation of linearity assumption is to use a polynomial regression model by adding polynomial terms”\n",
    "    - “Although we can use polynomial regression to model a nonlinear relationship, it is still considered a multiple linear regression model because of the linear regression coefficients, w”\n",
    "\n",
    "- Decision Tree Regression \n",
    "    - “An advantage of the decision tree algorithm is that it does not require any transformation of the features if we are dealing with nonlinear data, because decision trees analyze one feature at a time, rather than taking weighted combinations into account. (Likewise, normalizing or standardizing features is not required for decision trees.)”\n",
    "    - “When we used decision trees for classification, we defined entropy as a measure of impurity to determine which feature split maximizes the information gain (IG). We discussed Gini impurity and entropy as measures of impurity, which are both useful criteria for classification. To use a decision tree for regression, however, we need an impurity metric that is suitable for continuous variables, so we define the impurity measure of a node, t, as the MSE instead”\n",
    "        - “In the context of decision tree regression, the MSE is often referred to as within-node variance, which is why the splitting criterion is also better known as variance reduction.”\n",
    "        \n",
    "- Random Forest Regression \n",
    "    - “A random forest usually has a better generalization performance than an individual decision tree due to randomness, which helps to decrease the model's variance”\n",
    "    - “Other advantages of random forests are that they are less sensitive to outliers in the dataset and don't require much parameter tuning. The only parameter in random forests that we typically need to experiment with is the number of trees in the ensemble.”\n",
    "    - “We use the MSE criterion to grow the individual decision trees, and the predicted target variable is calculated as the average prediction over all decision trees.”\n",
    "\n",
    "### Ch. 11: “Working with Unlabeled Data – Clustering Analysis”\n",
    "- k-means\n",
    "    - “Prototype-based clustering means that each cluster is represented by a prototype, which is usually either the centroid (average) of similar points with continuous features, or the medoid (the most representative or the point that minimizes the distance to all other points that belong to a particular cluster) in the case of categorical features. \n",
    "    - While k-means is very good at identifying clusters with a spherical shape, one of the drawbacks of this clustering algorithm is that we have to specify the number of clusters, k, a priori”\n",
    "    - “ k-means algorithm, as summarized by the following four steps:\n",
    "        1. Randomly pick k centroids from the examples as initial cluster centers.\n",
    "        2. Assign each example to the nearest centroid, \n",
    "        3. Move the centroids to the center of the examples that were assigned to it.\n",
    "        4. Repeat steps 2 and 3 until the cluster assignments do not change or a user-defined tolerance or maximum number of iterations is reached.”\n",
    "    - “We can define similarity as the opposite of distance, and a commonly used distance for clustering examples with continuous features is the squared Euclidean distance between two points, x and y, in m-dimensional space”\n",
    "    - “Based on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia”\n",
    "    - “A problem with k-means is that one or more clusters can be empty. Note that this problem does not exist for k-medoids or fuzzy C-means, an algorithm that we will discuss later in this section.”\n",
    "        - “However, this problem is accounted for in the current k-means implementation in scikit-learn. If a cluster is empty, the algorithm will search for the example that is farthest away from the centroid of the empty cluster. Then it will reassign the centroid to be this farthest point.”\n",
    "    - “When we are applying k-means to real-world data using a Euclidean distance metric, we want to make sure that the features are measured on the same scale and apply z-score standardization or min-max scaling if necessary.”\n",
    "    - “The number of clusters to choose may not always be so obvious in real-world applications, especially if we are working with a higher dimensional dataset that cannot be visualized. \n",
    "    - The other properties of k-means are that clusters do not overlap and are not hierarchical, and we also assume that there is at least one item in each cluster. \n",
    "        - Later in this chapter, we will encounter different types of clustering algorithms, hierarchical and density-based clustering. Neither type of algorithm requires us to specify the number of clusters upfront or assume spherical structures in our dataset.\n",
    "\n",
    "- A smarter way of placing the initial cluster centroids using k-means++\n",
    "    - So far, we have discussed the classic k-means algorithm, which uses a random seed to place the initial centroids, which can sometimes result in bad clusterings or slow convergence if the initial centroids are chosen poorly[…]”\n",
    "       - “One way to address this issue is to run the k-means algorithm multiple times on a dataset and choose the best performing model in terms of the SSE.”\n",
    "       - “Another strategy is to place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means”\n",
    "    - “The initialization in k-means++ can be summarized as follows:\n",
    "        1. Initialize an empty set, M, to store the k centroids being selected.\n",
    "        2. Randomly choose the first centroid, , from the input examples and assign it to M.\n",
    "        3. For each example, , that is not in M, find the minimum squared distance, , to any of the centroids in M.\n",
    "        4. To randomly select the next centroid, , use a weighted probability distribution equal to .\n",
    "        5. Repeat steps 2 and 3 until k centroids are chosen.\n",
    "        6. Proceed with the classic k-means algorithm.\n",
    "\n",
    "- “Hard clustering describes a family of algorithms where each example in a dataset is assigned to exactly one cluster, as in the k-means and k-means++ algorithms that we discussed earlier in this chapter. \n",
    "    - In contrast, algorithms for soft clustering (sometimes also called fuzzy clustering) assign an example to one or more clusters. A popular example of soft clustering is the fuzzy C-means (FCM) algorithm (also called soft k-means or fuzzy k-means). ”\n",
    "    - “The FCM procedure is very similar to k-means. However, we replace the hard cluster assignment with probabilities for each point belonging to each cluster.”\n",
    "    - “As with the k-means algorithm, we can summarize the FCM algorithm in four key steps:\n",
    "        1. Specify the number of k centroids and randomly assign the cluster memberships for each point.\n",
    "        2. Compute the cluster centroids, .\n",
    "        3. Update the cluster memberships for each point.\n",
    "        4. Repeat steps 2 and 3 until the membership coefficients do not change or a user-defined tolerance or maximum number of iterations is reached.”\n",
    "    - “Just by looking at the equation to calculate the cluster memberships, we can say that each iteration in FCM is more expensive than an iteration in k-means. On the other hand, FCM typically requires fewer iterations overall to reach convergence.”\n",
    "\n",
    "- “Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the examples in the clusters are. To calculate the silhouette coefficient of a single example in our dataset, we can apply the following three steps:\n",
    "    1. Calculate the cluster cohesion, , as the average distance between an example, , and all other points in the same cluster.\n",
    "    2. Calculate the cluster separation, , from the next closest cluster as the average distance between the example, , and all examples in the nearest cluster.\n",
    "    3. Calculate the silhouette, , as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:”\n",
    "    - “The silhouette coefficient is bounded in the range –1 to 1. ”\n",
    "    \n",
    "- “In this section, we will take a look at an alternative approach to prototype-based clustering: hierarchical clustering. \n",
    "    - One advantage of the hierarchical clustering algorithm is that it allows us to plot dendrograms (visualizations of a binary hierarchical clustering), which can help with the interpretation of the results by creating meaningful taxonomies. \n",
    "    - “Another advantage of this hierarchical approach is that we do not need to specify the number of clusters upfront.”\n",
    "    - “The two main approaches to hierarchical clustering are agglomerative and divisive hierarchical clustering.”\n",
    "        - “In divisive hierarchical clustering, we start with one cluster that encompasses the complete dataset, and we iteratively split the cluster into smaller clusters until each cluster only contains one example. \n",
    "        - In this section, we will focus on agglomerative clustering, which takes the opposite approach. We start with each example as an individual cluster and merge the closest pairs of clusters until only one cluster remains.\n",
    "            - “The two standard algorithms for agglomerative hierarchical clustering are single linkage and complete linkage. \n",
    "                - Using single linkage, we compute the distances between the most similar members for each pair of clusters and merge the two clusters for which the distance between the most similar members is the smallest. \n",
    "                - The complete linkage approach is similar to single linkage but, instead of comparing the most similar members in each pair of clusters, we compare the most dissimilar members to perform the merge.”\n",
    "                - “Other commonly used algorithms for agglomerative hierarchical clustering include average linkage and Ward's linkage. \n",
    "                    - In average linkage, we merge the cluster pairs based on the minimum average distances between all group members in the two clusters. \n",
    "                    - In Ward's linkage, the two clusters that lead to the minimum increase of the total within-cluster SSE are merged.”\n",
    "            - “Hierarchical complete linkage clustering is an iterative procedure that can be summarized by the following steps:\n",
    "                1. Compute the distance matrix of all examples.\n",
    "                2. Represent each data point as a singleton cluster.\n",
    "                3. Merge the two closest clusters based on the distance between the most dissimilar (distant) members.\n",
    "                4. Update the similarity matrix.\n",
    "                5. Repeat steps 2-4 until one single cluster remains.\n",
    "\n",
    "- Locating regions of high density via DBSCAN - density-based spatial clustering of applications with noise (DBSCAN)\n",
    "    - “According to the DBSCAN algorithm, a special label is assigned to each example (data point) using the following criteria:\n",
    "        - A point is considered a core point if at least a specified number (MinPts) of neighboring points fall within the specified radius, .\n",
    "        - A border point is a point that has fewer neighbors than MinPts within ε, but lies within the  radius of a core point.\n",
    "        - All other points that are neither core nor border points are considered noise points.”\n",
    "    - “After labeling the points as core, border, or noise, the DBSCAN algorithm can be summarized in two simple steps:\n",
    "        - Form a separate cluster for each core point or connected group of core points. (Core points are connected if they are no farther away than .)\n",
    "        - Assign each border point to the cluster of its corresponding core point.”\n",
    "    - “One of the main advantages of using DBSCAN is that it does not assume that the clusters have a spherical shape as in k-means.\n",
    "        - Furthermore, DBSCAN is different from k-means and hierarchical clustering in that it doesn't necessarily assign each point to a cluster but is capable of removing noise points.\n",
    "        \n",
    "     - “However, we should also note some of the disadvantages of DBSCAN. With an increasing number of features in our dataset—\n",
    "         - Assuming a fixed number of training examples—the negative effect of the curse of dimensionality increases. This is especially a problem if we are using the Euclidean distance metric. However, the problem of the curse of dimensionality is not unique to DBSCAN: it also affects other clustering algorithms that use the Euclidean distance metric, for example, k-means and hierarchical clustering algorithms. \n",
    "         - In addition, we have two hyperparameters in DBSCAN (MinPts and E) that need to be optimized to yield good clustering results. Finding a good combination of MinPts and E can be problematic if the density differences in the dataset are relatively large.”\n",
    "\n",
    "- “In the context of the curse of dimensionality, it is thus common practice to apply dimensionality reduction techniques prior to performing clustering. Such dimensionality reduction techniques for unsupervised datasets include principal component analysis and radial basis function kernel principal component analysis”\n",
    "    - “Also, it is particularly common to compress datasets down to two-dimensional subspaces, which allows us to visualize the clusters and assigned labels using two-dimensional scatterplots, which are particularly helpful for evaluating the results.”\n",
    "    \n",
    "### “Implementing a Multilayer Artificial Neural Network from Scratch”\n",
    "- “Also, we learned about a certain trick to accelerate the model learning, the so-called stochastic gradient descent (SGD) optimization. SGD approximates the cost from a single training sample (online learning) or a small subset of training examples (mini-batch learning). \n",
    "    - We will make use of this concept later in this chapter when we implement and train a multilayer perceptron (MLP). ”\n",
    "    - “Apart from faster learning—due to the more frequent weight updates compared to gradient descent—its noisy nature is also regarded as beneficial when training multilayer NNs with nonlinear activation functions, which do not have a convex cost function”\n",
    "    - “The units in the hidden layer are fully connected to the input layer, and the output layer is fully connected to the hidden layer. If such a network has more than one hidden layer, we also call it a deep artificial NN”\n",
    "    - “However, the error gradients, which we will calculate later via backpropagation, will become increasingly small as more layers are added to a network. This vanishing gradient problem makes the model learning more challenging. \n",
    "        - Therefore, special algorithms have been developed to help train such DNN structures; this is known as deep learning”\n",
    "\n",
    "- “In this section, we will describe the process of forward propagation to calculate the output of an MLP model. To understand how it fits into the context of learning an MLP model, let's summarize the MLP learning procedure in three simple steps:\n",
    "    1. Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output.\n",
    "    2. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later.\n",
    "    3. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model.”\n",
    "    - “Finally, after we repeat these three steps for multiple epochs and learn the weights of the MLP, we use forward propagation to calculate the network output and apply a threshold function to obtain the predicted class labels in the one-hot representation, which we described in the previous section”\n",
    "    - “MLP is a typical example of a feedforward artificial NN. The term feedforward refers to the fact that each layer serves as the input to the next layer without loops, in contrast to recurrent NNs”\n",
    "    \n",
    "- “In NN training, it is really useful to compare training and validation accuracy, which helps us judge whether the network model performs well, given the architecture and hyperparameters”\n",
    "    - “For example, if we observe a low training and validation accuracy, there is likely an issue with the training dataset, or the hyperparameters settings are not ideal”\n",
    "    - “A relatively large gap between the training and the validation accuracy indicated that the model is likely overfitting the training dataset so that we want to reduce the number of parameters in the model or increase the regularization strength”\n",
    "    - “If both the training and validation accuracies are high, the model is likely to generalize well to new data, for example, the test dataset, which we use for the final model evaluation”\n",
    "    - “In general, training (deep) NNs is relatively expensive compared with the other models we've discussed so far. Thus, we want to stop it early in certain circumstances and start over with different hyperparameter settings”\n",
    "        - “On the other hand, if we find that it increasingly tends to overfit the training data (noticeable by an increasing gap between training and validation dataset performance), we may want to stop the training early as well”\n",
    "        \n",
    "- For regularization, “Remember that our goal is to minimize the cost function J(W); thus, we need to calculate the partial derivative of the parameters W with respect to each weight for every layer in the network”\n",
    "    - “In the next section, we will talk about the backpropagation algorithm, which allows us to calculate those partial derivatives to minimize the cost function.”\n",
    "\n",
    "- Backpropagation \n",
    "    -“In essence, we can think of backpropagation as a very computationally efficient approach to compute the partial derivatives of a complex cost function in multilayer NNs”\n",
    "    - “Here, our goal is to use those derivatives to learn the weight coefficients for parameterizing such a multilayer artificial NN”\n",
    "    - “Automatic differentiation comes with two modes, the forward and reverse modes; backpropagation is simply a special case of reverse-mode automatic differentiation. \n",
    "        - The key point is that applying the chain rule in the forward mode could be quite expensive since we would have to multiply large matrices for each layer (Jacobians) that we would eventually multiply by a vector to obtain the output.”\n",
    "        - “The trick of reverse mode is that we start from right to left: we multiply a matrix by a vector, which yields another vector that is multiplied by the next matrix and so on”\n",
    "        - “Matrix-vector multiplication is computationally much cheaper than matrix-matrix multiplication, which is why backpropagation is one of the most popular algorithms used in NN training”\n",
    "    - “In a previous section, we saw how to calculate the cost as the difference between the activation of the last layer and the target class label.”\n",
    "    - “Now, we will see how the backpropagation algorithm works to update the weights in our MLP model”\n",
    "    - “As we recall from the beginning of this chapter, we first need to apply forward propagation in order to obtain the activation of the output layer”\n",
    "    - “Concisely, we just forward-propagate the input features through the connection in the network”\n",
    "        - “In backpropagation, we propagate the error from right to left”\n",
    "\n",
    "- “You might be wondering why we did not use regular gradient descent but instead used mini-batch learning to train our NN for the handwritten digit classification. \n",
    "     - You may recall our discussion on SGD that we used to implement online learning. In online learning, we compute the gradient based on a single training example (k = 1) at a time to perform the weight update. Although this is a stochastic approach, it often leads to very accurate solutions with a much faster convergence than regular gradient descent. \n",
    "     - Mini-batch learning is a special form of SGD where we compute the gradient based on a subset k of the n training examples with 1 < k < n. Mini-batch learning has the advantage over online learning that we can make use of our vectorized implementations to improve computational efficiency. \n",
    "     - However, we can update the weights much faster than in regular gradient descent. Intuitively, you can think of mini-batch learning as predicting the voter turnout of a presidential election from a poll by asking only a representative subset of the population rather than asking the entire population (which would be equal to running the actual election).”\n",
    "\n",
    "- “By increasing the learning rate, we can more readily escape such local minima. On the other hand, we also increase the chance of overshooting the global optimum if the learning rate is too large.”\n",
    "\n",
    "### Ch. 13: “Parallelizing Neural Network Training with TensorFlow”\n",
    "- “TensorFlow is built around a computation graph composed of a set of nodes. \n",
    "    - Each node represents an operation that may have zero or more input or output. A tensor is created as a symbolic handle to refer to the input and output of these operations.”\n",
    "- “As mentioned at the beginning of this chapter, the Keras API is a wrapper around TensorFlow for building NN models”\n",
    "    - “In typical use cases, however, when the dataset is too large to fit into the computer memory, we will need to load the data from the main storage device (for example, the hard drive or solid-state drive) in chunks, that is, batch by batch (note the use of the term \"batch\" instead of \"mini-batch\" in this chapter to stay close to the TensorFlow terminology).”\n",
    "    - “In addition, we may need to construct a data-processing pipeline to apply certain transformations and preprocessing steps to our data, such as mean centering, scaling, or adding noise to augment the training procedure and to prevent overfitting.”\n",
    "\n",
    "- “Luckily, TensorFlow provides a special class for constructing efficient and convenient preprocessing pipelines. In this section, we will see an overview of different methods for constructing a TensorFlow Dataset, including dataset transformations and common preprocessing steps”\n",
    "    - “If the data already exists in the form of a tensor object, a Python list, or a NumPy array, we can easily create a dataset using the tf.data.Dataset.from_tensor_slices() function.”\n",
    "    \n",
    "- “The most commonly used approach for building an NN in TensorFlow is through tf.keras.Sequential(), which allows stacking layers to form a network.”\n",
    "    - “Furthermore, tf.keras allows us to define a model by subclassing tf.keras.Model. This gives us more control over the forward pass by defining the call() method for our model class to specify the forward pass explicitly. We will see examples of both of these approaches for building an NN model using the tf.keras API.”\n",
    "        - “Finally, as you will see in the following subsections, models built using the tf.keras API can be compiled and trained via the .compile() and .fit() methods”\n",
    "        - “TensorFlow instead provides already defined layers through tf.keras.layers that can be readily used as the building blocks of an NN model”\n",
    "        \n",
    "- “However, the logistic (sigmoid) activation function can be problematic if we have highly negative input, since the output of the sigmoid function will be close to zero in this case. If the sigmoid function returns output that is close to zero, the NN will learn very slowly, and it will be more likely to get trapped in the local minima during training. This is why people often prefer a hyperbolic tangent as an activation function in hidden layers”\n",
    "\n",
    "- “In certain contexts, it can be useful to compute meaningful class probabilities for multiclass predictions. In the next section, we will take a look at a generalization of the logistic function, the softmax function, which can help us with this task.”\n",
    "    - “The softmax function is a soft form of the argmax function; instead of giving a single class index, it provides the probability of each class.”\n",
    "\n",
    "- “Another sigmoidal function that is often used in the hidden layers of artificial NNs is the hyperbolic tangent (commonly known as tanh), which can be interpreted as a rescaled version of the logistic function” (TanH)\n",
    "    - “The advantage of the hyperbolic tangent over the logistic function is that it has a broader output spectrum ranging in the open interval (–1, 1), which can improve the convergence of the back-propagation algorithm ”\n",
    "    \n",
    "- “Rectified linear unit (ReLU) is another activation function that is often used in deep NNs”\n",
    "    - “To understand this problem, let’s assume that we initially have the net input zi = 20, which changes to z2 = 25. Computing the tanh activation, we get 1 and 1, which shows no change in the output (due to the asymptotic behavior of the tanh function and numerical errors).“This means that the derivative of activations with respect to the net input diminishes as z becomes large. As a result, learning weights during the training phase becomes very slow because the gradient terms may be very close to zero. ReLU activation addresses this issue”\n",
    "        - “ReLU is still a nonlinear function that is good for learning complex functions with NNs. Besides this, the derivative of ReLU, with respect to its input, is always 1 for positive input values. Therefore, it solves the problem of vanishing gradients, making it suitable for deep NNs”\n",
    "        - “We will use the ReLU activation function in the next chapter as an activation function for multilayer convolutional NNs.”\n",
    "\n",
    "### Ch. 14: Going Deeper – The Mechanics of TensorFlow\n",
    "- “TensorFlow performs its computations based on a directed acyclic graph (DAG)”\n",
    "    - “Each node resembles an operation, which applies a function to its input tensor or tensors and returns zero or more tensors as the output. TensorFlow builds this computation graph and uses it to compute the gradients accordingly.”\n",
    "    - “TensorFlow v1.x. Thus, TensorFlow v2 provides a tool called AutoGraph that can automatically transform Python code into TensorFlow's graph code for faster execution.”\n",
    "    - “You will recall that for NN models, initializing model parameters with random weights is necessary to break the symmetry during backpropagation—otherwise, a multilayer NN would be no more useful than a single-layer NN like logistic regression. When creating a TensorFlow Variable, we can also use a random initialization scheme.”\n",
    "\n",
    "- “In the early development of deep learning, it was observed that random uniform or random normal weight initialization could often result in a poor performance of the model during training.\n",
    "    - In 2010, Glorot and Bengio investigated the effect of initialization and proposed a novel, more robust initialization scheme to facilitate the training of deep networks. The general idea behind Xavier initialization is to roughly balance the variance of the gradients across different layers. Otherwise, some layers may get too much attention during training while the other layers lag behind.”\n",
    "    \n",
    "- “TensorFlow supports automatic differentiation, which can be thought of as an implementation of the chain rule for computing gradients of nested functions. When we define a series of operations that results in some output or even intermediate tensors, TensorFlow provides a context for calculating gradients of these computed tensors with respect to its dependent nodes in the computation graph.”\n",
    "\n",
    "- “Regarding the choices for optimization algorithms, SGD and Adam are the most widely used methods. The choice of loss function depends on the task; for example, you might use mean square error loss for a regression problem.”\n",
    "\n",
    "- “ In this section, we will switch gears and work with TensorFlow Estimators. The tf.estimator API encapsulates the underlying steps in machine learning tasks, such as training, prediction (inference), and evaluation. Estimators are more encapsulated but also more scalable when compared to the previous approaches that we have covered in this chapter.”\n",
    "    - “Also, the tf.estimator API adds support for running models on multiple platforms without requiring major code changes, which makes them more suitable for the so-called \"production phase\" in industry applications.”\n",
    "    - “One of the essential elements of Estimators is defining the feature columns as a mechanism for importing data into an Estimator-based model, which we will cover in the next section”\n",
    "    \n",
    "### Ch. 15: “Classifying Images with Deep Convolutional Neural Networks”\n",
    "- “Successfully extracting salient (relevant) features is key to the performance of any machine learning algorithm and traditional machine learning models rely on input features that may come from a domain expert or are based on computational feature extraction techniques”\n",
    "    - “Certain types of NNs, such as CNNs, are able to automatically learn the features from raw data that are most useful for a particular task. For this reason, it's common to consider CNN layers as feature extractors: the early layers (those right after the input layer) extract low-level features from raw data, and the later layers (often fully connected layers like in a multilayer perceptron (MLP)) use these features to predict a continuous target value or class label”\n",
    "    - “Certain types of multilayer NNs, and in particular, deep convolutional NNs, construct a so-called feature hierarchy by combining the low-level features in a layer-wise fashion to form high-level features. For example, if we're dealing with images, then low-level features, such as edges and blobs, are extracted from the earlier layers, which are combined together to form high-level features. These high-level features can form more complex shapes, such as the general contours of objects like buildings, cats, or dogs.”\n",
    "    - “This local patch of pixels is referred to as the local receptive field. \n",
    "    \n",
    "- CNNs will usually perform very well on image-related tasks, and that's largely due to two important ideas:\n",
    "    1. Sparse connectivity: A single element in the feature map is connected to only a small patch of pixels. \n",
    "    2. “Parameter-sharing: The same weights are used for different patches of the input image.”\n",
    "\n",
    "- “As a direct consequence of these two ideas, replacing a conventional, fully connected MLP with a convolution layer substantially decreases the number of weights (parameters) in the network and we will see an improvement in the ability to capture salient features. \n",
    "    - In the context of image data, it makes sense to assume that nearby pixels are typically more relevant to each other than pixels that are far away from each other.”\n",
    "\n",
    "- “Typically, CNNs are composed of several convolutional and subsampling layers that are followed by one or more fully connected layers at the end”\n",
    "    - Please note that subsampling layers, commonly known as pooling layers, do not have any learnable parameters; for instance, there are no weights or bias units in pooling layers. However, both the convolutional and fully connected layers have weights and biases that are optimized during training.\n",
    "\n",
    "- “To understand how convolution operations work, let's start with a convolution in one dimension, which is sometimes used for working with certain types of sequence data, such as text.”\n",
    "    - “A discrete convolution (or simply convolution) is a fundamental operation in a CNN. Therefore, it's important to understand how this operation works. In this section, we will cover the mathematical definition and discuss some of the naive algorithms to compute convolutions of one-dimensional tensors (vectors) and two-dimensional tensors (matrices).”\n",
    "    - “This process is called zero-padding or simply padding. Here, the number of zeros padded on each side is denoted by p\n",
    "    \n",
    "- Subsampling Layers \n",
    "    - “Subsampling is typically applied in two forms of pooling operations in CNNs: max-pooling and mean-pooling (also known as average-pooling).”\n",
    "    - “The pooling layer is usually denoted by pn1*n2. Here, the subscript determines the size of the neighborhood (the number of adjacent pixels in each dimension) where the max or mean operation is performed. We refer to such a neighborhood as the pooling size.\n",
    "    \n",
    "- “The advantage of pooling is twofold:\n",
    "    - Pooling (max-pooling) introduces a local invariance. This means that small changes in a local neighborhood do not change the result of max-pooling. Therefore, it helps with generating features that are more robust to noise in the input data.”\n",
    "    - “Pooling decreases the size of features, which results in higher computational efficiency. Furthermore, reducing the number of features may reduce the degree of overfitting as well.”\n",
    "\n",
    "- “Traditionally, pooling is assumed to be non-overlapping. Pooling is typically performed on non-overlapping neighborhoods, which can be done by setting the stride parameter equal to the pooling size. ”\n",
    "    - “While pooling is still an essential part of many CNN architectures, several CNN architectures have also been developed without using pooling layers. Instead of using pooling layers to reduce the feature size, researchers use convolutional layers with a stride of 2.”\n",
    "    - “In a sense, you can think of a convolutional layer with stride 2 as a pooling layer with learnable weights.”\n",
    "\n",
    "- “So far, you have learned about the basic building blocks of CNNs. The concepts illustrated in this chapter are not really more difficult than traditional multilayer NNs. We can say that the most important operation in a traditional NN is matrix multiplication.” “In a CNN, this operation is replaced by a convolution operation, as in Z = W * X + b, where X is a matrix representing the pixels in a  arrangement”\n",
    "    - “Furthermore, you will recall that subsampling is another building block of a CNN, which may appear in the form of pooling”\n",
    "    \n",
    "- “An input to a convolutional layer may contain one or more 2D arrays or matrices with dimensions N1 * N2 (for example, the image height and width in pixels). \n",
    "    - These N1 x N2 matrices are called channels. Conventional implementations of convolutional layers expect a rank-3 tensor representation as an input, for example a three-dimensional array, , where  is the number of input channels. \n",
    "    - For example, let's consider images as input to the first layer of a CNN. If the image is colored and uses the RGB color mode, then (for the red, green, and blue color channels in RGB). However, if the image is in grayscale, then we have , because there is only one channel with the grayscale pixel intensity values.\n",
    "\n",
    "- “Now that you are familiar with the structure of input data, the next question is, how can we incorporate multiple input channels in the convolution operation that we discussed in the previous sections? The answer is very simple: we perform the convolution operation for each channel separately and then add the results together using the matrix summation. \n",
    "    - “The final result, A, is a feature map. Usually, a convolutional layer of a CNN has more than one feature map. If we use multiple feature maps, the kernel tensor becomes four-dimensional: width * height * Cin * Cout\n",
    "        - Width and height is kernel size, Cin is the number of input channels, and Cout is the number of output feature maps\n",
    "\n",
    "- “Choosing the size of a network, whether we are dealing with a traditional (fully connected) NN or a CNN, has always been a challenging problem. For instance, the size of a weight matrix and the number of layers need to be tuned to achieve a reasonably good performance.”\n",
    "    - “One way to address this problem is to build a network with a relatively large capacity (in practice, we want to choose a capacity that is slightly larger than necessary) to do well on the training dataset. Then, to prevent overfitting, we can apply one or multiple regularization schemes to achieve a good generalization performance on new data, such as the held-out test dataset.”\n",
    "    - “While both L1 and L2 regularization can be used for NNs as well, with L2 being the more common choice of the two, there are other methods for regularizing NNs, such as dropout, which we discuss in this section.”\n",
    "    \n",
    "- “Dropout is usually applied to the hidden units of higher layers and works as follows: during the training phase of an NN, a fraction of the hidden units is randomly dropped at every iteration with probability”\n",
    "    - “When dropping a certain fraction of input neurons, the weights associated with the remaining neurons are rescaled to account for the missing (dropped) neurons.”\n",
    "    - “The effect of this random dropout is that the network is forced to learn a redundant representation of the data. Therefore, the network cannot rely on an activation of any set of hidden units, since they may be turned off at any time during training, and is forced to learn more general and robust patterns from the data.”\n",
    "    - “This random dropout can effectively prevent overfitting. ”\n",
    "    - “The following figure shows an example of applying dropout with probability p = 0.5 during the training phase, whereby half of the neurons will become inactive randomly (dropped units are selected randomly in each forward pass of training). However, during prediction, all neurons will contribute to computing the pre-activations of the next layer.”\n",
    "    - “As shown here, one important point to remember is that units may drop randomly during training only, whereas for the evaluation (inference) phase, all the hidden units must be active (for instance,  or ). To ensure that the overall activations are on the same scale during training and prediction, the activations of the active neurons have to be scaled appropriately (for example, by halving the activation if the dropout probability was set to p = 0.5).\n",
    "        - However, since it is inconvenient to always scale activations when making predictions, TensorFlow and other tools scale the activations during training (for example, by doubling the activations if the dropout probability was set to p = 0.5). This approach is commonly referred to as inverse dropout.\n",
    "        - “consider that in dropout, we have a different model for each mini-batch (due to setting the weights to zero randomly during each forward pass).\n",
    "        - “The restriction and aspect that distinguishes dropout from regular ensembling, however, is that we share the weights over these \"different models\", which can be seen as a form of regularization. Then, during \"inference\" (for instance, predicting the labels in the test dataset), we can average over all these different models that we sampled over during training. This is very expensive, though.”\n",
    "        - “Then, averaging the models, that is, computing the geometric mean of the class-membership probability that is returned by a model, i, can be computed”\n",
    "        - “Now, the trick behind dropout is that this geometric mean of the model ensembles (here, M models) can be approximated by scaling the predictions of the last (or final) model sampled during training by a factor of 1/(1 – p), which is much cheaper than computing the geometric mean explicitly using the previous equation. (In fact, the approximation is exactly equivalent to the true geometric mean if we consider linear models.)”\n",
    "\n",
    "- “In Chapter 13, Parallelizing Neural Network Training with TensorFlow, we saw different activation functions, such as ReLU, sigmoid, and tanh. \n",
    "    - Some of these activation functions, like ReLU, are mainly used in the intermediate (hidden) layers of an NN to add non-linearities to our model. But others, like sigmoid (for binary) and softmax (for multiclass), are added at the last (output) layer, which results in class-membership probabilities as the output of the model. If the sigmoid or softmax activations are not included at the output layer, then the model will compute the logits instead of the class-membership probabilities.\n",
    "\n",
    "- “Please note that computing the cross-entropy loss by providing the logits, and not the class-membership probabilities, is usually preferred due to numerical stability reasons. \n",
    "    - If we provide logits as inputs to the loss function and set from_logits=True, the respective TensorFlow function uses a more efficient implementation to compute the loss and derivative of the loss with respect to the weights. This is possible since certain mathematical terms cancel and thus don't have to be computed explicitly when providing logits as inputs.\n",
    "\n",
    "### Ch. 16: “Modeling Sequential Data Using Recurrent Neural Networks”\n",
    "- “In a standard feedforward network, information flows from the input to the hidden layer, and then from the hidden layer to the output layer. On the other hand, in an RNN, the hidden layer receives its input from both the input layer of the current time step and the hidden layer from the previous time step.”\n",
    "    - “The flow of information in adjacent time steps in the hidden layer allows the network to have a memory of past events. This flow of information is usually displayed as a loop, also known as a recurrent edge in graph notation, which is how this general RNN architecture got its name.”\n",
    "    - “As we know, each hidden unit in a standard NN receives only one input—the net preactivation associated with the input layer. In contrast, each hidden unit in an RNN receives two distinct sets of input—the preactivation from the input layer and the activation of the same hidden layer from the previous time step, t – 1.”\n",
    "        - “Similarly, in the case of a multilayer RNN, we can summarize the information flow as follows:\n",
    "            - layer = 1: Here, the hidden layer is represented as  and it receives its input from the data point, , and the hidden values in the same layer, but at the previous time step, .\n",
    "            - layer = 2: The second hidden layer, , receives its inputs from the outputs of the layer below at the current time step () and its own hidden values from the previous time step, ”\n",
    "    - “Since, in this case, each recurrent layer must receive a sequence as input, all the recurrent layers except the last one must return a sequence as output (that is, return_sequences=True). The behavior of the last recurrent layer depends on the type of problem.”\n",
    "\n",
    "- “The different weight matrices in a single-layer RNN are as follows:\n",
    "    - Wxh: The weight matrix between the input, , and the hidden layer, h'\n",
    "    - Whh: The weight matrix associated with the recurrent edge\n",
    "    - Who: The weight matrix between the hidden layer and output layer”\n",
    "\n",
    "- “So far, you have seen recurrent networks in which the hidden layer has the recurrent property. However, note that there is an alternative model in which the recurrent connection comes from the output layer. ”\n",
    "    - “In this case, the net activations from the output layer at the previous time step, , can be added in one of two ways:\n",
    "        - To the hidden layer at the current time step, (shown in the following figure as output-to-hidden recurrence)\n",
    "        - To the output layer at the current time step, (shown in the following figure as output-to-output recurrence)\n",
    "        \n",
    "- “Using gradient clipping, we specify a cut-off or threshold value for the gradients, and we assign this cut-off value to gradient values that exceed this value. \n",
    "    - In contrast, TBPTT simply limits the number of time steps that the signal can backpropagate after each forward pass. For example, even if the sequence has 100 elements or steps, we may only backpropagate the most recent 20 time steps.\n",
    "    - While both gradient clipping and TBPTT can solve the exploding gradient problem, the truncation limits the number of steps that the gradient can effectively flow back and properly update the weights. On the other hand, LSTM, designed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber, has been more successful in vanishing and exploding gradient problems while modeling long-range dependencies through the use of memory cells. Let's discuss LSTM in more detail.”\n",
    "\n",
    "- “The building block of an LSTM is a memory cell, which essentially represents or replaces the hidden layer of standard RNNs.”\n",
    "    - “In each memory cell, there is a recurrent edge that has the desirable weight, w = 1, as we discussed, to overcome the vanishing and exploding gradient problems. The values associated with this recurrent edge are collectively called the cell state. ”\n",
    "    \n",
    "- “In the next section, we will implement a many-to-many RNN for an application of language modeling”\n",
    "\n",
    "- Extracting Feature Embeddings \n",
    "    - “A more elegant approach is to map each word to a vector of a fixed size with real-valued elements (not necessarily integers). In contrast to the one-hot encoded vectors, we can use finite-sized vectors to represent an infinite number of real numbers. (In theory, we can extract infinite real numbers from a given interval, for example [–1, 1].)”\n",
    "    - “The advantages of embedding over one-hot encoding are as follows:\n",
    "        - A reduction in the dimensionality of the feature space to decrease the effect of the curse of dimensionality\n",
    "        - The extraction of salient features since the embedding layer in an NN can be optimized (or learned)”\n",
    "    - “Given a set of tokens of size n + 2 (n is the size of the token set, plus index 0 is reserved for the padding placeholder, and n + 1 is for the words not present in the token set), an embedding matrix of size (n + 2) * embedding_dim will be created where each row of this matrix represents numeric features associated with a token.”\n",
    "    - “The embedding matrix serves as the input layer to our NN models”\n",
    "    \n",
    "- “The Bidirectional wrapper makes two passes over each input sequence: a forward pass and a reverse or backward pass (note that this is not to be confused with the forward and backward passes in the context of backpropagation). The results of these forward and backward passes will be concatenated by default.”\n",
    "    - “We can also try other types of recurrent layers, such as SimpleRNN. However, as it turns out, a model built with regular recurrent layers won't be able to reach a good predictive performance (even on the training data). For example, if you try replacing the bidirectional LSTM layer in the previous code with a unidirectional SimpleRNN layer and train the model on full-length sequences, you may observe that the loss will not even decrease during training. The reason is that the sequences in this dataset are too long, so a model with a SimpleRNN layer cannot learn the long-term dependencies and may suffer from vanishing or exploding gradient problems.\n",
    "    \n",
    "- “However, a new architecture has recently emerged that has been shown to outperform the RNN-based seq2seq models in several NLP tasks. “It is called the Transformer architecture, capable of modeling global dependencies between input and output sequences, and was introduced in 2017 by Ashish Vaswani, et. al.”\n",
    "    - The Transformer architecture is based on a concept called attention, and more specifically, the self-attention mechanism.\n",
    "    - In this case, using the attention mechanism would mean that our model would be able to learn to focus on the parts of an input sequence that are more relevant to the sentiment.\n",
    "    - “Then, for a seq2seq task, the goal of self-attention is to model the dependencies of each element in the output sequence to the input elements”\n",
    "    - “In order to achieve this, attention mechanisms are composed of three stages.\n",
    "        - “Firstly, we derive importance weights based on the similarity between the current element and all other elements in the sequence.\n",
    "        - Secondly, we normalize the weights, which usually involves the use of the already familiar softmax function. \n",
    "        - Thirdly, we use these weights in combination with the corresponding sequence elements in order to compute the attention value.\n",
    "    - “More formally, the output of self-attention is the weighted sum of all input sequences.\n",
    "        - “Here, the weights, Wij, are computed based on the similarity between the current input element, x(i), and all other elements in the input sequence.”\n",
    "        - “After computing these similarity-based weights for the ith input and all inputs in the sequence (x(i) to x(T)), the \"raw\" weights (wio to wiT) are then normalized using the familiar softmax function,”\n",
    "        - “Notice that as a consequence of applying the softmax function, the weights will sum to 1 after this normalization”\n",
    "    - “To recap, let's summarize the three main steps behind the self-attention operation:\n",
    "        1. For a given input element, x(i), and each jth element in the range [0, T], compute the dot product, x(i)^Tx(j)\n",
    "        2. “Obtain the weight, Wij, by normalizing the dot products using the softmax function\n",
    "        3. “Compute the output, o(i), as the weighted sum over the entire input sequence\n",
    "    - “Now that you have been introduced to the basic concept behind self-attention, this subsection summarizes the more advanced self-attention mechanism that is used in the Transformer model”\n",
    "    - “To make the self-attention mechanism more flexible and amenable to model optimization, we will introduce three additional weight matrices that can be fit as model parameters during model training: Query, key, and value sequence\n",
    "    - “Another trick that greatly improves the discriminatory power of the self-attention mechanism is multi-head attention (MHA), which combines multiple self-attention operations together. In this case, each self-attention mechanism is called a head, which can be computed in parallel. \n",
    "    - Notice that in the Transformer architecture shown in the previous figure, we added two additional components that we haven't discussed yet. \n",
    "        - One of these components is the residual connection, which adds the output from a layer (or even a group of layers) to its input, that is, x + layer(x). \n",
    "            - The block consisting of a layer (or multiple layers) with such a residual connection is called a residual block. The Transformer block shown in the previous figure has two residual blocks.”\n",
    "    - “The other new component is layer normalization, which is denoted in the previous figure as \"Layer norm.\" \n",
    "        - “For now, you can think of layer normalization as a fancy or more advanced way of normalizing or scaling the NN inputs and activations in each layer.”\n",
    "\n",
    "### Ch. 17: Generative Adversarial Networks for Synthesizing New Data\n",
    "- Let's first look at the foundations of GAN models. The overall objective of a GAN is to synthesize new data that has the same distribution as its training dataset.\n",
    "- “Before we discuss how GANs work, we will first start with autoencoders, which can compress and decompress training data. \n",
    "    - While standard autoencoders cannot generate new data, understanding their function will help you to navigate GANs in the next section.”\n",
    "    - “Autoencoders are composed of two networks concatenated together: an encoder network and a decoder network.”\n",
    "        - “The encoder network receives a d-dimensional input feature vector associated with example x (that is, x E Rd) and encodes it into a p-dimensional vector, z (that is, Z E Rp). In other words, the role of the encoder is to learn how to model the function z = f(x) \n",
    "        - “The encoded vector, z, is also called the latent vector, or the latent feature representation. Typically, the dimensionality of the latent vector is less than that of the input examples; in other words, p < d. Hence, we can say that the encoder acts as a data compression function, xhat = g(z)\n",
    "\n",
    "- “Autoencoders are deterministic models, which means that after an autoencoder is trained, given an input, x, it will be able to reconstruct the input from its compressed version in a lower-dimensional space. ”\n",
    "    - “Therefore, it cannot generate new data beyond reconstructing its input through the transformation of the compressed representation.”\n",
    "    - “A generative model, on the other hand, can generate a new example, , from a random vector, z (corresponding to the latent representation). ”\n",
    "\n",
    "- “As we have shifted our attention from autoencoders to generative models, you may have noticed that the decoder component of an autoencoder has some similarities with a generative model. \n",
    "    - In particular, they both receive a latent vector, z, as input and return an output in the same space as x. (For the autoencoder,  is the reconstruction of an input, x, and for the generative model,  is a synthesized sample.)”\n",
    "    - “However, the major difference between the two is that we do not know the distribution of z in the autoencoder, while in a generative model, the distribution of z is fully characterizable.”\n",
    "    - “It is possible to generalize an autoencoder into a generative model, though. One approach is VAEs.\n",
    "        - In a VAE receiving an input example, x, the encoder network is modified in such a way that it computes two moments of the distribution of the latent vector: the mean, , and variance, . ”\n",
    "        - “During the training of a VAE, the network is forced to match these moments with those of a standard normal distribution (that is, zero mean and unit variance). \n",
    "        - Then, after the VAE model is trained, the encoder is discarded, and we can use the decoder network to generate new examples, , by feeding random z vectors from the \"learned\" Gaussian distribution.”\n",
    "\n",
    "- “Note that generative models are traditionally defined as algorithms that model data input distributions, p(x), or the joint distributions of the input data and associated targets, p(x, y). ”\n",
    "    - “In the context of deep learning, however, the term generative model is typically used to refer to models that generate realistic-looking data. This means that we can sample from input distributions, p(x), but we are not necessarily able to perform conditional inference.”\n",
    "    - A GAN model consists of an additional NN called discriminator (D), which is a classifier that learns to detect a synthesized image, , from a real image, x:”\n",
    "    - “In a GAN model, the two networks, generator and discriminator, are trained together. \n",
    "        - At first, after initializing the model weights, the generator creates images that do not look realistic. \n",
    "        - Similarly, the discriminator does a poor job of distinguishing between real images and images synthesized by the generator. But over time (that is, through training), both networks become better as they interact with each other. In fact, the two networks play an adversarial game, where the generator learns to improve its output to be able to fool the discriminator. At the same time, the discriminator becomes better at detecting the synthesized images.”\n",
    "\n",
    "- Loss functions \n",
    "    - “value function, which can be interpreted as a payoff: we want to maximize its value with respect to the discriminator (D), while minimizing its value with respect to the generator (G)”\n",
    "    - “One training step of a GAN model with such a value function requires two optimization steps: (1) maximizing the payoff for the discriminator and (2) minimizing the payoff for the generator.”\n",
    "    - “A practical way of training GANs is to alternate between these two optimization steps: (1) fix (freeze) the parameters of one network and optimize the weights of the other one, and (2) fix the second network and optimize the first one. This process should be repeated at each training iteration. ”\n",
    "    - “After optimizing the discriminator using the loss terms for real and fake samples, we then fix the discriminator and optimize the generator.”\n",
    "    - “In other words, even though the examples synthesized by the generator are fake and are therefore labeled 0, we can flip the labels by assigning label 1 to these examples, and minimize the binary cross-entropy loss with these new labels”\n",
    "        - “Given that the discriminator is a binary classifier (the class labels are 0 and 1 for fake and real images, respectively), we can use the binary cross-entropy loss function.”\n",
    "        \n",
    "- Google Colab \n",
    "    - “While the platform provides various different computing resources, such as CPUs, GPUs, and even tensor processing units (TPUs), it is important to highlight that the execution time is currently limited to 12 hours. Therefore, any notebook running longer than 12 hours will be interrupted.”\n",
    "\n",
    "- In the next section, we will implement a deep convolutional GAN (DCGAN), which uses convolutional layers for both the generator and the discriminator networks.\n",
    "    - “ In this article, the researchers proposed using convolutional layers for both the generator and discriminator networks. Starting from a random vector, z, the DCGAN first uses a fully connected layer to project z into a new vector with a proper size so that it can be reshaped into a spatial convolution representation (h x w x c), which is smaller than the output image size. Then, a series of convolutional layers, known as transposed convolution, are used to upsample the feature maps to the desired output image size.\n",
    "    - “While a convolution operation is usually used to downsample the feature space (for example, by setting the stride to 2, or by adding a pooling layer after a convolutional layer), a transposed convolution operation is usually used for upsampling the feature space.”\n",
    "    - “Transposed convolution is also called fractionally strided convolution. \n",
    "        - In deep learning literature, another common term that is used to refer to transposed convolution is deconvolution. \n",
    "        - However, note that deconvolution was originally defined as the inverse of a convolution operation, f, on a feature map, x, with weight parameters, w, producing feature map , x', fw(x) = x' . \n",
    "        - A deconvolution function, f-1, can then be defined as fw-1(f(x)) = x. However, note that the transposed convolution is merely focused on recovering the dimensionality of the feature space and not the actual values.”\n",
    "        - “Upsampling feature maps using transposed convolution works by inserting 0s between the elements of the input feature maps. \n",
    "        - The following illustration shows an example of applying transposed convolution to an input of size 4 x 4, with a stride of 2 x 2 and kernel size of 2 x 2. \n",
    "            - The matrix of size 9 x 9 in the center shows the results after inserting such 0s into the input feature map. \n",
    "            - Then, performing a normal convolution using the 2 x2 kernel with a stride of 1 results in an output of size 8 x 8. \n",
    "            - We can verify the backward direction by performing a regular convolution on the output with a stride of 2, which results in an output feature map of size 4 x 4, which is the same as the original input size:\n",
    "\n",
    "- “BatchNorm was introduced in 2015 by Sergey Ioffe and Christian Szegedy in the article Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif”\n",
    "    - “BatchNorm transforms a mini-batch of features based on its computed statistics. \n",
    "    - Assume that we have the net preactivation feature maps obtained after a convolutional layer in a four-dimensional tensor, Z, with the shape m x h x w x c, where m is the number of examples in the batch (i.e., batch size), h x w is the spatial dimension of the feature maps, and c is the number of channels. BatchNorm can be summarized in three steps, as follows:\n",
    "        1. Compute the mean and standard deviation of the net inputs for each mini-batch:\n",
    "        2. “Standardize the net inputs for all examples in the batch”\n",
    "        3. “Scale and shift the normalized net inputs using two learnable parameter vectors”\n",
    "    - “As a consequence, these net inputs are mean-centered and have unit variance, which is generally a useful property for gradient descent-based optimization.”\n",
    "    - “Initially, BatchNorm was developed to reduce the so-called internal covariance shift, which is defined as the changes that occur in the distribution of a layer's activations due to the updated network parameters during training.”\n",
    "        - “However, in 2018, S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry further investigated what makes BatchNorm so effective. In their study, the researchers observed that the effect of BatchNorm on the internal covariance shift is marginal. Based on the outcome of their experiments, they hypothesized that the effectiveness of BatchNorm is, instead, based on a smoother surface of the loss function, which makes the non-convex optimization more robust.”\n",
    "        \n",
    "- “As mentioned at the beginning of this chapter, the goal of a generative model is to learn how to synthesize new samples that have the same distribution as the distribution of the training dataset.”\n",
    "\n",
    "- It can be mathematically shown that the loss function in the original GAN indeed minimizes the JS divergence between the distribution of real and fake examples. But, as discussed in an article by Martin Arjovsky et al. (Wasserstein Generative Adversarial Networks, http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf), JS divergence has problems training a GAN model, and therefore, in order to improve the training, the researchers proposed using the EM distance as a measure of dissimilarity between the distribution of real and fake examples”\n",
    "\n",
    "- Due to the adversarial nature of GAN models, it is notoriously hard to train them. One common cause of failure in training GANs is when the generator gets stuck in a small subspace and learns to generate similar samples - This is called Mode Collapse\n",
    "    - “Besides the vanishing and exploding gradient problems that we saw previously, there are some further aspects that can also make training GAN models difficult (indeed, it is an art). Here are a few suggested tricks from GAN artists.”\n",
    "        - “One approach is called mini-batch discrimination, which is based on the fact that batches consisting of only real or fake examples are fed separately to the discriminator. In mini-batch discrimination, we let the discriminator compare examples across these batches to see whether a batch is real or fake. The diversity of a batch consisting of only real examples is most likely higher than the diversity of a fake batch if a model suffers from mode collapse.”\n",
    "        - “Another technique that is commonly used for stabilizing GAN training is feature matching. In feature matching, we make a slight modification to the objective function of the generator by adding an extra term that minimizes the difference between the original and synthesized images based on intermediate representations (feature maps) of the discriminator. ”\n",
    "        - “During the training, a GAN model can also get stuck in several modes and just hop between them. To avoid this behavior, you can store some old examples and feed them to the discriminator to prevent the generator from revisiting previous modes. This technique is referred to as experience replay. \n",
    "        \n",
    "### Ch. 18: “Reinforcement Learning for Decision Making in Complex Environments”\n",
    "- “In this chapter, we turn our attention to a separate category of machine learning, reinforcement learning (RL), which is different from the previous categories as it is focused on learning a series of actions for optimizing an overall reward—for example, winning at a game of chess”\n",
    "- “However, toward the end of this chapter, we will go over a more challenging example and utilize deep learning architectures for a particular RL approach, which is known as deep Q-learning.”\n",
    "    - “The key element that distinguishes RL from other subtasks of machine learning, such as supervised and unsupervised learning, is that RL is centered around the concept of learning by interaction. \n",
    "    - This means that in RL, the model learns from interactions with an environment to maximize a reward function.”\n",
    "    - “While maximizing a reward function is related to the concept of minimizing the cost function in supervised learning, the correct labels for learning a series of actions are not known or defined upfront in RL—instead, they need to be learned through interactions with the environment, in order to achieve a certain desired outcome—such as winning at a game.”\n",
    "    - With RL, the model (also called an agent) interacts with its environment, and by doing so generates a sequence of interactions that are together called an episode. Through these interactions, the agent collects a series of rewards determined by the environment. These rewards can be positive or negative, and sometimes they are not disclosed to the agent until the end of an episode.”\n",
    "    \n",
    "- “In all examples of RL, we can find two distinct entities: an agent and an environment. \n",
    "    - Formally, an agent is defined as an entity that learns how to make decisions and interacts with its surrounding environment by taking an action. In return, as a consequence of taking an action, the agent receives observations and a reward signal as governed by the environment. \n",
    "    - The environment is anything that falls outside the agent. The environment communicates with the agent and determines the reward signal for the agent's action as well as its observations.”\n",
    "    - “The reward signal is the feedback that the agent receives from interacting with the environment, which is usually provided in the form of a scalar value and can be either positive or negative.”\n",
    "    \n",
    "- “The following sections will begin by first examining the mathematical formulation of Markov decision processes, episodic versus continuing tasks, some key RL terminology, and dynamic programming using the Bellman equation.”\n",
    "    - “In general, the type of problems that RL deals with are typically formulated as Markov decision processes (MDPs). ”\n",
    "        - “The types of problems that require learning an interactive and sequential decision-making process, where the decision at time step t affects the subsequent situations, are mathematically formalized as Markov decision processes (MDPs).”\n",
    "        \n",
    "- “When the probability  is known, then the learning task can be solved with dynamic programming. But when the dynamics of the environment are not known, as it is the case in many real-world problems, then you would need to acquire a large number of samples through interacting with the environment to compensate for the unknown environment dynamics.”\n",
    "    - “Two main approaches for dealing with this problem are the model-free Monte Carlo (MC) and temporal difference (TD) methods. ”\n",
    "\n",
    "- “A Markov process can be represented as a directed cyclic graph in which the nodes in the graph represent the different states of the environment. The edges of the graph (that is, the connections between the nodes) represent the transition probabilities between the states.”\n",
    "    - “There are two types of trajectories. If an agent's trajectory can be divided into subparts such that each starts at time t = 0 and ends in a terminal state  (at t = T), the task is called an episodic task. \n",
    "        - On the other hand, if the trajectory is infinitely continuous without a terminal state, the task is called a continuing task.”\n",
    "        - “The task related to a learning agent for the game of chess is an episodic task, whereas a cleaning robot that is keeping a house tidy is typically performing a continuing task.”\n",
    "        \n",
    "- “This means that the return at time t is equal to the immediate reward r plus the discounted future-return at time t + 1. This is a very important property, which facilitates the computations of the return.”\n",
    "\n",
    "- “The value function, also referred to as the state-value function, measures the goodness of each state—in other words, how good or bad it is to be in a particular state. Note that the criterion for goodness is based on the return.”\n",
    "    - “In a Python implementation, this could be a list or a NumPy array whose indexes refer to different states; or, it could be a Python dictionary, where the dictionary keys map the states to the respective values.”\n",
    "    - “In short, the return is the weighted sum of rewards for an entire episode, which would be equal to the discounted final reward in our chess example (since there is only one reward). The value function is the expectation over all possible episodes, which basically computes how \"valuable\" it is on average to make a certain move.”\n",
    "    \n",
    "- “Before we move directly ahead into some RL algorithms, let's briefly go over the derivation for the Bellman equation, which we can use to implement the policy evaluation.”\n",
    "    - “The Bellman equation is one of the central elements of many RL algorithms. The Bellman equation simplifies the computation of the value function, such that rather than summing over multiple time steps, it uses a recursion that is similar to the recursion for computing the return.”\n",
    "    - “This is called the Bellman equation, which relates the value function for a state, s, to the value function of its subsequent state, s'. This greatly simplifies the computation of the value function because it eliminates the iterative loop along the time axis.\n",
    "    \n",
    "- “In this section, we will cover a series of learning algorithms. \n",
    "    - We will start with dynamic programming, which assumes that the transition dynamics (or the environment dynamics, that is, , are known. ”\n",
    "        - “The agent's state has the Markov property, which means that the next action and reward only depend on the current state and the choice of action we make at this moment or current time step.”\n",
    "        - “We should emphasize that dynamic programming is not a practical approach for solving RL problems. The problem with using dynamic programming is that it assumes full knowledge of the environment dynamics, which is usually unreasonable or impractical for most real-world applications.”\n",
    "        - “There are two main objectives via the tasks described in the following subsections:\n",
    "            1. Obtain the true state-value function, ; this task is also known as the prediction task and is accomplished with policy evaluation.\n",
    "            2. Find the optimal value function, , which is accomplished via generalized policy iteration.”\n",
    "        - “First, recall that a policy, , determines the probability of choosing each action, a, while the agent is at state s. ”\n",
    "            - “Now, in order to find  that always has a better or equal value for each state, we first compute the action-value function, , for each state, s, and action, a, based on the computed state value using the value function . \n",
    "            - We iterate through all the states, and for each state, s, we compare the value of the next state , that would occur if action a was selected.”\n",
    "            - “After we have obtained the highest state value by evaluating all state-action pairs via , we can compare the corresponding action with the action selected by the current policy. \n",
    "            - If the action suggested by the current policy (that is, ) is different than the action suggested by the action-value function (that is, ), then we can update the policy by reassigning the probabilities of actions to match the action that gives the highest action value, . This is called the policy improvement algorithm.”\n",
    "        - “Therefore, if we iteratively perform policy evaluation followed by policy improvement, we are guaranteed to find the optimal policy.\n",
    "            - “However, it can be more efficient if we combine the two tasks of policy evaluation and policy improvement into a single step”\n",
    "            \n",
    "    - “ Using MC methods, the learning process is based on the so-called simulated experience.”\n",
    "        - “For MC-based RL, we define an agent class that follows a probabilistic policy, , and based on this policy, our agent takes an action at each step. This results in a simulated episode.”\n",
    "        - “MC updates for estimating the value function are based on the total return obtained in that episode starting from the first time that state s is visited. This algorithm is called first-visit Monte Carlo value prediction.”\n",
    "        - “To solve this issue, we can extend the algorithm for estimating the first-visit MC state-value prediction. For instance, we can compute the estimated return for each state-action pair using the action-value function. To obtain this estimated return, we consider visits to each state-action pair (s, a), which refers to visiting state s and taking action a.”\n",
    "            - “However, a problem arises since some actions may never be selected, resulting in insufficient exploration. There are a few ways to resolve this. The simplest approach is called exploratory start, which assumes that every state-action pair has a non zero probability at the beginning of the episode.”\n",
    "            - “Another approach to deal with this lack-of-exploration issue is called -greedy policy”\n",
    "        - “MC control refers to the optimization procedure for improving a policy. Similar to the policy iteration approach in previous section (Dynamic programming), we can repeatedly alternate between policy evaluation and policy improvement until we reach the optimal policy.”\n",
    "        - “n order to avoid the lack-of-exploration problem, and to consider the non-visited state-action pairs as discussed earlier, we can let the non-optimal actions have a small chance () to be chosen. \n",
    "            - This is called the -greedy policy, according to which, all non-optimal actions at state s have a minimal  probability of being selected (instead of 0), and the optimal action has a probability of  (instead of 1).”\n",
    "\n",
    "    - “Recall that dynamic programming relies on the complete and accurate knowledge of the environment dynamics. The MC-based method, on the other hand, learns by simulated experience. In this section, we will now introduce a third RL method called TD learning, which can be considered as an improvement or extension of the MC-based RL approach.”\n",
    "        - “Similar to the MC technique, TD learning is also based on learning by experience and, therefore, does not require any knowledge of environment dynamics and transition probabilities. The main difference between the TD and MC techniques is that in MC, we have to wait until the end of the episode to be able to calculate the total return.”\n",
    "            - “However, in TD learning, we can leverage some of the learned properties to update the estimated values before reaching the end of the episode. This is called bootstrapping (in the context of RL, the term bootstrapping is not to be confused with the bootstrap estimates we used in Chapter 7, Combining Different Models for Ensemble Learning).”\n",
    "        - “Notice the difference between MC and TD. In MC, Gt:t+1 is not available until the end of the episode, so we should execute as many steps as needed to get there. On the contrary, in TD, we only need to go one step ahead to get the target return. This is also known as TD(0).”\n",
    "        - “Now that we have covered the prediction task using the TD algorithm, we can move on to the control task. We will cover two algorithms for TD control: an on-policy control and an off-policy control. In both cases, we use the GPI that was used in both the dynamic programming and MC algorithms. \n",
    "            - In on-policy TD control, the value function is updated based on the actions from the same policy that the agent is following, while in an off-policy algorithm, the value function is updated based on actions outside the current policy.”\n",
    "            - “On-policy TD control (SARSA)”\n",
    "            - “Off-policy TD control (Q-learning)”\n",
    "                - “Instead of updating the action-value function using the action value of  that is taken by the agent, we can find the best action even if it is not actually chosen by the agent following the current policy. (That's why this is considered an off-policy algorithm.)”\n",
    "                - “To do this, we can modify the update rule to consider the maximum Q-value by varying different actions in the next immediate state. ”\n",
    "\n",
    "- “OpenAI Gym is a specialized toolkit for facilitating the development of RL models. OpenAI Gym comes with several predefined environments. Some basic examples are CartPole and MountainCar, where the tasks are to balance a pole and to move a car up a hill, respectively, as the names suggest.”\n",
    "\n",
    "- Summary \n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
