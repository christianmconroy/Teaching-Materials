---
title: "Twitter Analysis in R"
author: "Christian Conroy"
date: "September 5, 2019"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
require(knitr)
require(readxl)
require(ggplot2)

opts_chunk$set(echo = TRUE)
options(digits = 3)

opts_knit$set(root.dir ="~/GeorgetownMPPMSFS/McCourtMPP/RFiles/NLP Training for Acc")
```

# Module 1: Case study: comparing Twitter archives
## https://www.tidytextmining.com/twitter.html

One type of text that gets plenty of attention is text shared online via Twitter. In fact, several of the sentiment lexicons used in this book (and commonly used in general) were designed for use with and validated on tweets. Both of the authors of this book are on Twitter and are fairly regular users of it, so in this case study, let’s compare the entire Twitter archives of Julia and David.

An individual can download their own Twitter archive by following directions available on Twitter’s website.

Let’s use the lubridate package to convert the string timestamps to date-time objects and initially take a look at our tweeting patterns overall (Figure 7.1).

```{r message = FALSE, warning= FALSE}

library(rtweet)

# whatever name you assigned to your created app
appname <- #

## api key (example below is not a real key)
key <- #

## api secret (example below is not a real key)
secret <- #

twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret,
  access_token = access_token,
  access_secret = access_secret)

```

```{r message = FALSE, warning= FALSE}

tweets_christian <- get_timeline("christianmconro", n = 3200)

```


```{r message = FALSE, warning= FALSE}

library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)

tweets <- bind_rows(tweets_christian %>% 
                      mutate(person = "Christian")) %>%
  mutate(timestamp = ymd_hms(created_at))

ggplot(tweets, aes(x = timestamp)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE)

```

Let’s use unnest_tokens() to make a tidy data frame of all the words in our tweets, and remove the common English stop words. There are certain conventions in how people use text on Twitter, so we will use a specialized tokenizer and do a bit more work with our text here than, for example, we did with the narrative text from Project Gutenberg.

First, we will remove tweets from this dataset that are retweets so that we only have tweets that we wrote ourselves. Next, the mutate() line cleans out some characters that we don’t want like ampersands and such.

In the call to unnest_tokens(), we unnest using the specialized “tweets” tokenizer that is built in to the tokenizers package (Mullen 2016). This tool is very useful for dealing with Twitter text or other text from online forums; it retains hashtags and mentions of usernames with the @ symbol.

Because we have kept text such as hashtags and usernames in the dataset, we can’t use a simple anti_join() to remove stop words. Instead, we can take the approach shown in the filter() line that uses str_detect() from the stringr package.

```{r message = FALSE, warning= FALSE}
library(tidytext)
library(stringr)

remove_reg <- "&amp;|&lt;|&gt;"
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
```

Now we can calculate word frequencies for each person. First, we group by person and count how many times each person used each word. Then we use left_join() to add a column of the total number of words used by each person. Finally, we calculate a frequency for each person and word.

```{r message = FALSE, warning= FALSE}

frequency <- tidy_tweets %>% 
  group_by(person) %>% 
  count(word, sort = TRUE) %>% 
  left_join(tidy_tweets %>% 
              group_by(person) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)

frequency

```

# Module 2: Case study: Twitter Analysis in R
## https://www.tidytextmining.com/twitter.html

Getting Started
It is fairly easy to create Twitter analysis in R. You need a few packages to kick things off.

```{r message = FALSE, warning= FALSE}
# install.packages('twitteR')
require(twitteR)
require(wordcloud)
# install.packages('igraph')
require(igraph)

```

Get the Tweets
This is achieved using the twitteR package. 

Authentication

```{r message = FALSE, warning= FALSE}

consumer_key <- #
 
consumer_secret <-  #
 
access_token <- #
 
access_secret <- #

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

```

This package offers a number of functions to access the twitter API. 

Here, the searchTwitter function is used. In this case, a search for 250 tweets containing the pattern '#occupy' is carried out. The value returned is a list of 250 tweet 'status' objects. These objects are the new(ish) R reference objects - similar to Python mutable objects.

```{r message = FALSE, warning= FALSE}

tweets <- searchTwitter("#occupy", n = 250)

```

The next step is to carry out some kind of analysis on the tweets. As a first example, an analysis of the tweeters is presented.

The tweet 'status' objects, as mentioned earlier, are R reference objects, and contain a number of items of information about a tweet. 

Perhaps most importantly, they contain the text of the tweet - but they also contain the 'screen name' of the tweeter. 

 Analysing these is also important - in the above example, an analysis of the authors of tweets relating to the Occupy movement should identify some key actors in this movement (or at least some that are active at the time of the tweet search).
 
The following code loops through the list of 'status' objects, extracts the 'screen names' of the tweeters and then - via the table function, a frequency table of the screen names is contructed. This is stored in the tweeter.freq array - the names of items in this array are the screen names, and the values are the frequency that each of these screen names appears in the tweets.

```{r message = FALSE, warning= FALSE}

# Create an empty character vector of the same length as the number of
# tweets
l <- length(tweets)
tweeter <- vector(mode = "character", length = l)

# Extract the screen names from each tweet status
for (i in 1:l) tweeter[i] <- tweets[[i]]$getScreenName()

# Remember this notation. For lists, one generally uses [[ to select any single element, whereas [ returns a list of the selected elements. Hence we are selecting on the right and returning on the left. 

# Compile the frequencies of each screen name
tweeter.freq <- table(tweeter)

```

Create a Word Cloud - Who are the Tweeters?
The last section created a frequency table of the screen names of people sending tweets. In this section that information will be visualised. 

One way to do this is to generate a 'Word Cloud' such as those featured in Wordle - and the wordcloud package in R is a tool for this.
 
Pretty much all of the work is done by the wordcloud function – the only other command sets a black background - this isn't necessary, but does create a fairly striking image.

```{r message = FALSE, warning= FALSE}

# Black backgrounds are 'this year's colour' in terms of dataviz!
par(bg = "black")

# Draw the word cloud
wordcloud(names(tweeter.freq), tweeter.freq, colors = c("tomato", 
    "wheat", "lightblue"), scale = c(6, 0.5), random.color = TRUE, rot.per = 0.5, 
    min.freq = 1, font = 2, family = "serif")

# Note with table objects, the actual object appears to give proportions with the words themselves coming from names()

# Because I am doing this now and not in the middle of occupy, I get different results than the original author of this tutorial did. 

```

For example, a more conventional representation of the screen name counts might be via a bar plot:

```{r message = FALSE, warning= FALSE}

par(mar = c(4, 8, 2, 4) + 0.1)
barplot(sort(tweeter.freq), horiz = TRUE, las = 2, col = "tomato", 
    xlab = "Tweet Tally")
title("Number of Tweets by Screen Name")

```

Parameters 1 and 2 : …are the words to appear in the wordcloud and the frequencies of the words respectively.
colors : These are the colours to draw the text.
scale : A pair of numbers, representing the smallest and largest text size in the word cloud. These aren't in points, but are realtive to 'standard' font size - so in this case sizes range from half the standard height to six times the standard height.
random.color : Set to TRUE if the colours specified above are assigned randomly to the text strings. The alternative (FALSE) is to assign according to text size.
rot.per : This is the proportion of words that are rotated by \( 90^\circ \) - here set to one half.
min.freq : The smallest frequency of a word that appears in the cloud. Here it is set to 1, so all words are included.
font : Here set to the R numeric value of 2 - giving a bold sans serif font - experiment has shown this to be effective.
family : Linked to the above – use a family of serif fonts.

To dig more deeply, the next set of code extracts the tweet texts (in the same way that the screen names were extracted above) - and then selectively prints tweets attributed to what look like the top two tweeters here. 
```{r message = FALSE, warning= FALSE}
# Create an empty vector to store the tweet texts
texts <- vector(mode = "character", length = l)

# Extract the tweet text from each tweet status
for (i in 1:l) texts[i] <- tweets[[i]]$getText()

# List all the tweets tweeted by @leofroth
texts[tweeter == "ritchiepage2001"]

```
So this guy just posted the same thing over and over again. Could be an automated bot of some sort. 

```{r message = FALSE, warning= FALSE}
# List all the tweets tweeted by second largest
texts[tweeter == "ProGloCommons"]

```

If we wanted to do this for ANY hashtag, we could use the function below 

```{r message = FALSE, warning= FALSE}

tweetcloud <- function(hashtag, ntweets = 250, scale = c(6, 0.5), 
    min.freq = 1) {
    tweets <- searchTwitter(hashtag, n = ntweets)
    l <- length(tweets)
    tweeter <- vector(mode = "character", length = l)
    for (i in 1:l) tweeter[i] <- tweets[[i]]$getScreenName()
    tweeter.freq <- table(tweeter)
    par(bg = "black")
    wordcloud(names(tweeter.freq), tweeter.freq, colors = c("tomato", "wheat", 
        "lightblue"), scale = scale, random.color = TRUE, rot.per = 0.5, min.freq = min.freq, 
        font = 2, family = "serif")
}

```


```{r message = FALSE, warning= FALSE}
tweetcloud("#SharpiePresident")

```

Looking at Relationships Between Hashtags

To identify linkages between hashtags, one possibility is to make use of graph theory.

In particular this is useful for analysing which hashtags tend to appear together in the same tweet. 

This information can be represented as a graph - each hashtag appearing in the collection of tweets is represented by a graph vertex - and pairs of hashtags appearing in the same tweet are represented by a graph edge.

The igraph package provides a set of tools for handling, analysing and visualising graph data. 

The first task here is to create a hashtag graph as described above. In the code below a function tags is defined. Given the text string of a tweet this returns a list of all of the hashtags in this string as a string array. It also converts these to upper case - this is to simplify the subsequent analysis, by assuming that, for example '#OCCUPY', '#Occupy' and '#occupy' are essentially the same hashtag. This function is then applied to each tweet text, and stored in taglist - a list of lists of the same length as the number of tweets - taglist[[i]] contains all of the hashtags extracted from tags[i].

```{r message = FALSE, warning= FALSE}
# define a tag extractor function
tags <- function(x) toupper(grep("^#", strsplit(x, " +")[[1]], value = TRUE))
# Create a list of the tag sets for each tweet
taglist <- vector(mode = "list", l)
# ... and populate it
for (i in 1:l) taglist[[i]] <- tags(texts[i])
# Now make a list of all unique hashtags
alltags <- NULL
for (i in 1:l) alltags <- union(alltags, taglist[[i]])

# Kind of similar to looping in python. We're creating empty objects and then populating

```

Once all of this code is run, the hashtag data and information about which hashtags appear in the same tweet will be represented. The next stage is to store this in an igraph graph object. Without going in to too much detail the code below does this.

```{r message = FALSE, warning= FALSE}
# Create an empty graph
hash.graph <- graph.empty(directed = FALSE)
# Populate it with nodes
hash.graph <- hash.graph + vertices(alltags)
# Populate it with edges
for (tags in taglist) {
    if (length(tags) > 1) {
        for (pair in combn(length(tags), 2, simplify = FALSE, FUN = function(x) sort(tags[x]))) {
            if (pair[1] != pair[2]) {
                if (hash.graph[pair[1], pair[2]] == 0) 
                  hash.graph <- hash.graph + edge(pair[1], pair[2])
            }
        }
    }
}
```

The above code stores the hashtags and their co-appearances in a graph object called hash.graph. However, it is more informative to visualise this than simply store it internally. The next block of code does this. 

The first line below states that the layout algorithm used to determine where the graph vertices should be drawn is the Kamada-Kawei algorithm - this is essentially a force-directed algorithm, intended to present the vertices and edges in away that positions groups of vertices with many interconnections close to one another. igraph offers several alternative algorithms - it may be interesting to explore and experiment.

The rest of the code supplies graphical parameters for the graph object – when the left hand side of the <- is V(hash.graph) the parameters refer to vertices, and when it is E(hash.graph) they refer to edges. If the value assigned is a scalar (or more accurately in R a vector of length 1) then it applies to all edges or vertices. If it is a vector of values, these are assigned to each individual vertex or edge - for example V(hash.graph)$label <- V(hash.graph)$name extracts all of the names used to refer to the vertices by igraph and assigns them to the labels used for the vertices when the graph is plotted. When these various parameters are set (again more details are in the igraph documentation links) a command is issued to plot the graph. To make the most of this, you will need to make the R plotting window quitew large - there are quite a few hashtags and they are interconnected by a complicated set of edges.

```{r message = FALSE, warning= FALSE}
hash.graph$layout <- layout.kamada.kawai
V(hash.graph)$color <- "tomato"
E(hash.graph)$color <- "black"
V(hash.graph)$label <- V(hash.graph)$name
V(hash.graph)$label.cex = 0.5
V(hash.graph)$size <- 20
V(hash.graph)$size2 <- 2
V(hash.graph)$shape <- "rectangle"
plot(hash.graph)

# Would need other way of visualizing
```

Hashtags that are more 'central' in the above graph might be thought of as representing key themes in the collection of tweet texts, since they are mentioned in the context of a large number of other themes.

(THIS SOUNDS KIND OF LIKE WE'RE GETTING INTO NETWORK ANALYSIS NOW!)


Fortunately, the igraph package provides a function to compute this measure of centrality.

```{r message = FALSE, warning= FALSE}
p.rank <- sort(betweenness(hash.graph, directed = TRUE), decreasing = TRUE)[-1]
p.rank <- sqrt(p.rank[p.rank > 0])
par(bg = "black")
wordcloud(names(p.rank), p.rank, colors = c("tomato", "wheat", "lightblue"), 
    scale = c(7, 0.5), random.color = TRUE, rot.per = 0.5, min.freq = 1, font = 2, 
    family = "serif")
```

# Module 3: Twitter Data Analysis with R
## http://www.rdatamining.com/docs/twitter-analysis-with-r

Analyzing RDataMining Account

Topic Modeling and Sentiment Analysis

1. Extract tweets and followers from the Twitter website with R
and the twitteR package
2. With the tm package, clean text by removing punctuations,
numbers, hyperlinks and stop words, followed by stemming
and stem completion
3. Build a term-document matrix
4. Analyse topics with the topicmodels package
5. Analyse sentiment with the sentiment140 package
6. Analyse following/followed and retweeting relationships with
the igraph package

# NOTE: WE WILL NOT DO THE SENTIMENT ANALYSIS PART AS THIS TUTORIAL USES A NOW DEPRICATED SENTIMENT140 PACKAGE

```{r message = FALSE, warning= FALSE}
# Load in Packages 
library(twitteR)
library(tm)
library(topicmodels)
library(igraph)
```

```{r message = FALSE, warning= FALSE}
consumer_key <- #
 
consumer_secret <- #
 
access_token <- #
 
access_secret <- #

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

```{r message = FALSE, warning= FALSE}
## Option 1: retrieve tweets from Twitter
## 3200 is the maximum to retrieve
tweets <- userTimeline("RDataMining", n = 3200)
```

```{r message = FALSE, warning= FALSE}
# Explore tweets and put in right format
(n.tweet <- length(tweets))

# convert tweets to a data frame
tweets.df <- twListToDF(tweets)
# tweet #190
tweets.df[190, c("id", "created", "screenName", "replyToSN",
"favoriteCount", "retweetCount", "longitude", "latitude", "text")]

# print tweet #190 and make text fit for slide width
writeLines(strwrap(tweets.df$text[190], 60))
```

Text Cleaning 

```{r message = FALSE, warning= FALSE}

library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(tweets.df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space (Get rid of @ and # in one swoop!)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp") # I think he's adding on here.
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy for stem completion later
myCorpusCopy <- myCorpus

```

Stemming and Stem Completion

```{r message = FALSE, warning= FALSE}

myCorpus <- tm_map(myCorpus, stemDocument) # stem words
writeLines(strwrap(myCorpus[[190]]$content, 60))
## r refer card data mine now provid link packag cran packag
## mapreduc hadoop ad
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}

myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
writeLines(strwrap(myCorpus[[190]]$content, 60))

# Evidently had to go mamnual to get the remaining parts?
```

Count Word Frequencies 

```{r message = FALSE, warning= FALSE}

# count word frequence (not sure why he's going about it so manually)
wordFreq <- function(corpus, word) {
results <- lapply(corpus,
function(x) { grep(as.character(x), pattern=paste0("\\<",word)) }
)
sum(unlist(results))
}
n.miner <- wordFreq(myCorpusCopy, "miner")
n.mining <- wordFreq(myCorpusCopy, "mining")
cat(n.miner, n.mining)

# cat is just concatenate and print. This is just searching how many times for each word in all the tweets I believe

```

Replace old word with new word 

```{r message = FALSE, warning= FALSE}

# replace oldword with newword
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub),
pattern=oldword, replacement=newword)
}
myCorpus <- replaceWord(myCorpus, "miner", "mining")
myCorpus <- replaceWord(myCorpus, "universidad", "university")
myCorpus <- replaceWord(myCorpus, "scienc", "science")

# Might be useful if dealing with British text. 
```

Build Term Document Matrix (The big show!)

```{r message = FALSE, warning= FALSE}

tdm <- TermDocumentMatrix(myCorpus,

control = list(wordLengths = c(1, Inf)))

tdm

```

Look at specific words in the document term matrix

```{r message = FALSE, warning= FALSE}
idx <- which(dimnames(tdm)$Terms %in% c("r", "data", "mining"))
as.matrix(tdm[idx, 21:30]) # Just looking at those last documents
```

Top Frequent Terms 
```{r message = FALSE, warning= FALSE}
# inspect frequent words
(freq.terms <- findFreqTerms(tdm, lowfreq = 20))

# I think the parantheses is just a short way to create an object and print it at same time rather than creating and calling out in separate lines

# It would appear that the cleaning missed some things on first look? But when I did it again without changing anything it was fine? 

term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 20)
df <- data.frame(term = names(term.freq), freq = term.freq)
```

Visualizing Term Frequency 

```{r message = FALSE, warning= FALSE}
library(ggplot2)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip() +
theme(axis.text=element_text(size=7))

```

Word Cloud 
```{r message = FALSE, warning= FALSE}
library(RColorBrewer)
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
```

```{r message = FALSE, warning= FALSE}
# plot word cloud
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
random.order = F, colors = pal)
```

Associations 

```{r message = FALSE, warning= FALSE}
# which words are associated with 'r'?
findAssocs(tdm, "r", 0.2)
# 0.2 is the lower corr limit, which is generous here

# which words are associated with 'data'?
findAssocs(tdm, "data", 0.2)
```
Topic Modeling

```{r message = FALSE, warning= FALSE}
# Once you've got it into tdm, you can do a lot. 

dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
term <- terms(lda, 7) # first 7 terms of every topic
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))

# This is a smaller data set, so it ran faster. Not sure how robust these groups are though. 
```

```{r message = FALSE, warning= FALSE}
library(data.table)

topics <- topics(lda) # 1st topic identified for every document (tweet)
topics <- data.frame(date=as.IDate(tweets.df$created), topic=topics)
ggplot(topics, aes(date, fill = term[topic])) +
geom_density(position = "stack")

# I guess this looks at how much falls into each over time? So just overall less tweeting at the end of 2013 or so, but then rises back in 2015
# Looks like when it comes back it's more about one topic than it was previously, though these topics are not very distinct. 

```

Retrieve User Info and Followers

```{r message = FALSE, warning= FALSE}
user <- getUser("RDataMining")
user$toDataFrame()
friends <- user$getFriends() # who this user follows
followers <- user$getFollowers() # this user's followers
followers2 <- followers[[1]]$getFollowers() # a follower's followers

# Look here in future for cool way to geographically map followers: 
http://biostat.jhsph.edu/~jleek/code/twitterMap.R

```

Top Retweets
```{r message = FALSE, warning= FALSE}
# select top retweeted tweets
table(tweets.df$retweetCount)
selected <- which(tweets.df$retweetCount >= 9)

# plot them
dates <- strptime(tweets.df$created, format="%Y-%m-%d")
plot(x=dates, y=tweets.df$retweetCount, type="l", col="grey",
xlab="Date", ylab="Times retweeted")
colors <- rainbow(10)[1:length(selected)]
points(dates[selected], tweets.df$retweetCount[selected],
pch=19, col=colors)
text(dates[selected], tweets.df$retweetCount[selected],
tweets.df$text[selected], col=colors, cex=.9)
```

# Module 4: Lesson 2. Twitter Data in R Using Rtweet: Analyze and Download Twitter Data
## https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/use-twitter-api-r/

In this lesson you will explore analyzing social media data accessed from twitter, in R. You will use the Twitter RESTful API to access data about both twitter users and what they are tweeting about

```{r message = FALSE, warning= FALSE}
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
```

I WILL SKIP AUTHENTICATION AS I ALREADY DID IT EARLIER FOR RTWEET!

Search Twitter for Tweets

Notice below you use the rtweet::search_tweets() function to search. search_tweets() requires the following arguments:

q: the query word that you want to look for
n: the number of tweets that you want returned. You can request up to a maximum of 18,000 tweets.

```{r message = FALSE, warning= FALSE}
## search for 500 tweets using the #rstats hashtag
rstats_tweets <- search_tweets(q = "#rstats",
                               n = 500)

# view the first 3 rows of the dataframe
head(rstats_tweets, n = 3)
```
Dealing with Retweets 

```{r message = FALSE, warning= FALSE}
# find recent tweets with #rstats but ignore retweets
rstats_tweets <- search_tweets("#rstats", n = 500,
                             include_rts = FALSE)

# view top 2 rows of data
head(rstats_tweets, n = 2)
```

Next, let’s figure out who is tweeting about R using the #rstats hashtag.

```{r message = FALSE, warning= FALSE}
head(rstats_tweets$screen_name)
unique(rstats_tweets$screen_name)
```
You can similarly use the search_users() function to just see what users are tweeting using a particular hashtag. This function returns just a data.frame of the users and information about their accounts

```{r message = FALSE, warning= FALSE}
users <- search_users("#DaBears",
                      n = 500)

head(users, n = 2)
```

Let’s learn a bit more about these people tweeting about R. First, where are they from?

```{r message = FALSE, warning= FALSE}
# how many locations are represented
length(unique(users$location))
## [1] 320

users %>%
  ggplot(aes(location)) +
  geom_bar() + coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Twitter users - unique locations ")
```

Let’s sort by count and just plot the top locations. To do this, you use top_n(). Note that in this case you are grouping your data by user. Thus top_n() will return locations with atleast 15 users associated with it.

```{r message = FALSE, warning= FALSE}
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Where Twitter users are from - unique locations ")

# Since I changed it up to focus on The Bears, it makes since that Chicago is so high. Obviously here it is including NAs. 

```

It looks like you have some NA or no data values in your list. Let’s remove those with na.omit().

```{r message = FALSE, warning= FALSE}
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location,n)) %>%
  na.omit() %>%
  top_n(20) %>%
  ggplot(aes(x = location,y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Location",
      y = "Count",
      title = "Twitter users - unique locations ")

# It's not catching what are the nas for some reason. Would likely have to code up front that the nas are blanks.
```

Looking at your data, what do you notice that might improve this plot? There are 314 unique locations in your list. However, everyone didn’t specify their locations using the approach. For example some just identified their country: United States for example and others specified a city and state. You may want to do some cleaning of these data to be able to better plot this distribution - especially if you want to create a map of these data!

# Module 5: Lesson 2. Twitter Data and Network Analysis with R
## https://rpubs.com/ben_bellman/rtweet_tidygraph

This guide will illustrate how to use the rtweet package to download Twitter data, and introduce network analysis with tidygraph package.

(I ALREADY AUTHENTICATED R TWEET EARLIER SO NO NEED TO DO AGAIN HERE)


```{r message = FALSE, warning= FALSE}
library(tidyverse)
library(rtweet)
```

Weakness of rtweet here is that it returns data just from last 6-9 days

```{r message = FALSE, warning= FALSE}
# we can get recent tweets by hashtag
# returns data from the last 6-9 days
rstats <- search_tweets("#rstats", n = 50)
```

```{r message = FALSE, warning= FALSE}
# and also by account handle
(cctweets <- search_tweets("christianmconro", n = 50))

```

```{r message = FALSE, warning= FALSE}
# Get user profile description and location
cc_profile <- search_users("christianmconro")
cc_profile$description
cc_profile$location # I don't have this listed on my profile. 

```

When working with APIs, it’s important to remember rate limiting. API queries cost server time and money, so rate limiting API keys helps regulate traffic. The Twitter API is actually a collection of smaller APIs, all of which have different limits. We can check our key’s status on all limits with the rate_limit() function. The reset column is time remaining in minutes.

```{r message = FALSE, warning= FALSE}
rate_limit()

```

The rate limit for accessing friends (Twitter’s term for who an account follows) is 15 requests per 15 minutes. I follow 757 people, which would take nearly 13 hours to download. Instead, I’ll analyze 75 account sample of my 231 followers, telling R to sleep for 15 minutes before pulling the next 15 accounts’ friends.

```{r message = FALSE, warning= FALSE}
my_followers <- get_followers("christianmconro")
glimpse(my_followers)

```

The result is a tbl with one column, a de-identified account ID.I prefer to treat it as a vector, and sample 75 accounts.

```{r message = FALSE, warning= FALSE}
ids <- sample(my_followers$user_id, 75)
```

Now I’ll loop through the IDs and pause the loop every 15 accounts

```{r message = FALSE, warning= FALSE}
# create empty list to store results
friends <- list()

# start loop
for (a in 1:length(ids)){
  friends[[a]] <- get_friends(ids[a])
  
  # pause if divisible by 15
  if (a %% 15 == 0){
    Sys.sleep(15*60) # must enter time in seconds
  }
}

# Combine data tables in list
friends <- bind_rows(friends) %>% 
  rename(friend = user_id)
write.csv(friends, "twitter_friends.csv")

# The API limits make this a little challenging it seems. This takes forever. Def don't want to do again. 

```

Using tidygraph

Let’s bring in the Twitter friend data, and turn it into an tbl_graph object. But first, let’s see how many of these follower’s friends are also my followers.



```{r message = FALSE, warning= FALSE}
friends <- read.csv("twitter_friends.csv", stringsAsFactors = FALSE)
```

```{r message = FALSE, warning= FALSE}
filter(friends, friend %in% user)
```

So Some of these followers are following my other followers. 

Let's check to see if they are following any of the same people. 

```{r message = FALSE, warning= FALSE}
net <- friends %>% 
  group_by(friend) %>% 
  mutate(count = n()) %>% 
  ungroup() %>% 
  filter(count > 2)
glimpse(net)

# Would not work when the filter was just > 1. Got weights of 0. Upping it to 2 made it work though. Perhaps 1 actually just meant there were no connections and so it couldn't be plotted?


```

We’ve got links! Let’s create a directed tbl_graph!

```{r message = FALSE, warning= FALSE}
library(tidygraph)

g <- net %>% 
  select(user, friend) %>%  # drop the count column
  as_tbl_graph()
g

```

Network analysis is best expressed with visualizations, which can be implemented with the ggplot framework using the ggraph package.

```{r message = FALSE, warning= FALSE}
library(ggraph)
ggraph(g) +
  geom_edge_link() +
  geom_node_point(size = 3, colour = 'steelblue') +
  theme_graph()

# I'd want to narrow this down much more for it to be any use. Way to dense right now. 
```

```{r message = FALSE, warning= FALSE}
ggraph(g) +
  geom_edge_link(edge_width = 0.25, arrow = arrow(30, unit(.15, "cm"))) +
  theme_graph()
```

Let’s calculate the centrality of the nodes, and visualize this attribute. tidygraph contains LOTS different measures of centrality, so I’ll use betweenness, since that is the concept I’m most familiar with.

```{r message = FALSE, warning= FALSE}
g2 <- net %>% 
  select(user, friend) %>%  # drop the count column
  as_tbl_graph(directed = F) %>%  # make undirected
  activate(nodes) %>% 
  mutate(centrality = centrality_betweenness())
g2
```

```{r message = FALSE, warning= FALSE}
ggraph(g2) +
  geom_edge_link() +
  geom_node_point(aes(size = centrality, colour = centrality)) +
  theme_graph()

# Still not super useful
```

# Module 6: Analyzing Tweets in R
## https://medium.com/the-artificial-impostor/analyzing-tweets-with-r-92ff2ef990c6

As the name suggests, tidytext aims to provide text processing capability in the tidyverse ecosystem.

Chapter 7 of the book provides a case study comparing tweet archives of the two authors.

# AGAIN SKIPPING AUTHENTICATION SINCE I'VE ALREADY DONE IT.

The main access point for this post is userTimeline. It downloads at most 3200 recent tweets of a public twitter user. The default includeRts=FALSE parameter seems to remove a lot of false positives, so we’ll instead do it manually later.

Authentication

```{r message = FALSE, warning= FALSE}
library(twitteR)

consumer_key <- #
 
consumer_secret <- #
 
access_token <- #
 
access_secret <- #

setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

```

```{r message = FALSE, warning= FALSE}


christian <- userTimeline("christianmconro", n=3200, includeRts=T)
danett <- userTimeline("DanettSong", n=3200, includeRts=T)
carlos <- userTimeline("cprietonews", n=3200, includeRts=T)

```

We’ll now convert them to data frames:

```{r message = FALSE, warning= FALSE}

df.christian <- twListToDF(christian)
df.danett <- twListToDF(danett)
df.carlos <- twListToDF(carlos)

```

We’re going to only keep columns text, favoriteCount, screenName, created and retweetCount, and filter out those rows with isRetweet=TRUE. (statusSource might be of interest in some applications.)

```{r message = FALSE, warning= FALSE}
tweets <- bind_rows(
  df.christian %>% 
    filter(isRetweet==F) %>%
    select(text, screenName, created, retweetCount, favoriteCount),
  df.danett %>% filter(isRetweet==F) %>%
    select(text, screenName, created, retweetCount, favoriteCount),
  df.carlos %>% filter(isRetweet==F) %>%
    select(text, screenName, created, retweetCount, favoriteCount))

# Really convenient because we can put them all in the same dataframe, and filter the rows and columns for each at same time. There is already a screenname column too. 
```

Now we plot the time distribution of the tweets:

```{r message = FALSE, warning= FALSE}
ggplot(tweets, aes(x = created, fill = screenName)) +
  geom_histogram(
    position = "identity", bins = 50, show.legend = FALSE) +
  facet_wrap(~screenName, ncol = 1, scales = "free_y") + 
  ggtitle("Tweet Activity (Adaptive y-axis)")
```

To look at relative instead of absolute:

```{r message = FALSE, warning= FALSE}
ggplot(tweets, aes(x = created, fill = screenName)) +
  geom_histogram(
    position = "identity", bins = 50, show.legend = FALSE) +
  facet_wrap(~screenName, ncol = 1) + 
  ggtitle("Tweet Activity (Adaptive y-axis)")
```

Word frequencies
From this point we’ll enter the world of tidyverse:

```{r message = FALSE, warning= FALSE}
replace_reg <- "http[s]?://[A-Za-z\\d/\\.]+|&amp;|&lt;|&gt;"
# There's got to be a built in package for this? If not always come back to this. 
unnest_reg  <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  mutate(id = row_number()) %>%
  unnest_tokens(
    word, text, token = "regex", pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
```

(It’s worth noting that the pattern used in unnest_tokens are for matching separators, not tokens.) Now we have the data in one-token-per-row tidy text format. We can use it to do some counting:

```{r message = FALSE, warning= FALSE}
frequency <- tidy_tweets %>% 
  group_by(screenName) %>% 
  count(word, sort = TRUE) %>% 
  left_join(tidy_tweets %>% 
              group_by(screenName) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)


```

```{r message = FALSE, warning= FALSE}
frequency.spread <- frequency %>% 
  select(screenName, word, freq) %>% 
  spread(screenName, freq) %>%
  arrange(desc(christianmconro), desc(cprietonews), desc(DanettSong))
```

```{r message = FALSE, warning= FALSE}
library(scales)
ggplot(frequency.spread, aes(christianmconro, DanettSong)) +
  geom_jitter(
    alpha = 0.1, size = 2.5, width = 0.15, height = 0.15) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 0) +    
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red") + theme_bw()
```

Comparing word usage

```{r message = FALSE, warning= FALSE}
word_ratios <- tidy_tweets %>%
  filter(screenName != "DanettSong") %>%
  filter(!str_detect(word, "^@")) %>%
  count(word, screenName) %>%
  filter(sum(n) >= 10) %>%
  ungroup() %>%
  spread(screenName, n, fill = 0) %>%
  mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %>%
  mutate(logratio = log(christianmconro / cprietonews)) %>%
  arrange(desc(logratio))

word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  ylab("log odds ratio (christianmconro/cprietonews)") +
  scale_fill_discrete(name = "", labels = c("christianmconro", "cprietonews"))
```

We can count all the retweets and favorites and count which words are more likely to appear. Note it’s important to count each word only once in every tweet. We achieve this by grouping by (id, word, screenName) and summarise with a first function:

```{r message = FALSE, warning= FALSE}
totals <- tweets %>%
  group_by(screenName) %>% 
  summarise(total_rts = sum(retweetCount)) 

word_by_rts <- tidy_tweets %>% 
  group_by(id, word, screenName) %>% 
  summarise(rts = first(retweetCount)) %>% 
  group_by(screenName, word) %>% 
  summarise(retweetCount = median(rts), uses = n()) %>% # So after the grouping here
  left_join(totals) %>%
  filter(retweetCount != 0) %>%
  ungroup()

word_by_rts %>%
  filter(uses >= 5) %>%
  group_by(screenName) %>%
  top_n(10, favoriteCount) %>%
  arrange(favoriteCount) %>%
  ungroup() %>%
  mutate(
    word = reorder(
      paste(word, screenName, sep = "__"), 
      retweetCount)) %>%
  ungroup() %>%
  ggplot(aes(word, favroiteCount, fill = screenName)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ screenName, scales = "free_y", ncol = 1) +
  scale_x_discrete(labels = function(x) gsub("__.+$","", x)) +  
  coord_flip() +
  labs(x = NULL, 
       y = "Median 
       # of retweetCount for tweets containing each word")
```

```{r message = FALSE, warning= FALSE}
totals <- tweets %>%
  group_by(screenName) %>% 
  summarise(total_favs = sum(favoriteCount)) 

word_by_favs <- tidy_tweets %>% 
  group_by(id, word, screenName) %>% 
  summarise(favs = first(favoriteCount)) %>% 
  group_by(screenName, word) %>% 
  summarise(favoriteCount = median(favs), uses = n()) %>% # So after the grouping here
  left_join(totals) %>%
  filter(favoriteCount != 0) %>%
  ungroup()

word_by_favs %>%
  filter(uses >= 5) %>%
  group_by(screenName) %>%
  top_n(10, favoriteCount) %>%
  arrange(favoriteCount) %>%
  ungroup() %>%
  mutate(
    word = reorder(
      paste(word, screenName, sep = "__"), 
      favoriteCount)) %>%
  ungroup() %>%
  ggplot(aes(word, favoriteCount, fill = screenName)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ screenName, scales = "free_y", ncol = 1) +
  scale_x_discrete(labels = function(x) gsub("__.+$","", x)) +  
  coord_flip() +
  labs(x = NULL, 
       y = "Median 
       # of favoriteCount for tweets containing each word")
```

The three of us evidently don't get a lot of retweets. what about favorites? Carlos does well consistently but Danett did well with BroadwayforHillary. Either way we all suck. 

# Module 6: Twitter Analytics Using R 
## https://wetlands.io/maps/Twitter_Analytics_R.nb.html

# Skipping authenticationa as I've done above - Using TwitteR on this one. 

Extract Tweets Using a Search Term

```{r message = FALSE, warning= FALSE}
search.string <- "NeZha"
result.term <- searchTwitter(search.string, n = 100)
head(result.term)
```

Convert Results to Data Frame

```{r message = FALSE, warning= FALSE}
df.term <- twListToDF(result.term)
write.csv(df.term, "Nezha.csv")
```

Can also seach using lat/lon

```{r message = FALSE, warning= FALSE}
result.latlon <- searchTwitter('nba', geocode='29.8174,-95.6814,20mi', n = 100)
head(result.latlon)
```

Identifying trending topics 

You can use TwitteR to identify what is currently “trending” on Twitter in a specific location by using Yahoo’s Where On Earth ID, or woeid. You can look at all places around the world that have a woeid by entering the following R script:

```{r message = FALSE, warning= FALSE}
availableTrendLocations()
```

You can also find the woeid for any places near a particular latitude-longitude coordinate pair. To find the woeid for New York City, you can enter the following R script:


So if I wanted to see what was trending in DC today:

```{r message = FALSE, warning= FALSE}
closestTrendLocations(38.9162204,-77.044123)
```

```{r message = FALSE, warning= FALSE}
dc <- getTrends(2514815)
head(dc,n = 10)
```

Get Info and Extract Tweets from a Specific User

```{r message = FALSE, warning= FALSE}
test_user <- getUser("christianmconro")
test_user$id
test_user$getDescription()
test_user$getFollowersCount()
# Fuck. I need more followers.
test_user$getFriends(n=5)

```

Use Time Line lets you get various timelines 
```{r message = FALSE, warning= FALSE}
userTimeline(user = "christianmconro", n = 5)

```

Creating a word cloud from tweets

Extract tweets 

```{r message = FALSE, warning= FALSE}
tweets <- searchTwitter("#nba", n=1000, lang="en")
tweets.text <- sapply(tweets, function(x) x$getText())

```

Clean Up Texts 

```{r message = FALSE, warning= FALSE}
# Replace blank space (“rt”)
tweets.text <- gsub("rt", "", tweets.text)
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
 
# #convert all text to lower case
tweets.text <- tolower(tweets.text)

# This is very useful for all the cleaning. 

# I had to manually add this one because there were still somoe problems with characters in the text 
# The only way to work around this problem was to remove all non graphical characters
tweets.text =str_replace_all(tweets.text,"[^[:graph:]]", " ") 
```

Remove Stop Words - No need to enter manual list. Just use tm. 

```{r message = FALSE, warning= FALSE}
library(tm)
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x,stopwords()))

```

Generate the actual word cloud

```{r message = FALSE, warning= FALSE}
library(wordcloud)
#generate wordcloud
wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
```

# Module 7: Text Processing and Sentiment Analysis of Twitter Data
## https://hackernoon.com/text-processing-and-sentiment-analysis-of-twitter-data-22ff5e51e14c


```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```

```{r message = FALSE, warning= FALSE}

```


