{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying BERT Multilingual Classifier to Predict Account Suspension \n",
    "\n",
    "Phase 2\n",
    "\n",
    "Guidance from: https://github.com/kacossio/TeamPython/blob/master/Bert%20Multilingual%20Embedding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Load Packages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "import emoji\n",
    "import random \n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters \n",
    "########## Set Parameters\n",
    "\n",
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "import_bucket = \"data/s3_data/processed_splits/\"\n",
    "\n",
    "embedding_bucket = \"data/s3_data/embeddings/iterate_embeds\"\n",
    "\n",
    "results_bucket = 'data/s3_data/full_clean_2/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load in Data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load in data from S3\n",
    "\n",
    "# Import Train and Measure Balance\n",
    "# Import Flattened Data\n",
    "df_train = pd.read_csv(import_bucket + \"x_train.csv\", error_bad_lines=False, encoding='utf-8')\n",
    "\n",
    "df_train['suspended'] = pd.to_numeric(df_train['suspended'], errors='coerce')\n",
    "df_train = df_train[df_train['suspended'].notna()]\n",
    "\n",
    "# Import Test and Measure Balance\n",
    "\n",
    "df_test = pd.read_csv(import_bucket + \"x_test.csv\", error_bad_lines=False, encoding='utf-8')\n",
    "\n",
    "df_test['suspended'] = pd.to_numeric(df_test['suspended'], errors='coerce')\n",
    "df_test = df_test[df_test['suspended'].notna()]\n",
    "\n",
    "# Import Validation and Measure Balance\n",
    "# Import Flattened Data\n",
    "df_valid = pd.read_csv(import_bucket + \"x_validation.csv\", error_bad_lines=False, encoding='utf-8')\n",
    "\n",
    "df_valid['suspended'] = pd.to_numeric(df_valid['suspended'], errors='coerce')\n",
    "df_valid = df_valid[df_valid['suspended'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that Target Variable is Numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['suspended'] = df_train['suspended'].astype(int)\n",
    "df_valid['suspended'] = df_valid['suspended'].astype(int)\n",
    "df_test['suspended'] = df_test['suspended'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 211283 Valid: 64625 Test 64921\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", len(df_train), \"Valid:\", len(df_valid), \"Test\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates(subset=['id', 'created_at', 'text'])\n",
    "df_valid = df_valid.drop_duplicates(subset=['id', 'created_at', 'text'])\n",
    "df_test = df_test.drop_duplicates(subset=['id', 'created_at', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure binary possibly_sensitive vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['possibly_sensitive'][df_train['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan\n",
    "df_valid['possibly_sensitive'][df_valid['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan\n",
    "df_test['possibly_sensitive'][df_test['possibly_sensitive'].apply(lambda x: isinstance(x, str))] =np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages \n",
    "from translate import Translator\n",
    "import spacy\n",
    "import langid\n",
    "import keras_bert\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime as dt\n",
    "import pytz\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Dates to Unix Epoch Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to convert dates into float (Unix Epoch Times )\n",
    "def convert_dates_float(df):\n",
    "    '''\n",
    "    Convert key input data variables to numeric format for tensors. Uses unix epoch time in seconds. \n",
    "    '''\n",
    "    # created_at (tweet)\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    my_datetime = dt.datetime(1970,1,1) \n",
    "    good_dt = pytz.timezone('UTC').localize(my_datetime)\n",
    "    df['created_at'] = (df['created_at'] - good_dt).dt.total_seconds()\n",
    "\n",
    "    # User.created_at (account)\n",
    "    df['user.created_at'] = pd.to_datetime(df['user.created_at'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    my_datetime = dt.datetime(1970,1,1) \n",
    "    good_dt = pytz.timezone('UTC').localize(my_datetime)\n",
    "    df['user.created_at'] = (df['user.created_at'] - good_dt).dt.total_seconds()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert binary and categorical variables to one-hot encoded (not sure this is best or not)\n",
    "\n",
    "Options \n",
    "\n",
    "- Integer Encoding: Where each unique label is mapped to an integer.\n",
    "- One Hot Encoding: Where each label is mapped to a binary vector.\n",
    "- Learned Embedding: Where a distributed representation of the categories is learned.\n",
    "\n",
    "We use one hot encoding below. \n",
    "\n",
    "#### We use get_dummies below instead of one_hot_encoder as get dummies knows how to deal with missingness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One Hote Encoding (Unix Epoch Times )\n",
    "def one_hot(df_train, df_valid, df_test): \n",
    "    '''\n",
    "    One hot encoding requires the full dataset in order to ensure that there end up the same amount of columns for test, validation and train.\n",
    "    We therefore combine train, valid, and test, fill nas with 0 where necessary, and one hot encode categorical vars. \n",
    "    '''\n",
    "    df_train['split'] = \"train\"\n",
    "    df_valid['split'] = \"valid\"\n",
    "    df_test['split'] = \"test\"\n",
    "    df = pd.concat([df_train, df_test, df_valid], ignore_index=True, sort=False)\n",
    "    df = convert_dates_float(df)\n",
    "    # Extra layer of Processing \n",
    "    df = df[df['retweet_count'] != \"False\"] \n",
    "    df['quoted_status.user.followers_count'] = df['quoted_status.user.followers_count'].fillna(0) \n",
    "    df['quoted_status.user.friends_count'] = df['quoted_status.user.friends_count'].fillna(0) \n",
    "    df['retweeted_status.user.followers_count'] = df['retweeted_status.user.followers_count'].fillna(0) \n",
    "    df['retweeted_status.user.friends_count'] = df['retweeted_status.user.friends_count'].fillna(0) \n",
    "    # One-hot\n",
    "    df = df.drop([\"user.protected.1\"], axis=1)\n",
    "    df = pd.get_dummies(df, columns=[\"source\", \"lang\", \"possibly_sensitive\", \"withheld_in_countries\", \"place.country\", \n",
    "                                         \"user.geo_enabled\", \"user.lang\", \"user.verified\", \"user.has_extended_profile\",\n",
    "                                        \"user.lang\", \"user.protected\", \"user.time_zone\", \"user.verified\", \"user.default_profile\",\n",
    "                                        \"is_quote_status\"])\n",
    "    return df\n",
    "# Tp get rid of: Text, user.protected.1, user.protected.2, user.protected.3, \n",
    "# To concat (or get rid of): user.description, user.location, user.name, user.screen_name\n",
    "# to potentially take out entirely - user.id (This would explain everything)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split one-hot encoded df back apart into train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot(df_train, df_valid, df_test)\n",
    "df_train_f = df[df['split'] == \"train\"]\n",
    "df_valid_f = df[df['split'] == \"valid\"]\n",
    "df_test_f = df[df['split'] == \"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f = df_train_f.drop_duplicates(subset=['id'])\n",
    "df_valid_f = df_valid_f.drop_duplicates(subset=['id'])\n",
    "df_test_f = df_test_f.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train: 115766 unique ids train: 115766 full valid: 35120 unique ids valid: 35120 full test: 34839 unique ids test: 34839\n"
     ]
    }
   ],
   "source": [
    "print(\"full train:\", len(df_train_f), \"unique ids train:\", len(df_train_f['id'].unique()), \n",
    "      \"full valid:\", len(df_valid_f), \"unique ids valid:\", len(df_valid_f['id'].unique()), \n",
    "      \"full test:\", len(df_test_f), \"unique ids test:\", len(df_test_f['id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f.to_csv(results_bucket + 'df_train_full_cleaned_pe.csv', index=False, encoding = \"utf_8_sig\")\n",
    "df_test_f.to_csv(results_bucket + 'df_test_full_cleaned_pe.csv', index=False, encoding = \"utf_8_sig\")\n",
    "df_valid_f.to_csv(results_bucket + 'df_valid_full_cleaned_pe.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
