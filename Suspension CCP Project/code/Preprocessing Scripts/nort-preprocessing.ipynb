{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012821,
     "end_time": "2021-04-23T20:21:11.367634",
     "exception": false,
     "start_time": "2021-04-23T20:21:11.354813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:11.394442Z",
     "iopub.status.busy": "2021-04-23T20:21:11.393761Z",
     "iopub.status.idle": "2021-04-23T20:21:12.626404Z",
     "shell.execute_reply": "2021-04-23T20:21:12.627079Z"
    },
    "papermill": {
     "duration": 1.24826,
     "end_time": "2021-04-23T20:21:12.627492",
     "exception": false,
     "start_time": "2021-04-23T20:21:11.379232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "import emoji\n",
    "import random \n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011539,
     "end_time": "2021-04-23T20:21:12.652439",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.640900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Set Relevant Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.680210Z",
     "iopub.status.busy": "2021-04-23T20:21:12.679514Z",
     "iopub.status.idle": "2021-04-23T20:21:12.684166Z",
     "shell.execute_reply": "2021-04-23T20:21:12.684779Z"
    },
    "papermill": {
     "duration": 0.020727,
     "end_time": "2021-04-23T20:21:12.684982",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.664255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "original_data = \"/kaggle/input/processed-sus/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011781,
     "end_time": "2021-04-23T20:21:12.708887",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.697106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.737665Z",
     "iopub.status.busy": "2021-04-23T20:21:12.736872Z",
     "iopub.status.idle": "2021-04-23T20:21:12.766527Z",
     "shell.execute_reply": "2021-04-23T20:21:12.767123Z"
    },
    "papermill": {
     "duration": 0.046214,
     "end_time": "2021-04-23T20:21:12.767394",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.721180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = listdir(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011556,
     "end_time": "2021-04-23T20:21:12.791232",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.779676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For testrun subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.819044Z",
     "iopub.status.busy": "2021-04-23T20:21:12.818392Z",
     "iopub.status.idle": "2021-04-23T20:21:12.823266Z",
     "shell.execute_reply": "2021-04-23T20:21:12.822760Z"
    },
    "papermill": {
     "duration": 0.020249,
     "end_time": "2021-04-23T20:21:12.823457",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.803208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#onlyfiles = onlyfiles[:2]\n",
    "#onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.864638Z",
     "iopub.status.busy": "2021-04-23T20:21:12.863633Z",
     "iopub.status.idle": "2021-04-23T20:21:12.866428Z",
     "shell.execute_reply": "2021-04-23T20:21:12.866980Z"
    },
    "papermill": {
     "duration": 0.031774,
     "end_time": "2021-04-23T20:21:12.867178",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.835404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_data(folder, filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe. Concatenates multiple files if necessary. \n",
    "    Inputs: Directory path, number of rows to skip\n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "    fields = ['created_at', 'text', 'source', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive',\n",
    "                      'withheld_in_countries', 'place.country', 'quoted_status_id', 'user.created_at',\n",
    "                      'user.description', 'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
    "                       'user.geo_enabled', 'user.has_extended_profile', 'user.lang', 'user.listed_count',\n",
    "                       'user.location', 'user.name', 'user.protected', 'user.screen_name',\n",
    "                       'user.time_zone', 'user.verified', 'user.protected', 'user.default_profile',\n",
    "                       'is_quote_status', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count',\n",
    "                       'retweeted_status.user.followers_count', 'retweeted_status.id', 'retweeted_status.user.friends_count', 'user.url',\n",
    "                       'in_reply_to_status_id', 'id_str', 'user.id', 'suspended', 'user.statuses_count', 'id']\n",
    "    df_frame = []\n",
    "    # Identify if directory or file path was provided\n",
    "    # Iterate through files of the directory\n",
    "    for filename in filelist:\n",
    "        dataframe = pd.read_csv(folder + filename, error_bad_lines=False, encoding='utf-8', usecols=fields, low_memory = True)\n",
    "        # Put in string format\n",
    "        dataframe['id'] = dataframe['id'].astype('str')\n",
    "        dataframe['id'] = dataframe['id'].apply(lambda x: \"'\" + x + \"'\")\n",
    "        dataframe = dataframe.drop_duplicates(subset=['id'])\n",
    "\n",
    "        df_frame.append(dataframe)\n",
    "\n",
    "    df = pd.concat(df_frame, ignore_index=True, sort=False)\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    # Clean up rows for any irregularities  \n",
    "    df = df[df['created_at'] != \"False\"]\n",
    "    df = df[df['created_at'] != \"created_at\"]\n",
    "    df['user.id'] = pd.to_numeric(df['user.id'], errors='coerce')\n",
    "    df = df[df['user.id'].isnull() != True]\n",
    "    \n",
    "    # Return ID to float format \n",
    "    df['id'] = df['id'].apply(lambda x: x.replace(\"'\", ''))\n",
    "    df['id'] = df['id'].astype('float')\n",
    "\n",
    "    # Format dates \n",
    "    df[\"user.created_at\"] = pd.to_datetime(df[\"user.created_at\"], format='%a %b %d %H:%M:%S %z %Y')\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], format='%Y-%m-%d %H:%M:%S').dt.tz_localize('UTC')\n",
    "    \n",
    "    df = df[df['retweeted_status.id'].isna()]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.896343Z",
     "iopub.status.busy": "2021-04-23T20:21:12.895302Z",
     "iopub.status.idle": "2021-04-23T20:37:21.609969Z",
     "shell.execute_reply": "2021-04-23T20:37:21.610500Z"
    },
    "papermill": {
     "duration": 968.730962,
     "end_time": "2021-04-23T20:37:21.610699",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.879737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>...</th>\n",
       "      <th>user.listed_count</th>\n",
       "      <th>user.location</th>\n",
       "      <th>user.name</th>\n",
       "      <th>user.protected</th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>user.statuses_count</th>\n",
       "      <th>user.time_zone</th>\n",
       "      <th>user.url</th>\n",
       "      <th>user.verified</th>\n",
       "      <th>suspended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-27 07:20:09+00:00</td>\n",
       "      <td>1.287649e+18</td>\n",
       "      <td>1287648959603695616.0</td>\n",
       "      <td>@RFS_China çªæ‰ªç»ˆé”…éœ‡çš„æ˜¯èƒğŸå®³ğŸŒ¶</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1287643146071941120.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zh</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æˆ˜å¿½å±€é’“é±¼éƒ¨å…¼åé¼ éƒ¨å¸ä»¤</td>\n",
       "      <td>False</td>\n",
       "      <td>SFAC_TFA_CN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-27 07:16:49+00:00</td>\n",
       "      <td>1.287648e+18</td>\n",
       "      <td>1287648119421714432.0</td>\n",
       "      <td>@Anoz1379 @pluckesualive å¥½å‰å®³å•Šï¼Œéª‚å®Œäººå°±Bï¼ŒğŸ‘´åˆšæƒ³æ€¼å›å»å°±ç»™Bäº†</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1287595079473848320.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zh</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æˆ˜å¿½å±€é’“é±¼éƒ¨å…¼åé¼ éƒ¨å¸ä»¤</td>\n",
       "      <td>False</td>\n",
       "      <td>SFAC_TFA_CN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-27 03:46:03+00:00</td>\n",
       "      <td>1.287595e+18</td>\n",
       "      <td>1287595079473848320.0</td>\n",
       "      <td>@Anoz1379 @pluckesualive çœ‹æ¥ä½ å¦ˆåº”è¯¥æ˜¯æ²¡æŒ¡ä½</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1286218642380660736.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zh</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>æˆ˜å¿½å±€é’“é±¼éƒ¨å…¼åé¼ éƒ¨å¸ä»¤</td>\n",
       "      <td>False</td>\n",
       "      <td>SFAC_TFA_CN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at            id                 id_str  \\\n",
       "0 2020-07-27 07:20:09+00:00  1.287649e+18  1287648959603695616.0   \n",
       "1 2020-07-27 07:16:49+00:00  1.287648e+18  1287648119421714432.0   \n",
       "2 2020-07-27 03:46:03+00:00  1.287595e+18  1287595079473848320.0   \n",
       "\n",
       "                                             text              source  \\\n",
       "0                          @RFS_China çªæ‰ªç»ˆé”…éœ‡çš„æ˜¯èƒğŸå®³ğŸŒ¶     Twitter Web App   \n",
       "1  @Anoz1379 @pluckesualive å¥½å‰å®³å•Šï¼Œéª‚å®Œäººå°±Bï¼ŒğŸ‘´åˆšæƒ³æ€¼å›å»å°±ç»™Bäº†     Twitter Web App   \n",
       "2             @Anoz1379 @pluckesualive çœ‹æ¥ä½ å¦ˆåº”è¯¥æ˜¯æ²¡æŒ¡ä½  Twitter for iPhone   \n",
       "\n",
       "   in_reply_to_status_id is_quote_status retweet_count favorite_count lang  \\\n",
       "0  1287643146071941120.0           False           0.0            1.0   zh   \n",
       "1  1287595079473848320.0           False           0.0            0.0   zh   \n",
       "2  1286218642380660736.0           False           0.0            0.0   zh   \n",
       "\n",
       "   ... user.listed_count user.location     user.name user.protected  \\\n",
       "0  ...               0.0           NaN  æˆ˜å¿½å±€é’“é±¼éƒ¨å…¼åé¼ éƒ¨å¸ä»¤          False   \n",
       "1  ...               0.0           NaN  æˆ˜å¿½å±€é’“é±¼éƒ¨å…¼åé¼ éƒ¨å¸ä»¤          False   \n",
       "2  ...               0.0           NaN  æˆ˜å¿½å±€é’“é±¼éƒ¨å…¼åé¼ éƒ¨å¸ä»¤          False   \n",
       "\n",
       "  user.screen_name user.statuses_count user.time_zone user.url user.verified  \\\n",
       "0      SFAC_TFA_CN               243.0            NaN      NaN         False   \n",
       "1      SFAC_TFA_CN               243.0            NaN      NaN         False   \n",
       "2      SFAC_TFA_CN               243.0            NaN      NaN         False   \n",
       "\n",
       "  suspended  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = import_data(original_data, onlyfiles)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012554,
     "end_time": "2021-04-23T20:37:21.635821",
     "exception": false,
     "start_time": "2021-04-23T20:37:21.623267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Create Train, Test, Valid Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:21.677466Z",
     "iopub.status.busy": "2021-04-23T20:37:21.676659Z",
     "iopub.status.idle": "2021-04-23T20:37:23.526688Z",
     "shell.execute_reply": "2021-04-23T20:37:23.525928Z"
    },
    "papermill": {
     "duration": 1.878324,
     "end_time": "2021-04-23T20:37:23.526859",
     "exception": false,
     "start_time": "2021-04-23T20:37:21.648535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## 4. Split into Train vs. Valid vs. Test (Note particular method due to panel data)\n",
    "#- Test: 20%\n",
    "#- Train: 60%\n",
    "#- Validation: 20%\n",
    "\n",
    "# https://towardsdatascience.com/assigning-panel-data-to-training-testing-and-validation-groups-for-machine-learning-models-7017350ab86e\n",
    "# Here is a few lines of python code to that ensure that your training, testing and validation groups are independent.\n",
    "# Get a Unique List of All IDs (machines).\n",
    "pd_id=df.drop_duplicates(subset='user.id')\n",
    "pd_id=pd_id[['user.id']]\n",
    "\n",
    "# Create a new variable with a random number between 0 and .\n",
    "np.random.seed(42)\n",
    "pd_id['wookie'] = (np.random.randint(0, 10000, pd_id.shape[0]))/10000\n",
    "pd_id=pd_id[['user.id', 'wookie']]\n",
    "\n",
    "#Give each machine a 20% chance of being in the validation, \n",
    "#a 20% chance of being in the testing and a \n",
    "# 60% chance of being in the training data set.\n",
    "# Split into Train vs. Valid vs. Test\n",
    "#- Test: 20%\n",
    "#- Train: 60%\n",
    "#- Validation: 20%\n",
    "pd_id['MODELING_GROUP'] = np.where(((pd_id.wookie <= 0.60)), 'TRAINING', np.where(((pd_id.wookie <= 0.80)), 'VALIDATION', 'TESTING'))\n",
    "\n",
    "tips_summed = pd_id.groupby(['MODELING_GROUP'])['wookie'].count()\n",
    "\n",
    "# Append the Group of each id to each individual record.\n",
    "df=df.sort_values(by=['user.id'], ascending=[True])\n",
    "\n",
    "pd_id=pd_id.sort_values(by=['user.id'], ascending=[True])\n",
    "df = df.merge(pd_id, on=['user.id'], how='inner')\n",
    "\n",
    "#Train\n",
    "X_train = df[df['MODELING_GROUP'] == 'TRAINING']\n",
    "X_train = X_train.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_train = df[df['MODELING_GROUP'] == 'TRAINING'].suspended\n",
    "\n",
    "# Validation\n",
    "X_valid = df[df['MODELING_GROUP'] == 'VALIDATION']\n",
    "X_valid = X_valid.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_valid = df[df['MODELING_GROUP'] == 'VALIDATION'].suspended\n",
    "\n",
    "# Test\n",
    "X_test = df[df['MODELING_GROUP'] == 'TESTING']\n",
    "X_test = X_test.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_test = df[df['MODELING_GROUP'] == 'TESTING'].suspended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.559783Z",
     "iopub.status.busy": "2021-04-23T20:37:23.559062Z",
     "iopub.status.idle": "2021-04-23T20:37:23.563974Z",
     "shell.execute_reply": "2021-04-23T20:37:23.564443Z"
    },
    "papermill": {
     "duration": 0.024614,
     "end_time": "2021-04-23T20:37:23.564643",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.540029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train: 149932 unique ids train: 149932 full valid: 51994 unique ids valid: 51994 full test: 38025 unique ids test: 38025\n"
     ]
    }
   ],
   "source": [
    "print(\"full train:\", len(X_train), \"unique ids train:\", len(y_train), \n",
    "      \"full valid:\", len(X_valid), \"unique ids valid:\", len(y_valid), \n",
    "      \"full test:\", len(X_test), \"unique ids test:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01313,
     "end_time": "2021-04-23T20:37:23.593516",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.580386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.640131Z",
     "iopub.status.busy": "2021-04-23T20:37:23.634779Z",
     "iopub.status.idle": "2021-04-23T20:37:23.663043Z",
     "shell.execute_reply": "2021-04-23T20:37:23.662427Z"
    },
    "papermill": {
     "duration": 0.056187,
     "end_time": "2021-04-23T20:37:23.663202",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.607015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tweets_per_minute = True): # no *args or **kargs\n",
    "        self.tweets_per_minute = tweets_per_minute\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        X_train_2 = X.copy()\n",
    "        # User Age (tweet created_at - account created_at)\n",
    "        X_train_2[\"user_age\"] = (X_train_2[\"created_at\"] - X_train_2[\"user.created_at\"]).dt.days\n",
    "        # Tweets per day \n",
    "        X_train_2['tweets_per_day'] = X_train_2['user.statuses_count'].astype(float)/X_train_2[\"user_age\"] \n",
    "        # Calculate time since last tweet\n",
    "        X_train_2[\"since_last_tweet_mins\"] = X_train_2.sort_values(['user.id','created_at']).groupby('user.id')['created_at'].diff().dt.seconds.div(60)\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].min().to_frame('since_last_tweet_mins_min'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].max().to_frame('since_last_tweet_mins_max'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].mean().to_frame('since_last_tweet_mins_mean'), on = [\"user.id\"])        \n",
    "        \n",
    "        X_train_2['date'] = X_train_2['created_at'].dt.date\n",
    "        X_train_2['hour'] = X_train_2['created_at'].dt.hour\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2[['user.id', 'date', 'hour', 'id']].groupby(['user.id', 'date', 'hour']).count().groupby('user.id', sort=False)[\"id\"].mean().reset_index(name ='avg_tweets_per_hr'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2[['user.id', 'date', 'id']].groupby(['user.id', 'date']).count().groupby('user.id', sort=False)[\"id\"].mean().reset_index(name ='avg_tweets_per_day'), on = [\"user.id\"])\n",
    "\n",
    "        X_train_2.loc[X_train_2['quoted_status_id'].notna(), 'quoted_status_id'] = 1\n",
    "        X_train_2.loc[X_train_2['quoted_status_id'].isna(), 'quoted_status_id'] = 0\n",
    "        X_train_2['no_hashtags'] = X_train_2['text'].apply(lambda x: len(re.findall(r\"#(\\w+)\", x)))\n",
    "        X_train_2['no_mentions'] = X_train_2['text'].apply(lambda x: len(re.findall(\"@(\\w{1,15})\", x)))\n",
    "        X_train_2['no_urls'] = X_train_2['text'].apply(lambda x: len(re.findall(\"(?P<url>https?://[^\\s]+)\", x)))\n",
    "        X_train_2['tw_len'] = X_train_2['text'].apply(lambda x: len(x))\n",
    "        X_train_2['followers_per_followees'] = X_train_2['user.followers_count'].astype('float')/X_train_2['user.friends_count'].astype('float')\n",
    "\n",
    "        # URLs (percent of tweets with them)\n",
    "        X_train_2[\"containsURL\"] = (X_train_2['no_urls']  > 0).astype(int)\n",
    "        url_counts = X_train_2.groupby('user.id').agg({'created_at':'count', \n",
    "                         'containsURL':'sum'})\n",
    "        url_counts['user.urls_per_tweet'] = url_counts['containsURL']/url_counts['created_at']\n",
    "        X_train_2 = pd.merge(X_train_2, url_counts[['user.urls_per_tweet']], on = [\"user.id\"])  \n",
    "\n",
    "        # Hashtags, Mentions, and URLS\n",
    "        url_counts = X_train_2.groupby('user.id').agg({'created_at':'count', \n",
    "                 'no_hashtags':'sum', 'no_mentions':'sum', 'no_urls':'sum'})\n",
    "        url_counts['no_hashtags_per_tweet'] = url_counts['no_hashtags']/url_counts['created_at']\n",
    "        url_counts['no_mentions_per_tweet'] = url_counts['no_mentions']/url_counts['created_at']\n",
    "        url_counts['no_urls_per_tweet'] = url_counts['no_urls']/url_counts['created_at']\n",
    "        X_train_2 = pd.merge(X_train_2, url_counts[['no_hashtags_per_tweet', 'no_mentions_per_tweet', 'no_urls_per_tweet']], on = [\"user.id\"])  \n",
    "\n",
    "        X_train_2['user.followers_count'] = X_train_2['user.followers_count'].astype('float')\n",
    "        X_train_2['user.friends_count'] = X_train_2['user.friends_count'].astype('float')\n",
    "\n",
    "        # Pace of follower and friend add-on during collected time period \n",
    "        avg_friends_per_day = X_train_2.groupby(['user.id', 'date'], as_index=True).mean()[['user.followers_count', 'user.friends_count']]\n",
    "        avg_friends_change = avg_friends_per_day.sort_values(['user.id','date']).groupby('user.id').diff().rename(columns={'user.followers_count':'user.followers_countdailychange','user.friends_count' : 'user.friends_countdailychange'})\n",
    "        X_train_2 = pd.merge(X_train_2, avg_friends_change.groupby(['user.id'], as_index=True)[['user.followers_countdailychange', 'user.friends_countdailychange']].mean(), on = [\"user.id\"]) \n",
    "\n",
    "        # Pace of follower and friend add-on overall \n",
    "        X_train_2['user.friend_rate'] = X_train_2['user.friends_count']/X_train_2['user_age']\n",
    "        X_train_2['user.followers_rate'] = X_train_2['user.followers_count']/X_train_2['user_age']\n",
    "\n",
    "        X_train_2['user.has_url'] = (X_train_2['user.url'].fillna(False) != False).astype(int)\n",
    "        X_train_2['user.has_location'] = (X_train_2['user.location'].fillna(False) != False).astype(int)\n",
    "        X_train_2['user.screen_name.digit_length'] = X_train_2['user.screen_name'].apply(lambda x: len(re.sub(\"[^0-9]\", \"\", x)) if pd.notnull(x) else x)\n",
    "        X_train_2['user.screen_name.length'] = X_train_2['user.screen_name'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in tweet\n",
    "        #X_train_2['text'] = X_train_2['text'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in bio \n",
    "        X_train_2['user.description'] = X_train_2['user.description'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in name\n",
    "        X_train_2['user.name'] = X_train_2['user.name'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Create binary for whether it is reply or not \n",
    "        X_train_2['is_reply'] = (X_train_2['in_reply_to_status_id'].fillna(False) != False).astype(int)\n",
    "        \n",
    "        # Reset IDs for transferrability \n",
    "        X_train_2['id'] = X_train_2['id'].astype('str')\n",
    "        X_train_2['id'] = X_train_2['id'].apply(lambda x: \"'\" + x + \"'\")\n",
    "\n",
    "        X_train_2 = X_train_2[['id','created_at', 'text', 'source', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive',\n",
    "                              'withheld_in_countries', 'place.country', 'quoted_status_id', 'user.id', 'user.created_at',\n",
    "                              'user.description', 'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
    "                               'user.geo_enabled', 'user.has_extended_profile', 'user.lang', 'user.listed_count',\n",
    "                               'user.location', 'user.name', 'user.protected', 'user.screen_name', 'user.statuses_count',\n",
    "                               'user.time_zone', 'user.verified', 'user.protected', 'user.default_profile',\n",
    "                               'is_quote_status', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count',\n",
    "                               'retweeted_status.user.followers_count', 'retweeted_status.user.friends_count', 'user_age', \n",
    "                              'tweets_per_day', 'since_last_tweet_mins', 'since_last_tweet_mins_min',\n",
    "                               'since_last_tweet_mins_max', 'since_last_tweet_mins_mean', 'avg_tweets_per_hr', \n",
    "                               'avg_tweets_per_day', 'no_hashtags', 'no_mentions', 'no_urls', 'tw_len',\n",
    "                              'followers_per_followees', 'containsURL',  'user.urls_per_tweet', 'no_hashtags_per_tweet',\n",
    "                              'no_mentions_per_tweet', 'no_urls_per_tweet', 'user.followers_countdailychange', \n",
    "                               'user.friends_countdailychange', 'user.friend_rate', 'user.followers_rate',\n",
    "                              'user.has_url', 'user.has_location', 'user.screen_name.digit_length', \n",
    "                               'user.screen_name.length', 'is_reply',  'suspended']]\n",
    "        \n",
    "        return X_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013292,
     "end_time": "2021-04-23T20:37:23.690405",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.677113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Run Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.726396Z",
     "iopub.status.busy": "2021-04-23T20:37:23.725637Z",
     "iopub.status.idle": "2021-04-23T20:37:23.729174Z",
     "shell.execute_reply": "2021-04-23T20:37:23.728529Z"
    },
    "papermill": {
     "duration": 0.025295,
     "end_time": "2021-04-23T20:37:23.729365",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.704070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline([\n",
    "        #('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder())\n",
    "        #('std_scaler', StandardScaler()), # feature scaling\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013397,
     "end_time": "2021-04-23T20:37:23.756951",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.743554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Run and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.792797Z",
     "iopub.status.busy": "2021-04-23T20:37:23.791762Z",
     "iopub.status.idle": "2021-04-23T20:38:52.898068Z",
     "shell.execute_reply": "2021-04-23T20:38:52.898831Z"
    },
    "papermill": {
     "duration": 89.128125,
     "end_time": "2021-04-23T20:38:52.899072",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.770947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train_tr = test_pipeline.fit_transform(X_train)\n",
    "\n",
    "X_train_tr.to_csv('x_train.csv', index=False, encoding = \"utf_8_sig\")\n",
    "y_train.to_csv('y_train.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "# Test\n",
    "X_train_tr = test_pipeline.fit_transform(X_test)\n",
    "\n",
    "X_train_tr.to_csv('x_test.csv', index=False, encoding = \"utf_8_sig\")\n",
    "y_test.to_csv('y_test.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "# Valid \n",
    "X_train_tr = test_pipeline.fit_transform(X_valid)\n",
    "\n",
    "X_train_tr.to_csv('x_validation.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "y_valid.to_csv('y_validation.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1069.024119,
   "end_time": "2021-04-23T20:38:54.429933",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-23T20:21:05.405814",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
