{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012821,
     "end_time": "2021-04-23T20:21:11.367634",
     "exception": false,
     "start_time": "2021-04-23T20:21:11.354813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:11.394442Z",
     "iopub.status.busy": "2021-04-23T20:21:11.393761Z",
     "iopub.status.idle": "2021-04-23T20:21:12.626404Z",
     "shell.execute_reply": "2021-04-23T20:21:12.627079Z"
    },
    "papermill": {
     "duration": 1.24826,
     "end_time": "2021-04-23T20:21:12.627492",
     "exception": false,
     "start_time": "2021-04-23T20:21:11.379232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "import emoji\n",
    "import random \n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011539,
     "end_time": "2021-04-23T20:21:12.652439",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.640900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Set Relevant Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.680210Z",
     "iopub.status.busy": "2021-04-23T20:21:12.679514Z",
     "iopub.status.idle": "2021-04-23T20:21:12.684166Z",
     "shell.execute_reply": "2021-04-23T20:21:12.684779Z"
    },
    "papermill": {
     "duration": 0.020727,
     "end_time": "2021-04-23T20:21:12.684982",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.664255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "original_data = \"/kaggle/input/processed-sus/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011781,
     "end_time": "2021-04-23T20:21:12.708887",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.697106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.737665Z",
     "iopub.status.busy": "2021-04-23T20:21:12.736872Z",
     "iopub.status.idle": "2021-04-23T20:21:12.766527Z",
     "shell.execute_reply": "2021-04-23T20:21:12.767123Z"
    },
    "papermill": {
     "duration": 0.046214,
     "end_time": "2021-04-23T20:21:12.767394",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.721180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = listdir(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011556,
     "end_time": "2021-04-23T20:21:12.791232",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.779676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For testrun subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.819044Z",
     "iopub.status.busy": "2021-04-23T20:21:12.818392Z",
     "iopub.status.idle": "2021-04-23T20:21:12.823266Z",
     "shell.execute_reply": "2021-04-23T20:21:12.822760Z"
    },
    "papermill": {
     "duration": 0.020249,
     "end_time": "2021-04-23T20:21:12.823457",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.803208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#onlyfiles = onlyfiles[:2]\n",
    "#onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.864638Z",
     "iopub.status.busy": "2021-04-23T20:21:12.863633Z",
     "iopub.status.idle": "2021-04-23T20:21:12.866428Z",
     "shell.execute_reply": "2021-04-23T20:21:12.866980Z"
    },
    "papermill": {
     "duration": 0.031774,
     "end_time": "2021-04-23T20:21:12.867178",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.835404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_data(folder, filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe. Concatenates multiple files if necessary. \n",
    "    Inputs: Directory path, number of rows to skip\n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "    fields = ['created_at', 'text', 'source', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive',\n",
    "                      'withheld_in_countries', 'place.country', 'quoted_status_id', 'user.created_at',\n",
    "                      'user.description', 'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
    "                       'user.geo_enabled', 'user.has_extended_profile', 'user.lang', 'user.listed_count',\n",
    "                       'user.location', 'user.name', 'user.protected', 'user.screen_name',\n",
    "                       'user.time_zone', 'user.verified', 'user.protected', 'user.default_profile',\n",
    "                       'is_quote_status', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count',\n",
    "                       'retweeted_status.user.followers_count', 'retweeted_status.id', 'retweeted_status.user.friends_count', 'user.url',\n",
    "                       'in_reply_to_status_id', 'id_str', 'user.id', 'suspended', 'user.statuses_count', 'id']\n",
    "    df_frame = []\n",
    "    # Identify if directory or file path was provided\n",
    "    # Iterate through files of the directory\n",
    "    for filename in filelist:\n",
    "        dataframe = pd.read_csv(folder + filename, error_bad_lines=False, encoding='utf-8', usecols=fields, low_memory = True)\n",
    "        # Put in string format\n",
    "        dataframe['id'] = dataframe['id'].astype('str')\n",
    "        dataframe['id'] = dataframe['id'].apply(lambda x: \"'\" + x + \"'\")\n",
    "        dataframe = dataframe.drop_duplicates(subset=['id'])\n",
    "\n",
    "        df_frame.append(dataframe)\n",
    "\n",
    "    df = pd.concat(df_frame, ignore_index=True, sort=False)\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    # Clean up rows for any irregularities  \n",
    "    df = df[df['created_at'] != \"False\"]\n",
    "    df = df[df['created_at'] != \"created_at\"]\n",
    "    df['user.id'] = pd.to_numeric(df['user.id'], errors='coerce')\n",
    "    df = df[df['user.id'].isnull() != True]\n",
    "    \n",
    "    # Return ID to float format \n",
    "    df['id'] = df['id'].apply(lambda x: x.replace(\"'\", ''))\n",
    "    df['id'] = df['id'].astype('float')\n",
    "\n",
    "    # Format dates \n",
    "    df[\"user.created_at\"] = pd.to_datetime(df[\"user.created_at\"], format='%a %b %d %H:%M:%S %z %Y')\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], format='%Y-%m-%d %H:%M:%S').dt.tz_localize('UTC')\n",
    "    \n",
    "    df = df[df['retweeted_status.id'].isna()]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:21:12.896343Z",
     "iopub.status.busy": "2021-04-23T20:21:12.895302Z",
     "iopub.status.idle": "2021-04-23T20:37:21.609969Z",
     "shell.execute_reply": "2021-04-23T20:37:21.610500Z"
    },
    "papermill": {
     "duration": 968.730962,
     "end_time": "2021-04-23T20:37:21.610699",
     "exception": false,
     "start_time": "2021-04-23T20:21:12.879737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>...</th>\n",
       "      <th>user.listed_count</th>\n",
       "      <th>user.location</th>\n",
       "      <th>user.name</th>\n",
       "      <th>user.protected</th>\n",
       "      <th>user.screen_name</th>\n",
       "      <th>user.statuses_count</th>\n",
       "      <th>user.time_zone</th>\n",
       "      <th>user.url</th>\n",
       "      <th>user.verified</th>\n",
       "      <th>suspended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-27 07:20:09+00:00</td>\n",
       "      <td>1.287649e+18</td>\n",
       "      <td>1287648959603695616.0</td>\n",
       "      <td>@RFS_China 窝扪终锅震的是胎🍐害🌶</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1287643146071941120.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zh</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>战忽局钓鱼部兼反鼠部司令</td>\n",
       "      <td>False</td>\n",
       "      <td>SFAC_TFA_CN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-27 07:16:49+00:00</td>\n",
       "      <td>1.287648e+18</td>\n",
       "      <td>1287648119421714432.0</td>\n",
       "      <td>@Anoz1379 @pluckesualive 好厉害啊，骂完人就B，👴刚想怼回去就给B了</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1287595079473848320.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zh</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>战忽局钓鱼部兼反鼠部司令</td>\n",
       "      <td>False</td>\n",
       "      <td>SFAC_TFA_CN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-27 03:46:03+00:00</td>\n",
       "      <td>1.287595e+18</td>\n",
       "      <td>1287595079473848320.0</td>\n",
       "      <td>@Anoz1379 @pluckesualive 看来你妈应该是没挡住</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>1286218642380660736.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>zh</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>战忽局钓鱼部兼反鼠部司令</td>\n",
       "      <td>False</td>\n",
       "      <td>SFAC_TFA_CN</td>\n",
       "      <td>243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at            id                 id_str  \\\n",
       "0 2020-07-27 07:20:09+00:00  1.287649e+18  1287648959603695616.0   \n",
       "1 2020-07-27 07:16:49+00:00  1.287648e+18  1287648119421714432.0   \n",
       "2 2020-07-27 03:46:03+00:00  1.287595e+18  1287595079473848320.0   \n",
       "\n",
       "                                             text              source  \\\n",
       "0                          @RFS_China 窝扪终锅震的是胎🍐害🌶     Twitter Web App   \n",
       "1  @Anoz1379 @pluckesualive 好厉害啊，骂完人就B，👴刚想怼回去就给B了     Twitter Web App   \n",
       "2             @Anoz1379 @pluckesualive 看来你妈应该是没挡住  Twitter for iPhone   \n",
       "\n",
       "   in_reply_to_status_id is_quote_status retweet_count favorite_count lang  \\\n",
       "0  1287643146071941120.0           False           0.0            1.0   zh   \n",
       "1  1287595079473848320.0           False           0.0            0.0   zh   \n",
       "2  1286218642380660736.0           False           0.0            0.0   zh   \n",
       "\n",
       "   ... user.listed_count user.location     user.name user.protected  \\\n",
       "0  ...               0.0           NaN  战忽局钓鱼部兼反鼠部司令          False   \n",
       "1  ...               0.0           NaN  战忽局钓鱼部兼反鼠部司令          False   \n",
       "2  ...               0.0           NaN  战忽局钓鱼部兼反鼠部司令          False   \n",
       "\n",
       "  user.screen_name user.statuses_count user.time_zone user.url user.verified  \\\n",
       "0      SFAC_TFA_CN               243.0            NaN      NaN         False   \n",
       "1      SFAC_TFA_CN               243.0            NaN      NaN         False   \n",
       "2      SFAC_TFA_CN               243.0            NaN      NaN         False   \n",
       "\n",
       "  suspended  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = import_data(original_data, onlyfiles)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012554,
     "end_time": "2021-04-23T20:37:21.635821",
     "exception": false,
     "start_time": "2021-04-23T20:37:21.623267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Create Train, Test, Valid Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:21.677466Z",
     "iopub.status.busy": "2021-04-23T20:37:21.676659Z",
     "iopub.status.idle": "2021-04-23T20:37:23.526688Z",
     "shell.execute_reply": "2021-04-23T20:37:23.525928Z"
    },
    "papermill": {
     "duration": 1.878324,
     "end_time": "2021-04-23T20:37:23.526859",
     "exception": false,
     "start_time": "2021-04-23T20:37:21.648535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## 4. Split into Train vs. Valid vs. Test (Note particular method due to panel data)\n",
    "#- Test: 20%\n",
    "#- Train: 60%\n",
    "#- Validation: 20%\n",
    "\n",
    "# https://towardsdatascience.com/assigning-panel-data-to-training-testing-and-validation-groups-for-machine-learning-models-7017350ab86e\n",
    "# Here is a few lines of python code to that ensure that your training, testing and validation groups are independent.\n",
    "# Get a Unique List of All IDs (machines).\n",
    "pd_id=df.drop_duplicates(subset='user.id')\n",
    "pd_id=pd_id[['user.id']]\n",
    "\n",
    "# Create a new variable with a random number between 0 and .\n",
    "np.random.seed(42)\n",
    "pd_id['wookie'] = (np.random.randint(0, 10000, pd_id.shape[0]))/10000\n",
    "pd_id=pd_id[['user.id', 'wookie']]\n",
    "\n",
    "#Give each machine a 20% chance of being in the validation, \n",
    "#a 20% chance of being in the testing and a \n",
    "# 60% chance of being in the training data set.\n",
    "# Split into Train vs. Valid vs. Test\n",
    "#- Test: 20%\n",
    "#- Train: 60%\n",
    "#- Validation: 20%\n",
    "pd_id['MODELING_GROUP'] = np.where(((pd_id.wookie <= 0.60)), 'TRAINING', np.where(((pd_id.wookie <= 0.80)), 'VALIDATION', 'TESTING'))\n",
    "\n",
    "tips_summed = pd_id.groupby(['MODELING_GROUP'])['wookie'].count()\n",
    "\n",
    "# Append the Group of each id to each individual record.\n",
    "df=df.sort_values(by=['user.id'], ascending=[True])\n",
    "\n",
    "pd_id=pd_id.sort_values(by=['user.id'], ascending=[True])\n",
    "df = df.merge(pd_id, on=['user.id'], how='inner')\n",
    "\n",
    "#Train\n",
    "X_train = df[df['MODELING_GROUP'] == 'TRAINING']\n",
    "X_train = X_train.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_train = df[df['MODELING_GROUP'] == 'TRAINING'].suspended\n",
    "\n",
    "# Validation\n",
    "X_valid = df[df['MODELING_GROUP'] == 'VALIDATION']\n",
    "X_valid = X_valid.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_valid = df[df['MODELING_GROUP'] == 'VALIDATION'].suspended\n",
    "\n",
    "# Test\n",
    "X_test = df[df['MODELING_GROUP'] == 'TESTING']\n",
    "X_test = X_test.drop(['wookie', 'MODELING_GROUP'], axis=1)\n",
    "y_test = df[df['MODELING_GROUP'] == 'TESTING'].suspended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.559783Z",
     "iopub.status.busy": "2021-04-23T20:37:23.559062Z",
     "iopub.status.idle": "2021-04-23T20:37:23.563974Z",
     "shell.execute_reply": "2021-04-23T20:37:23.564443Z"
    },
    "papermill": {
     "duration": 0.024614,
     "end_time": "2021-04-23T20:37:23.564643",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.540029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train: 149932 unique ids train: 149932 full valid: 51994 unique ids valid: 51994 full test: 38025 unique ids test: 38025\n"
     ]
    }
   ],
   "source": [
    "print(\"full train:\", len(X_train), \"unique ids train:\", len(y_train), \n",
    "      \"full valid:\", len(X_valid), \"unique ids valid:\", len(y_valid), \n",
    "      \"full test:\", len(X_test), \"unique ids test:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01313,
     "end_time": "2021-04-23T20:37:23.593516",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.580386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.640131Z",
     "iopub.status.busy": "2021-04-23T20:37:23.634779Z",
     "iopub.status.idle": "2021-04-23T20:37:23.663043Z",
     "shell.execute_reply": "2021-04-23T20:37:23.662427Z"
    },
    "papermill": {
     "duration": 0.056187,
     "end_time": "2021-04-23T20:37:23.663202",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.607015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tweets_per_minute = True): # no *args or **kargs\n",
    "        self.tweets_per_minute = tweets_per_minute\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        X_train_2 = X.copy()\n",
    "        # User Age (tweet created_at - account created_at)\n",
    "        X_train_2[\"user_age\"] = (X_train_2[\"created_at\"] - X_train_2[\"user.created_at\"]).dt.days\n",
    "        # Tweets per day \n",
    "        X_train_2['tweets_per_day'] = X_train_2['user.statuses_count'].astype(float)/X_train_2[\"user_age\"] \n",
    "        # Calculate time since last tweet\n",
    "        X_train_2[\"since_last_tweet_mins\"] = X_train_2.sort_values(['user.id','created_at']).groupby('user.id')['created_at'].diff().dt.seconds.div(60)\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].min().to_frame('since_last_tweet_mins_min'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].max().to_frame('since_last_tweet_mins_max'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2.groupby(['user.id'], sort=False)['since_last_tweet_mins'].mean().to_frame('since_last_tweet_mins_mean'), on = [\"user.id\"])        \n",
    "        \n",
    "        X_train_2['date'] = X_train_2['created_at'].dt.date\n",
    "        X_train_2['hour'] = X_train_2['created_at'].dt.hour\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2[['user.id', 'date', 'hour', 'id']].groupby(['user.id', 'date', 'hour']).count().groupby('user.id', sort=False)[\"id\"].mean().reset_index(name ='avg_tweets_per_hr'), on = [\"user.id\"])\n",
    "        X_train_2 = pd.merge(X_train_2, X_train_2[['user.id', 'date', 'id']].groupby(['user.id', 'date']).count().groupby('user.id', sort=False)[\"id\"].mean().reset_index(name ='avg_tweets_per_day'), on = [\"user.id\"])\n",
    "\n",
    "        X_train_2.loc[X_train_2['quoted_status_id'].notna(), 'quoted_status_id'] = 1\n",
    "        X_train_2.loc[X_train_2['quoted_status_id'].isna(), 'quoted_status_id'] = 0\n",
    "        X_train_2['no_hashtags'] = X_train_2['text'].apply(lambda x: len(re.findall(r\"#(\\w+)\", x)))\n",
    "        X_train_2['no_mentions'] = X_train_2['text'].apply(lambda x: len(re.findall(\"@(\\w{1,15})\", x)))\n",
    "        X_train_2['no_urls'] = X_train_2['text'].apply(lambda x: len(re.findall(\"(?P<url>https?://[^\\s]+)\", x)))\n",
    "        X_train_2['tw_len'] = X_train_2['text'].apply(lambda x: len(x))\n",
    "        X_train_2['followers_per_followees'] = X_train_2['user.followers_count'].astype('float')/X_train_2['user.friends_count'].astype('float')\n",
    "\n",
    "        # URLs (percent of tweets with them)\n",
    "        X_train_2[\"containsURL\"] = (X_train_2['no_urls']  > 0).astype(int)\n",
    "        url_counts = X_train_2.groupby('user.id').agg({'created_at':'count', \n",
    "                         'containsURL':'sum'})\n",
    "        url_counts['user.urls_per_tweet'] = url_counts['containsURL']/url_counts['created_at']\n",
    "        X_train_2 = pd.merge(X_train_2, url_counts[['user.urls_per_tweet']], on = [\"user.id\"])  \n",
    "\n",
    "        # Hashtags, Mentions, and URLS\n",
    "        url_counts = X_train_2.groupby('user.id').agg({'created_at':'count', \n",
    "                 'no_hashtags':'sum', 'no_mentions':'sum', 'no_urls':'sum'})\n",
    "        url_counts['no_hashtags_per_tweet'] = url_counts['no_hashtags']/url_counts['created_at']\n",
    "        url_counts['no_mentions_per_tweet'] = url_counts['no_mentions']/url_counts['created_at']\n",
    "        url_counts['no_urls_per_tweet'] = url_counts['no_urls']/url_counts['created_at']\n",
    "        X_train_2 = pd.merge(X_train_2, url_counts[['no_hashtags_per_tweet', 'no_mentions_per_tweet', 'no_urls_per_tweet']], on = [\"user.id\"])  \n",
    "\n",
    "        X_train_2['user.followers_count'] = X_train_2['user.followers_count'].astype('float')\n",
    "        X_train_2['user.friends_count'] = X_train_2['user.friends_count'].astype('float')\n",
    "\n",
    "        # Pace of follower and friend add-on during collected time period \n",
    "        avg_friends_per_day = X_train_2.groupby(['user.id', 'date'], as_index=True).mean()[['user.followers_count', 'user.friends_count']]\n",
    "        avg_friends_change = avg_friends_per_day.sort_values(['user.id','date']).groupby('user.id').diff().rename(columns={'user.followers_count':'user.followers_countdailychange','user.friends_count' : 'user.friends_countdailychange'})\n",
    "        X_train_2 = pd.merge(X_train_2, avg_friends_change.groupby(['user.id'], as_index=True)[['user.followers_countdailychange', 'user.friends_countdailychange']].mean(), on = [\"user.id\"]) \n",
    "\n",
    "        # Pace of follower and friend add-on overall \n",
    "        X_train_2['user.friend_rate'] = X_train_2['user.friends_count']/X_train_2['user_age']\n",
    "        X_train_2['user.followers_rate'] = X_train_2['user.followers_count']/X_train_2['user_age']\n",
    "\n",
    "        X_train_2['user.has_url'] = (X_train_2['user.url'].fillna(False) != False).astype(int)\n",
    "        X_train_2['user.has_location'] = (X_train_2['user.location'].fillna(False) != False).astype(int)\n",
    "        X_train_2['user.screen_name.digit_length'] = X_train_2['user.screen_name'].apply(lambda x: len(re.sub(\"[^0-9]\", \"\", x)) if pd.notnull(x) else x)\n",
    "        X_train_2['user.screen_name.length'] = X_train_2['user.screen_name'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in tweet\n",
    "        #X_train_2['text'] = X_train_2['text'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in bio \n",
    "        X_train_2['user.description'] = X_train_2['user.description'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Convert emojis to words in name\n",
    "        X_train_2['user.name'] = X_train_2['user.name'].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \" \")) if pd.notnull(x) else x)\n",
    "\n",
    "        # Create binary for whether it is reply or not \n",
    "        X_train_2['is_reply'] = (X_train_2['in_reply_to_status_id'].fillna(False) != False).astype(int)\n",
    "        \n",
    "        # Reset IDs for transferrability \n",
    "        X_train_2['id'] = X_train_2['id'].astype('str')\n",
    "        X_train_2['id'] = X_train_2['id'].apply(lambda x: \"'\" + x + \"'\")\n",
    "\n",
    "        X_train_2 = X_train_2[['id','created_at', 'text', 'source', 'retweet_count', 'favorite_count', 'lang', 'possibly_sensitive',\n",
    "                              'withheld_in_countries', 'place.country', 'quoted_status_id', 'user.id', 'user.created_at',\n",
    "                              'user.description', 'user.favourites_count', 'user.followers_count', 'user.friends_count',\n",
    "                               'user.geo_enabled', 'user.has_extended_profile', 'user.lang', 'user.listed_count',\n",
    "                               'user.location', 'user.name', 'user.protected', 'user.screen_name', 'user.statuses_count',\n",
    "                               'user.time_zone', 'user.verified', 'user.protected', 'user.default_profile',\n",
    "                               'is_quote_status', 'quoted_status.user.followers_count', 'quoted_status.user.friends_count',\n",
    "                               'retweeted_status.user.followers_count', 'retweeted_status.user.friends_count', 'user_age', \n",
    "                              'tweets_per_day', 'since_last_tweet_mins', 'since_last_tweet_mins_min',\n",
    "                               'since_last_tweet_mins_max', 'since_last_tweet_mins_mean', 'avg_tweets_per_hr', \n",
    "                               'avg_tweets_per_day', 'no_hashtags', 'no_mentions', 'no_urls', 'tw_len',\n",
    "                              'followers_per_followees', 'containsURL',  'user.urls_per_tweet', 'no_hashtags_per_tweet',\n",
    "                              'no_mentions_per_tweet', 'no_urls_per_tweet', 'user.followers_countdailychange', \n",
    "                               'user.friends_countdailychange', 'user.friend_rate', 'user.followers_rate',\n",
    "                              'user.has_url', 'user.has_location', 'user.screen_name.digit_length', \n",
    "                               'user.screen_name.length', 'is_reply',  'suspended']]\n",
    "        \n",
    "        return X_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013292,
     "end_time": "2021-04-23T20:37:23.690405",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.677113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Run Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.726396Z",
     "iopub.status.busy": "2021-04-23T20:37:23.725637Z",
     "iopub.status.idle": "2021-04-23T20:37:23.729174Z",
     "shell.execute_reply": "2021-04-23T20:37:23.728529Z"
    },
    "papermill": {
     "duration": 0.025295,
     "end_time": "2021-04-23T20:37:23.729365",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.704070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline([\n",
    "        #('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder())\n",
    "        #('std_scaler', StandardScaler()), # feature scaling\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013397,
     "end_time": "2021-04-23T20:37:23.756951",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.743554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Run and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-23T20:37:23.792797Z",
     "iopub.status.busy": "2021-04-23T20:37:23.791762Z",
     "iopub.status.idle": "2021-04-23T20:38:52.898068Z",
     "shell.execute_reply": "2021-04-23T20:38:52.898831Z"
    },
    "papermill": {
     "duration": 89.128125,
     "end_time": "2021-04-23T20:38:52.899072",
     "exception": false,
     "start_time": "2021-04-23T20:37:23.770947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train_tr = test_pipeline.fit_transform(X_train)\n",
    "\n",
    "X_train_tr.to_csv('x_train.csv', index=False, encoding = \"utf_8_sig\")\n",
    "y_train.to_csv('y_train.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "# Test\n",
    "X_train_tr = test_pipeline.fit_transform(X_test)\n",
    "\n",
    "X_train_tr.to_csv('x_test.csv', index=False, encoding = \"utf_8_sig\")\n",
    "y_test.to_csv('y_test.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "# Valid \n",
    "X_train_tr = test_pipeline.fit_transform(X_valid)\n",
    "\n",
    "X_train_tr.to_csv('x_validation.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "y_valid.to_csv('y_validation.csv', index=False, encoding = \"utf_8_sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1069.024119,
   "end_time": "2021-04-23T20:38:54.429933",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-23T20:21:05.405814",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
