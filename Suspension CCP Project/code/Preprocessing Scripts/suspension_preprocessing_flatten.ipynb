{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Load Packages\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import os \n",
    "import time\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tweepy\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "# !pip install tweepy\n",
    "\n",
    "########## Set Parameters\n",
    "\n",
    "# Indicate how many rows to skip before columns\n",
    "# Note: Python uses zero-based indexing, so skiprow=0 begins at the first row of file,\n",
    "# while skiprow=1 begins at the second row.\n",
    "skiprow=0\n",
    "\n",
    "# Indicate name of column that contains text data for analysis\n",
    "text_column = \"text\"\n",
    "\n",
    "filepath = \"data/\"\n",
    "\n",
    "import_bucket = \"joe-exotic-2020\"\n",
    "\n",
    "results_bucket = 'processed' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3 = boto3.client('s3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joe-exotic-2020/net-2020-09-18.csv', 'joe-exotic-2020/net-2020-09-19.csv']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "if re.findall(\"net\",obj.key)]\n",
    "filelist = [ x for x in filelist  if \"processed\" not in x ]\n",
    "filelist = filelist[53:55]\n",
    "filelist\n",
    "\n",
    "############# Import Data\n",
    "\n",
    "def import_data(filelist):\n",
    "    '''Read in data from excel files into Pandas dataframe. Concatenates multiple files if necessary. \n",
    "    Inputs: Directory path, number of rows to skip\n",
    "    Outputs: Pandas dataframe containing imported data\n",
    "    '''\n",
    "    # Identify if directory or file path was provided\n",
    "    #filelist = [os.path.join(obj.bucket_name, obj.key) \n",
    "        #for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "        #if re.findall(\"net\",obj.key)]\n",
    "    #filelist = filelist[0]\n",
    "    if type(filelist) == list:\n",
    "        dataframes = []\n",
    "        # Iterate through files of the directory\n",
    "        for filename in filelist:\n",
    "            object_key = filename.rsplit('/', 1)[1]\n",
    "            csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "            body = csv_obj['Body']\n",
    "            csv_string = body.read().decode('utf-8')\n",
    "            dataframe = pd.read_csv(StringIO(csv_string))\n",
    "            dataframes.append(dataframe)\n",
    "        df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "    else:\n",
    "        # Read in single file\n",
    "        object_key = filelist.rsplit('/', 1)[1]\n",
    "        csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "        body = csv_obj['Body']\n",
    "        csv_string = body.read().decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(csv_string))\n",
    "    rows = len(df)\n",
    "    dups = len(df) - len(df.drop_duplicates())\n",
    "    \n",
    "    # Check for text_column\n",
    "    try:\n",
    "        if len(df[text_column]) > 1:  \n",
    "            pass\n",
    "    except:\n",
    "        print(\"Cannot find text column. Please confirm that the text_column and skiprow parameters are updated.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "import json\n",
    "import pandas \n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df\n",
    "\n",
    "\n",
    "def extract_and_flatten_json(filelist):\n",
    "    df = import_data(filelist)\n",
    "    json_cols = ['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', \n",
    "             'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
    "             'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'lang']\n",
    "    df['_json'] = df['_json'].map(lambda x: dict(eval(x)))\n",
    "    df_2 = json_normalize(df['_json'], meta=json_cols)\n",
    "    cols_to_use = df_2.columns.difference(df.columns)\n",
    "    df_3 = pd.merge(df, df_2[cols_to_use], left_index = True, right_index = True)\n",
    "    return df_3\n",
    "\n",
    "def screen_name(col):\n",
    "    keys_to_extract = [\"screen_name\"]\n",
    "    screen_names = list()\n",
    "    names = list()\n",
    "    try:\n",
    "        if len(col) > 0:\n",
    "            for i in range(0,len(col)):\n",
    "                a_subset = {key: col[i][key] for key in keys_to_extract}\n",
    "                df = pd.DataFrame(a_subset, index=[0])\n",
    "                q = df.loc[:, 'screen_name'].item()\n",
    "                screen_names.append(q)\n",
    "                b = df.loc[:, 'name'].item()\n",
    "                names.append(q)\n",
    "        else:\n",
    "            screen_names.append(\"None\")\n",
    "            names.append(\"None\")\n",
    "    except:\n",
    "        pass\n",
    "    return screen_names, names\n",
    "\n",
    "def media(col):\n",
    "    keys_to_extract = [\"id\", \"id_str\", \"media_url\", \"media_url_https\", \"url\", \"display_url\", \"expanded_url\", \"type\"]\n",
    "    try:\n",
    "            a_subset = {key: col[0][key] for key in keys_to_extract}\n",
    "            df = pd.DataFrame(a_subset, index=[0])\n",
    "            id_s = df.loc[:, 'id'].item()\n",
    "            id_str = df.loc[:, 'id_str'].item()\n",
    "            media_url = df.loc[:, 'media_url'].item()\n",
    "            media_url_https = df.loc[:, 'media_url_https'].item()\n",
    "            url = df.loc[:, 'url'].item()\n",
    "            display_url = df.loc[:, 'display_url'].item()\n",
    "            expanded_url = df.loc[:, 'expanded_url'].item()\n",
    "            type_s = df.loc[:, 'type'].item()\n",
    "    except:\n",
    "            id_s = \"None\"\n",
    "            id_str = \"None\"\n",
    "            media_url = \"None\"\n",
    "            media_url_https = \"None\"\n",
    "            url = \"None\"\n",
    "            display_url = \"None\"\n",
    "            expanded_url = \"None\"\n",
    "            type_s = \"None\"\n",
    "    return id_s, id_str, media_url, media_url_https, url, display_url, expanded_url, type_s\n",
    "\n",
    "def extended_media(col):\n",
    "    keys_to_extract = ['id', 'id_str', 'media_url', 'media_url_https', 'url', 'display_url', 'expanded_url', 'type']\n",
    "    id_s = list()\n",
    "    id_str = list()\n",
    "    media_url = list()\n",
    "    media_url_https = list()\n",
    "    url = list()\n",
    "    display_url = list()\n",
    "    expanded_url = list()\n",
    "    type_s = list()\n",
    "    try:\n",
    "        if len(col) > 0:\n",
    "            for i in range(0,len(col)):\n",
    "                a_subset = {key: col[i][key] for key in keys_to_extract}\n",
    "                df = pd.DataFrame(a_subset, index=[0])\n",
    "                id_s_a = df.loc[:, 'id'].item()\n",
    "                id_str_a = df.loc[:, 'id_str'].item()\n",
    "                media_url_a = df.loc[:, 'media_url'].item()\n",
    "                media_url_https_a = df.loc[:, 'media_url_https'].item()\n",
    "                url_a = df.loc[:, 'url'].item()\n",
    "                display_url_a = df.loc[:, 'display_url'].item()\n",
    "                expanded_url_a = df.loc[:, 'expanded_url'].item()\n",
    "                type_s_a = df.loc[:, 'type'].item()\n",
    "                id_s.append(id_s_a)\n",
    "                id_str.append(id_str_a)\n",
    "                media_url.append(media_url_a)\n",
    "                media_url_https.append(media_url_https_a)\n",
    "                url.append(url_a)\n",
    "                display_url.append(display_url_a)\n",
    "                expanded_url.append(expanded_url_a)\n",
    "                type_s.append(type_s_a)\n",
    "        else:\n",
    "            id_s.append(\"None\")\n",
    "            id_str.append(\"None\")\n",
    "            media_url.append(\"None\")\n",
    "            media_url_https.append(\"None\")\n",
    "            url.append(\"None\")\n",
    "            display_url.append(\"None\")\n",
    "            expanded_url.append(\"None\")\n",
    "            type_s.append(\"None\")\n",
    "    except:\n",
    "        pass\n",
    "    return  id_s, id_str, media_url, media_url_https, url, display_url, expanded_url, type_s\n",
    "\n",
    "def entities_symbols(col):\n",
    "    keys_to_extract = [\"text\"]\n",
    "    text = list()\n",
    "    try:\n",
    "        if col.astype(bool) == True & col.notnull() == False:\n",
    "            for i in range(0,len(col)):\n",
    "                a_subset = {key: col[i][key] for key in keys_to_extract}\n",
    "                df = pd.DataFrame(a_subset, index=[0])\n",
    "                t_a = df.loc[:, 'text'].item()\n",
    "                text.append(t_a)\n",
    "        elif col.astype(bool) == True & col.notnull() == True:\n",
    "            text.append(\"None\")\n",
    "        else:\n",
    "            text.append(\"None\")\n",
    "    except:\n",
    "        pass\n",
    "    return  text\n",
    "\n",
    "def entities_urls(col):\n",
    "    keys_to_extract = ['url', 'expanded_url', 'display_url']\n",
    "    urls = list()\n",
    "    ex_urls = list()\n",
    "    di_urls = list()\n",
    "    try:\n",
    "        if col.astype(bool) == True & col.notnull() == False:\n",
    "            for i in range(0,len(col)):\n",
    "                a_subset = {key: col[i][key] for key in keys_to_extract}\n",
    "                df = pd.DataFrame(a_subset, index=[0])\n",
    "                urls_t = df.loc[:, 'url'].item()\n",
    "                ex_urls_t = df.loc[:, 'ex_url'].item()\n",
    "                di_urls_t = df.loc[:, 'di_url'].item()\n",
    "                urls.append(urls_t)\n",
    "                ex_urls.append(ex_urls_t)\n",
    "                di_urls.append(di_urls_t)\n",
    "        elif col.astype(bool) == True & col.notnull() == True:\n",
    "            urls.append(\"None\")\n",
    "            ex_urls.append(\"None\")\n",
    "            di_urls.append(\"None\")\n",
    "        else:\n",
    "            urls.append(\"None\")\n",
    "            ex_urls.append(\"None\")\n",
    "            di_urls.append(\"None\")\n",
    "    except:\n",
    "        pass\n",
    "    return  urls, ex_urls, di_urls\n",
    "\n",
    "def geo_pull(col):\n",
    "    keys_to_extract = [\"coordinates\", \"type\"]\n",
    "    try:\n",
    "            a_subset = {key: col[0][key] for key in keys_to_extract}\n",
    "            df = pd.DataFrame(a_subset, index=[0])\n",
    "            coords = df.loc[:, 'coordinates'].item()\n",
    "            type_s = df.loc[:, 'type'].item()\n",
    "    except:\n",
    "            coords = \"None\"\n",
    "            type_s = \"None\"\n",
    "    return coords, type_s\n",
    "\n",
    "def second_flattening(filelist):\n",
    "    df_3 = extract_and_flatten_json(filelist)\n",
    "    \n",
    "    # Primary User \n",
    "    df_3['entities.user_mentions.screen_name'] = df_3['entities.user_mentions'].apply(lambda x: screen_name(x)[0])\n",
    "    df_3['entities.user_mentions.name'] = df_3['entities.user_mentions'].apply(lambda x: screen_name(x)[1])\n",
    "\n",
    "    df_3['entities.hashtags.text'] = df_3['entities.hashtags'].apply(lambda x: entities_symbols(x))\n",
    "    df_3['entities.symbols.text'] = df_3['entities.symbols'].apply(lambda x: entities_symbols(x))\n",
    "\n",
    "    df_3['entities.urls.url'] = df_3['entities.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['entities.urls.ex_url'] = df_3['entities.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['entities.urls.di_url'] = df_3['entities.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "    \n",
    "    # Quoted_Status\n",
    "    df_3['quoted_status.entities.user_mentions.screen_name'] = df_3['quoted_status.entities.user_mentions'].apply(lambda x: screen_name(x)[0])\n",
    "    df_3['quoted_status.entities.user_mentions.name'] = df_3['quoted_status.entities.user_mentions'].apply(lambda x: screen_name(x)[1])\n",
    "\n",
    "    df_3['quoted_status.entities.media.id'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[0])\n",
    "    df_3['quoted_status.entities.media.id_str'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[1])\n",
    "    df_3['quoted_status.entities.media.media_url'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[2])\n",
    "    df_3['quoted_status.entities.media.media_url_https'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[3])\n",
    "    df_3['quoted_status.entities.media.url'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[4])\n",
    "    df_3['quoted_status.entities.media.display_url'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[5])\n",
    "    df_3['quoted_status.entities.media.expanded_url'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[6])\n",
    "    df_3['quoted_status.entities.media.type'] = df_3['quoted_status.entities.media'].apply(lambda x: media(x)[7])\n",
    "\n",
    "    df_3['quoted_status.extended_entities.media.id'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[0])\n",
    "    df_3['quoted_status.extended_entities.media.id_str'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[1])\n",
    "    df_3['quoted_status.extended_entities.media.media_url'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[2])\n",
    "    df_3['quoted_status.extended_entities.media.media_url_https'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[3])\n",
    "    df_3['quoted_status.extended_entities.media.url'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[4])\n",
    "    df_3['quoted_status.extended_entities.media.display_url'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[5])\n",
    "    df_3['quoted_status.extended_entities.media.expanded_url'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[6])\n",
    "    df_3['quoted_status.extended_entities.media.type'] = df_3['quoted_status.extended_entities.media'].apply(lambda x: media(x)[7])\n",
    "\n",
    "    df_3['quoted_status.entities.hashtags.text'] = df_3['quoted_status.entities.hashtags'].apply(lambda x: entities_symbols(x))\n",
    "    df_3['quoted_status.entities.symbols.text'] = df_3['quoted_status.entities.symbols'].apply(lambda x: entities_symbols(x))\n",
    "\n",
    "    df_3['quoted_status.entities.urls.url'] = df_3['quoted_status.entities.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['quoted_status.entities.urls.ex_url'] = df_3['quoted_status.entities.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['quoted_status.entities.urls.di_url'] = df_3['quoted_status.entities.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['quoted_status.geo.coordinates'] = df_3['quoted_status.geo'].apply(lambda x: geo_pull(x)[0])\n",
    "    df_3['quoted_status.geo.type'] = df_3['quoted_status.geo'].apply(lambda x: geo_pull(x)[1])\n",
    "\n",
    "    df_3['quoted_status.user.entities.url.urls.url'] = df_3['quoted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['quoted_status.user.entities.url.urls.ex_url'] = df_3['quoted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['quoted_status.user.entities.url.urls.di_url'] = df_3['quoted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['quoted_status.user.entities.description.urls.url'] = df_3['quoted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['quoted_status.user.entities.description.urls.ex_url'] = df_3['quoted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['quoted_status.user.entities.description.urls.di_url'] = df_3['quoted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "    \n",
    "    # Retweeted_Status\n",
    "    df_3['retweeted_status.entities.user_mentions.screen_name'] = df_3['retweeted_status.entities.user_mentions'].apply(lambda x: screen_name(x)[0])\n",
    "    df_3['retweeted_status.entities.user_mentions.name'] = df_3['retweeted_status.entities.user_mentions'].apply(lambda x: screen_name(x)[1])\n",
    "\n",
    "    df_3['retweeted_status.entities.media.id'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[0])\n",
    "    df_3['retweeted_status.entities.media.id_str'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[1])\n",
    "    df_3['retweeted_status.entities.media.media_url'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[2])\n",
    "    df_3['retweeted_status.entities.media.media_url_https'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[3])\n",
    "    df_3['retweeted_status.entities.media.url'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[4])\n",
    "    df_3['retweeted_status.entities.media.display_url'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[5])\n",
    "    df_3['retweeted_status.entities.media.expanded_url'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[6])\n",
    "    df_3['retweeted_status.entities.media.type'] = df_3['retweeted_status.entities.media'].apply(lambda x: media(x)[7])\n",
    "\n",
    "    df_3['retweeted_status.extended_entities.media.id'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[0])\n",
    "    df_3['retweeted_status.extended_entities.media.id_str'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[1])\n",
    "    df_3['retweeted_status.extended_entities.media.media_url'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[2])\n",
    "    df_3['retweeted_status.extended_entities.media.media_url_https'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[3])\n",
    "    df_3['retweeted_status.extended_entities.media.url'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[4])\n",
    "    df_3['retweeted_status.extended_entities.media.display_url'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[5])\n",
    "    df_3['retweeted_status.extended_entities.media.expanded_url'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[6])\n",
    "    df_3['retweeted_status.extended_entities.media.type'] = df_3['retweeted_status.extended_entities.media'].apply(lambda x: media(x)[7])\n",
    "\n",
    "    df_3['retweeted_status.entities.hashtags.text'] = df_3['retweeted_status.entities.hashtags'].apply(lambda x: entities_symbols(x))\n",
    "    df_3['retweeted_status.entities.symbols.text'] = df_3['retweeted_status.entities.symbols'].apply(lambda x: entities_symbols(x))\n",
    "\n",
    "    df_3['retweeted_status.entities.urls.url'] = df_3['retweeted_status.entities.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['retweeted_status.entities.urls.ex_url'] = df_3['retweeted_status.entities.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['retweeted_status.entities.urls.di_url'] = df_3['retweeted_status.entities.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['retweeted_status.geo.coordinates'] = df_3['retweeted_status.geo'].apply(lambda x: geo_pull(x)[0])\n",
    "    df_3['retweeted_status.geo.type'] = df_3['retweeted_status.geo'].apply(lambda x: geo_pull(x)[1])\n",
    "\n",
    "    df_3['retweeted_status.user.entities.url.urls.url'] = df_3['retweeted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['retweeted_status.user.entities.url.urls.ex_url'] = df_3['retweeted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['retweeted_status.user.entities.url.urls.di_url'] = df_3['retweeted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['retweeted_status.user.entities.description.urls.url'] = df_3['retweeted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['retweeted_status.user.entities.description.urls.ex_url'] = df_3['retweeted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['retweeted_status.user.entities.description.urls.di_url'] = df_3['retweeted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    # Retweeted_Status_Quoted_Status\n",
    "    df_3['retweeted_status.quoted_status.user_mentions.screen_name'] = df_3['retweeted_status.quoted_status.entities.user_mentions'].apply(lambda x: screen_name(x)[0])\n",
    "    df_3['retweeted_status.quoted_status.user_mentions.name'] = df_3['retweeted_status.quoted_status.entities.user_mentions'].apply(lambda x: screen_name(x)[1])\n",
    "\n",
    "    df_3['retweeted_status.quoted_status.entities.media.id'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[0])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.id_str'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[1])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.media_url'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[2])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.media_url_https'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[3])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.url'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[4])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.display_url'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[5])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.expanded_url'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[6])\n",
    "    df_3['retweeted_status.quoted_status.entities.media.type'] = df_3['retweeted_status.quoted_status.entities.media'].apply(lambda x: media(x)[7])\n",
    "\n",
    "    df_3['retweeted_status.extended_entities.media.id'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[0])\n",
    "    df_3['retweeted_status.extended_entities.media.id_str'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[1])\n",
    "    df_3['retweeted_status.extended_entities.media.media_url'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[2])\n",
    "    df_3['retweeted_status.extended_entities.media.media_url_https'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[3])\n",
    "    df_3['retweeted_status.extended_entities.media.url'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[4])\n",
    "    df_3['retweeted_status.extended_entities.media.display_url'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[5])\n",
    "    df_3['retweeted_status.extended_entities.media.expanded_url'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[6])\n",
    "    df_3['retweeted_status.extended_entities.media.type'] = df_3['retweeted_status.quoted_status.extended_entities.media'].apply(lambda x: media(x)[7])\n",
    "\n",
    "    df_3['retweeted_status.quoted_status.entities.hashtags.text'] = df_3['retweeted_status.quoted_status.entities.hashtags'].apply(lambda x: entities_symbols(x))\n",
    "    df_3['retweeted_status.quoted_status.entities.symbols.text'] = df_3['retweeted_status.quoted_status.entities.symbols'].apply(lambda x: entities_symbols(x))\n",
    "\n",
    "    df_3['retweeted_status.quoted_status.entities.urls.url'] = df_3['retweeted_status.quoted_status.entities.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['retweeted_status.quoted_status.entities.urls.ex_url'] = df_3['retweeted_status.quoted_status.entities.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['retweeted_status.quoted_status.entities.urls.di_url'] = df_3['retweeted_status.quoted_status.entities.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['retweeted_status.quoted_status.geo.coordinates'] = df_3['retweeted_status.quoted_status.geo'].apply(lambda x: geo_pull(x)[0])\n",
    "    df_3['retweeted_status.quoted_status.geo.type'] = df_3['retweeted_status.quoted_status.geo'].apply(lambda x: geo_pull(x)[1])\n",
    "\n",
    "    df_3['retweeted_status.quoted_status.user.entities.url.urls.url'] = df_3['retweeted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['retweeted_status.quoted_status.user.entities.url.urls.ex_url'] = df_3['retweeted_status.quoted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['retweeted_status.quoted_status.user.entities.url.urls.di_url'] = df_3['retweeted_status.quoted_status.user.entities.url.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['retweeted_status.quoted_status.user.entities.description.urls.url'] = df_3['retweeted_status.quoted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['retweeted_status.quoted_status.user.entities.description.urls.ex_url'] = df_3['retweeted_status.quoted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['retweeted_status.quoted_status.user.entities.description.urls.di_url'] = df_3['retweeted_status.quoted_status.user.entities.description.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    # Original \n",
    "    df_3['symbols.text'] = df_3['symbols'].apply(lambda x: entities_symbols(x))\n",
    "    df_3['hashtags.text'] = df_3['hashtags'].apply(lambda x: entities_symbols(x))\n",
    "\n",
    "    df_3['urls.url'] = df_3['urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['urls.ex_url'] = df_3['urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['urls.di_url'] = df_3['urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    # User\n",
    "    df_3['user.entities.description.urls.url'] = df_3['user.entities.description.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['user.entities.description.urls.ex_url'] = df_3['user.entities.description.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['user.entities.description.urls.di_url'] = df_3['user.entities.description.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_3['user.entities.url.urls.url'] = df_3['user.entities.url.urls'].apply(lambda x: entities_urls(x)[0])\n",
    "    df_3['user.entities.url.urls.ex_url'] = df_3['user.entities.url.urls'].apply(lambda x: entities_urls(x)[1])\n",
    "    df_3['user.entities.url.urls.di_url'] = df_3['user.entities.url.urls'].apply(lambda x: entities_urls(x)[2])\n",
    "\n",
    "    df_4 = df_3.drop(['_json','entities', 'extended_entities', 'place', 'quoted_status', 'geo', 'retweeted_status', 'user', 'author', \n",
    "                  'entities.user_mentions', 'coordinates', 'entities.media', 'extended_entities.media',\n",
    "                  'quoted_status.place', \n",
    "                  'quoted_status.geo', \n",
    "                  'quoted_status.entities.user_mentions', 'quoted_status.coordinates', 'quoted_status.entities.media',\n",
    "                  'quoted_status.extended_entities.media', 'quoted_status.user.entities.description.urls',\n",
    "                  'quoted_status.user.entities.description.urls', 'entities.hashtags', 'entities.symbols',\t'entities.urls',\n",
    "                 'hashtags', 'symbols', 'urls', 'quoted_status.entities.hashtags', 'quoted_status.entities.symbols', \n",
    "                  'quoted_status.entities.urls', 'retweeted_status.entities.user_mentions', 'retweeted_status.entities.media', \n",
    "                  'retweeted_status.extended_entities.media', 'retweeted_status.entities.hashtags', 'retweeted_status.entities.symbols', \n",
    "                  'retweeted_status.entities.urls', 'retweeted_status.geo','retweeted_status.user.entities.url.urls', \n",
    "                  'retweeted_status.user.entities.description.urls', 'retweeted_status.quoted_status.entities.user_mentions', \n",
    "                  'retweeted_status.quoted_status.entities.media', 'retweeted_status.quoted_status.extended_entities.media', \n",
    "                  'retweeted_status.quoted_status.entities.hashtags', 'retweeted_status.quoted_status.entities.symbols', \n",
    "                  'retweeted_status.quoted_status.entities.urls', 'retweeted_status.quoted_status.geo',\n",
    "                  'retweeted_status.user.entities.url.urls', 'retweeted_status.quoted_status.user.entities.description.urls', \n",
    "                  'user.entities.description.urls', 'user.entities.url.urls'], axis=1)\n",
    "    \n",
    "    # Eliminate redundancies \n",
    "    df_5 = df_4.drop(['geo.coordinates', 'geo.type', 'quoted_status.user.entities.url.urls', 'retweeted_status.coordinates',\n",
    "                     'retweeted_status.place', 'retweeted_status.quoted_status.coordinates', 'retweeted_status.quoted_status.place',\n",
    "                     'retweeted_status.quoted_status.user.entities.url.urls', 'user_mentions', \"quoted_status.user.is_translation_enabled\", \n",
    "                      \"quoted_status.user.is_translator\", \"quoted_status.user.notifications\", \"quoted_status.user.profile_background_color\", \n",
    "                      \"quoted_status.user.profile_background_image_url\", \"quoted_status.user.profile_background_image_url_https\", \n",
    "                      \"quoted_status.user.profile_background_tile\", \"quoted_status.user.profile_image_url\", \n",
    "                      \"quoted_status.user.profile_link_color\", \"quoted_status.user.profile_sidebar_border_color\", \n",
    "                      \"quoted_status.user.profile_sidebar_fill_color\", \"quoted_status.user.profile_text_color\", \n",
    "                      \"quoted_status.user.profile_use_background_image\", \"quoted_status.user.translator_type\", \n",
    "                      \"quoted_status.user.utc_offset\", \"retweeted_status.quoted_status.user.is_translation_enabled\",\n",
    "                      \"retweeted_status.quoted_status.user.is_translator\", \"retweeted_status.quoted_status.user.notifications\",\n",
    "                      \"retweeted_status.quoted_status.user.profile_background_color\", \n",
    "                      \"retweeted_status.quoted_status.user.profile_background_image_url\", \n",
    "                      \"retweeted_status.quoted_status.user.profile_background_image_url_https\", \n",
    "                      \"retweeted_status.quoted_status.user.profile_background_tile\", \"retweeted_status.quoted_status.user.profile_link_color\", \n",
    "                      \"retweeted_status.quoted_status.user.profile_sidebar_border_color\",\n",
    "                      \"retweeted_status.quoted_status.user.profile_sidebar_fill_color\", \n",
    "                      \"retweeted_status.quoted_status.user.profile_text_color\",\n",
    "                      \"retweeted_status.quoted_status.user.profile_use_background_image\", \n",
    "                      \"retweeted_status.quoted_status.user.translator_type\", \"retweeted_status.user.is_translation_enabled\", \n",
    "                      \"retweeted_status.user.is_translator\", \"retweeted_status.user.notifications\", \n",
    "                      \"retweeted_status.user.profile_background_color\", \"retweeted_status.user.profile_background_image_url\", \n",
    "                      \"retweeted_status.user.profile_background_image_url_https\", \"retweeted_status.user.profile_background_tile\", \n",
    "                      \"retweeted_status.user.profile_image_url\", \"retweeted_status.user.profile_link_color\", \n",
    "                      \"retweeted_status.user.profile_sidebar_border_color\", \"retweeted_status.user.profile_sidebar_fill_color\", \n",
    "                      \"retweeted_status.user.profile_text_color\", \"retweeted_status.user.profile_use_background_image\", \"retweeted_status.user.translator_type\", \n",
    "                      \"user.is_translation_enabled\", \"user.is_translator\", \"user.notifications\", \"user.profile_background_color\",\n",
    "                      \"user.profile_background_image_url\", \"user.profile_background_image_url_https\", \"user.profile_background_tile\", \n",
    "                      \"user.profile_link_color\", \"user.profile_sidebar_border_color\", \"user.profile_sidebar_fill_color\", \n",
    "                      \"user.profile_text_color\", \"user.profile_use_background_image\", \"user.translator_type\"\n",
    "                     ], axis=1)\n",
    "    return df_5\n",
    "\n",
    "def full_preprocessing(filelist):\n",
    "    df_5 = second_flattening(filelist)\n",
    "    # Read in single file\n",
    "    filelist_2 = [os.path.join(obj.bucket_name, obj.key) \n",
    "        for obj in s3_resource.Bucket(name=import_bucket).objects.all() \n",
    "        if re.findall(\"susp\",obj.key)]\n",
    "    object_key = filelist_2[0].split('/', 1)[1]\n",
    "    csv_obj = s3.get_object(Bucket=import_bucket, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    tw_users = pd.read_csv(StringIO(csv_string))\n",
    "    \n",
    "    df_6 = pd.merge(df_5, tw_users[['screen_name', 'counts', 'suspended']], left_on = [\"user.screen_name\"], right_on = [\"screen_name\"])\n",
    "    return df_6\n",
    "    #df_6.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "    #s3_resource.Object(import_bucket, results_bucket + '/' + filelist[0].rsplit('/', 1)[1] + \"_processed.csv\").put(Body=csv_buffer.getvalue())\n",
    "    \n",
    "filelist[0]\n",
    "    \n",
    "filelist[1]\n",
    "\n",
    "df_test = full_preprocessing(filelist[0])\n",
    "\n",
    "df_test_2 = full_preprocessing(filelist[1])\n",
    "\n",
    "df_test\n",
    "\n",
    "df_test_2\n",
    "\n",
    "#dataframe[dataframe['user.created_at'] == '人如饮水，冷暖自知']\n",
    "df_test['user.created_at'][df_test['user.created_at'].astype('str').apply(lambda x: len(x) !=30) == True]\n",
    "\n",
    "#dataframe[dataframe['user.created_at'] == '人如饮水，冷暖自知']\n",
    "df_test_2['user.created_at'][df_test_2['user.created_at'].astype('str').apply(lambda x: len(x) !=30) == True]\n",
    "\n",
    "df_test_2['user.created_at'] = df_test_2['user.created_at'].astype('str')\n",
    "\n",
    "df_test_2['user.created_at'].to_csv(\"test_column.csv\")\n",
    "\n",
    "results_bucket + '/' + filelist[1].rsplit('/', 1)[1] + \"_processed.csv\"\n",
    "\n",
    "import_bucket\n",
    "\n",
    "df_test.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "s3_resource.Object(import_bucket, results_bucket + '/' + filelist[0].rsplit('/', 1)[1] + \"_processed.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "#df_test_2.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "#s3_resource.Object(import_bucket, results_bucket + '/' + filelist[1].rsplit('/', 1)[1] + \"_processed.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "df_test_2.to_csv('s3://joe-exotic-2020/processed/net-2020-09-19.csv_processed.csv', index=False, encoding = \"utf_8_sig\")\n",
    "\n",
    "dataframes = list()\n",
    "for filename in filelist:\n",
    "    df = full_preprocessing(filename)\n",
    "    df.to_csv(csv_buffer, index=False, encoding = \"utf_8_sig\")\n",
    "    s3_resource.Object(import_bucket, results_bucket + '/' + filename.rsplit('/', 1)[] + \"_processed.csv\").put(Body=csv_buffer.getvalue())\n",
    "    #dataframes.append(df)\n",
    "    \n",
    "    filelist[0].rsplit('/', 1)[0]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
