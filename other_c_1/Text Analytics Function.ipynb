{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.11.9)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.14.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christian.conroy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/christian.conroy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.models import Phrases \n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.test.utils import datapath\n",
    "from pprint import pprint\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "stops.extend(['cik', 'company', 'name', 'clancy', 'corpform','type','kfiling', 'date', 'accession', 'number', 'item', 'table', 'contents'])\n",
    "st1= ['after', 'afterwards','again','against', 'all', 'almost','alone','along','already','also','although', 'always', \n",
    "      'am','among','amongst', 'amoungst', 'amount','an','and','another','any','anyhow','anyone','anything', 'anyway',\n",
    "      'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', \n",
    "      'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', \n",
    "      'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant','co','con','could','couldnt','cry','de',\n",
    "      'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else',\n",
    "      'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except',\n",
    "      'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found',\n",
    "      'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', \n",
    "      'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "      'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself',\n",
    "      'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile',\n",
    "      'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', \n",
    "      'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', \n",
    "      'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', \n",
    "      'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', \n",
    "      'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', \n",
    "      'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', \n",
    "      'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', \n",
    "      'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon',\n",
    "      'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru',\n",
    "      'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', \n",
    "      'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when',  'whence', \n",
    "      'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which',\n",
    "      'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without',\n",
    "      'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "# More extension \n",
    "stops.extend(st1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def build_text(doc_set):\n",
    "    for line in doc_set:\n",
    "        yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "        \n",
    "def process_texts(texts):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "    \n",
    "    1. Stopword Removal.\n",
    "    2. Collocation detection.\n",
    "    3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts: Tokenized texts.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    texts: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [bigram[line] for line in texts]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    texts = [[lemmatizer.lemmatize(word) for word in line] for line in texts]\n",
    "    #texts = [[word.split('/')[0] for word in lemmatize(' '.join(line), allowed_tags=re.compile('(NN)'), min_length=3)] for line in texts]\n",
    "    return texts\n",
    "\n",
    "# So we're setting the word limit in one go here.\n",
    "# I skipped the lemmative part as I just couldn't get it working with the bytes strings issue\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, texts, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    numtopics = []\n",
    "    for num_topics in range(1, limit):\n",
    "        numtopics.append(num_topics)\n",
    "        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "    \n",
    "    return lm_list, c_v, numtopics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/scikit_learn_iris.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #### Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    ###### All operations start here\n",
    "    start_time = time.time()\n",
    "    #### Bring in the data\n",
    "    data_list = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    comp10ks = []\n",
    "    for i in range(0,len(data_list)):\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    else: \n",
    "        try:\n",
    "        s = pd.read_csv(data_location, delimiter = None, error_bad_lines = False, header = None, warn_bad_lines=False)\n",
    "        text_file = s[0].str.cat(sep=' ')\n",
    "        comp10ks.append([data_list[i].split('/')[2].split('_')[0], ' '.join(data_list[i].split('/')[2].split('_')[3:]),text_file])\n",
    "        except:\n",
    "            pass  \n",
    "    # Convert All to Pandas Dataframe \n",
    "    cols = ['year', 'compname', \"text\"]\n",
    "    comp10ks = pd.DataFrame(comp10ks, columns=cols)\n",
    "    comp10ks.Year = pd.to_datetime(comp10ks.year).dt.to_period('y')\n",
    "    #### Pre-process and clean\n",
    "    comp10ks['text'] = comp10ks['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "    comp10ks['text'] = comp10ks.text.str.replace(r'<.*?>', '', regex= True)\n",
    "    comp10ks['text'] = comp10ks.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "    # Convert the titles to lowercase\n",
    "    comp10ks['text'] = comp10ks['text'].map(lambda x: x.lower())\n",
    "    comp10ks['text'] = comp10ks.text.str.replace('\\n', '')\n",
    "    comp10ks['text'] = comp10ks.text.str.replace('\\t', '')\n",
    "    comp10ks['text'] = comp10ks.text.str.replace('&nbsp;','')\n",
    "    # Pull out fields\n",
    "    comp10ks['bus_address'] = comp10ks.text.str.extract('business address: street 1:(.*?)mail address')\n",
    "    comp10ks['mail_address'] = comp10ks.text.str.extract('mail address: street 1:(.*?)10-k')\n",
    "    # Scrape out risk text using url\n",
    "    comp10ks['riskfac'] = comp10ks.text.str.extract('item 1a(.*?)item 2')\n",
    "    comp10ks['mandisc'] = comp10ks.text.str.extract('item 7(.*?)item 8')\n",
    "    comp10ks['fullrisktext'] = comp10ks['riskfac'] + comp10ks['mandisc']\n",
    "    # Pull out stopwords\n",
    "    comp10ks['text'] = comp10ks['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stops]))\n",
    "    comp10ks['fullrisktext'] = comp10ks['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stops]))\n",
    "    # Tokenize\n",
    "    comp10ks['words'] = comp10ks['text'].map(preprocess)\n",
    "    # Create Bag of Words and convert to Pandas DataFrame\n",
    "    rows = list()\n",
    "    for row in comp10ks[['compname', 'year', 'words']].iterrows():\n",
    "        r = row[1]\n",
    "        for word in r.words:\n",
    "            rows.append((r.compname, r.year, word))\n",
    "    words = pd.DataFrame(rows, columns=['compname', 'year', 'word'])\n",
    "    ##### Use Case 1: Total Word Counts by Doc\n",
    "    # Calculate total number of words by doc\n",
    "    counts_summary = words.groupby(['compname','year']).size().reset_index(name='count')\n",
    "    ### Use Case 2: Word Counts for Risk/Uncertainty Language\n",
    "    # Lay out the list of the words that we're looking for \n",
    "    wordslist = ['anticipate', 'believe', 'depend', 'fluctuate', 'indefinite', 'likelihood', 'possible', 'predict', 'risk', 'uncertain']\n",
    "    # Subset bag of words by risk/uncertainty words\n",
    "    word_risk = counts_full[counts_full['word'].isin(wordslist)]\n",
    "    word_risk_wide = word_risk.groupby(['compname', 'year', 'word'])['count'].sum().unstack('word')\n",
    "    word_risk_wide = word_risk_wide[['anticipate', 'believe', 'depend', 'fluctuate', 'likelihood', 'possible', 'predict', 'risk', \n",
    "            'uncertain']].fillna(value=0)\n",
    "    # USe Case 3: Risk Topic Modeling\n",
    "    # Tokenize and Preprocess\n",
    "    doc_set = comp10ks['fullrisktext']\n",
    "    token_text = list(build_text(doc_set.dropna()))\n",
    "    # Create Bigrams \n",
    "    bigram = gensim.models.Phrases(token_text)\n",
    "    # Process Texts\n",
    "    texts_clean = process_texts(token_text)\n",
    "    # Create Dictionary and Corpus \n",
    "    dictionary = Dictionary(texts_clean)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "    # Run the LDA Model with the optimal number of topics\n",
    "    lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=20)\n",
    "    opt = pd.DataFrame(numtopics, c_v)\n",
    "    opt = opt.reset_index()\n",
    "    s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "    lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "    # Prep for merging LDA models into broader dataframe \n",
    "    doc_lda = ldamodel[corpus]\n",
    "    # Prep for Mallet's LDA Version \n",
    "    # Create function for Mallet's\n",
    "    def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10ks):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['risk_dominant_topic', 'risk_perc_contribution', 'risk_topic_keywords']\n",
    "        # Add original text to the end of the output\n",
    "        data = pd.Series(comp10ks.text.values.tolist())\n",
    "        compname = pd.Series(comp10ks.compname.values.tolist())\n",
    "        year = pd.Series(comp10ks.year.values.tolist())\n",
    "        sent_topics_df = pd.concat([sent_topics_df, data, compname, year], axis=1)\n",
    "        return(sent_topics_df)\n",
    "    # Run Mallet's LDA Version \n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10ks)\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['risk_document_no', 'risk_dominant_topic', 'risk_topic_perc_contrib', 'risk_keywords', 'fullrisktext' ,'compname', 'year']\n",
    "    # Merge with Original Data Frame \n",
    "    Final_LDA = df_dominant_topic.merge(comp10ks, left_on=['compname', 'year'], right_on=['compname', 'year'], how='left')\n",
    "    # Drop Storage Heavy Text Column \n",
    "    Final_LDA = Final_LDA[['risk_document_no', 'risk_dominant_topic', 'risk_topic_perc_contrib', 'risk_keywords', 'compname', 'year']]\n",
    "    # Merge all together\n",
    "    # Merge counts summaries and risk words\n",
    "    Words_10k = counts_summary.merge(word_risk_wide, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    # Merge back with original info \n",
    "    comp10ks_2 = comp10ks[['year', 'compname', 'bus_address', 'mail_address']]\n",
    "    Words_10k_2 = comp10ks_2.merge(Words_10k, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    Text_10k_analysis = Words_10k_2.merge(Final_LDA, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    del [[comp10ks, counts_summary, words, word_risk, word_risk_wide, df_dominant_topic, opt, texts_clean, s]]\n",
    "    gc.collect()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(Text_10k_analysis, os.path.join(args.model_dir, \"Text_10k_analysis.joblib\"))\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return results\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    Text_10k_analysis = joblib.load(os.path.join(model_dir, \"Text_10k_analysis.joblib\"))\n",
    "    return Text_10k_analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/mxnet_mnist_byom/mxnet_mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import re\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import time\n",
    "\n",
    "# The method below collects the data from S3 along with a dictionary of \n",
    "# hyperparameters which only contains learning rate, and other parameters. (Figure out)\n",
    "## DON'T REALLY NEED TO CODE THIS BECAUSE YOU CAN CREATE JOBS IN SAGEMAKER IF YOU WANT\n",
    "\n",
    "def train(data_list, hyperparameters= {'learning_rate': 0.11}, num_cpus=0, num_gpus =1 , **kwargs):\n",
    "    start_time = time.time()\n",
    "    comp10ks = []\n",
    "    for i in range(0,len(data_list)):\n",
    "        key = data_list[i].split('/')[2]\n",
    "        data_location = 's3://{}/{}/{}'.format(bucket, data_key, key)\n",
    "        try:\n",
    "            s = pd.read_csv(data_location, delimiter = None, error_bad_lines = False, header = None, warn_bad_lines=False)\n",
    "            text_file = s[0].str.cat(sep=' ')\n",
    "            comp10ks.append([data_list[i].split('/')[2].split('_')[0], ' '.join(data_list[i].split('/')[2].split('_')[3:]),text_file])\n",
    "        except:\n",
    "            pass  \n",
    "    # Convert All to Pandas Dataframe \n",
    "    cols = ['year', 'compname', \"text\"]\n",
    "    comp10ks = pd.DataFrame(comp10ks, columns=cols)\n",
    "    comp10ks.Year = pd.to_datetime(comp10ks.year).dt.to_period('y')\n",
    "    \n",
    "    #Need to look into how to best structure the hyperparameters\n",
    "    # Preprocesing\n",
    "    comp10ks['text'] = comp10ks['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "    comp10ks['text'] = comp10ks.text.str.replace(r'<.*?>', '', regex= True)\n",
    "    comp10ks['text'] = comp10ks.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "    # Convert the titles to lowercase\n",
    "    comp10ks['text'] = comp10ks['text'].map(lambda x: x.lower())\n",
    "    comp10ks['text'] = comp10ks.text.str.replace('\\n', '')\n",
    "    comp10ks['text'] = comp10ks.text.str.replace('\\t', '')\n",
    "    comp10ks['text'] = comp10ks.text.str.replace('&nbsp;','')\n",
    "    # Pull out fields\n",
    "    comp10ks['bus_address'] = comp10ks.text.str.extract('business address: street 1:(.*?)mail address')\n",
    "    comp10ks['mail_address'] = comp10ks.text.str.extract('mail address: street 1:(.*?)10-k')\n",
    "    # Scrape out risk text using url\n",
    "    comp10ks['riskfac'] = comp10ks.text.str.extract('item 1a(.*?)item 2')\n",
    "    comp10ks['mandisc'] = comp10ks.text.str.extract('item 7(.*?)item 8')\n",
    "    comp10ks['fullrisktext'] = comp10ks['riskfac'] + comp10ks['mandisc']\n",
    "    \n",
    "    # Tokenize and Preprocess\n",
    "    comp10ks['words'] = comp10ks['text'].map(preprocess)\n",
    "    # Create Bag of Words and convert to Pandas DataFrame\n",
    "    rows = list()\n",
    "    for row in comp10ks[['compname', 'year', 'words']].iterrows():\n",
    "        r = row[1]\n",
    "        for word in r.words:\n",
    "            rows.append((r.compname, r.year, word))\n",
    "    words = pd.DataFrame(rows, columns=['compname', 'year', 'word'])\n",
    "    ##### Use Case 1: Total Word Counts by Doc\n",
    "    # Calculate total number of words by doc\n",
    "    counts_summary = words.groupby(['compname','year']).size().reset_index(name='count')\n",
    "    # Total Document Word Counts \n",
    "    counts_full = words.groupby(['compname','year', 'word']).size().reset_index(name='count')\n",
    "    ### Use Case 2: Word Counts for Risk/Uncertainty Language\n",
    "    # Lay out the list of the words that we're looking for \n",
    "    wordslist = ['anticipate', 'believe', 'depend', 'fluctuate', 'indefinite', 'likelihood', 'possible', 'predict', 'risk', 'uncertain']\n",
    "    # Subset bag of words by risk/uncertainty words\n",
    "    word_risk = counts_full[counts_full['word'].isin(wordslist)]\n",
    "    word_risk_wide = word_risk.groupby(['compname', 'year', 'word'])['count'].sum().unstack('word')\n",
    "    word_risk_wide = word_risk_wide[['anticipate', 'believe', 'depend', 'fluctuate', 'likelihood', 'possible', 'predict', 'risk', \n",
    "            'uncertain']].fillna(value=0)\n",
    "    # Lemmatize - Come back and fix\n",
    "    # comp10kbizdes['Text'] = map(lemma.lemmatize, comp10kbizdes['Text']\n",
    "    # Tokenize and Preprocess\n",
    "    doc_set = comp10ks['fullrisktext']\n",
    "    token_text = list(build_text(doc_set.dropna()))\n",
    "    # Create Bigrams \n",
    "    bigram = gensim.models.Phrases(token_text)\n",
    "    # Process Texts\n",
    "    texts_clean = process_texts(token_text)\n",
    "    # Create Dictionary and Corpus \n",
    "    dictionary = Dictionary(texts_clean)\n",
    "    #dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000) # 15 might be prob with only 116\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "    # Run the LDA Model with the optimal number of topics\n",
    "    lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=10)\n",
    "    opt = pd.DataFrame(numtopics, c_v)\n",
    "    opt = opt.reset_index()\n",
    "    s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "    lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "    # Prep for merging LDA models into broader dataframe \n",
    "    doc_lda = ldamodel[corpus]\n",
    "    # Prep for Mallet's LDA Version \n",
    "\n",
    "    # Create function for Mallet's\n",
    "    def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10ks):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['risk_dominant_topic', 'risk_perc_contribution', 'risk_topic_keywords']\n",
    "        # Add original text to the end of the output\n",
    "        data = pd.Series(comp10ks.text.values.tolist())\n",
    "        compname = pd.Series(comp10ks.compname.values.tolist())\n",
    "        year = pd.Series(comp10ks.year.values.tolist())\n",
    "        sent_topics_df = pd.concat([sent_topics_df, data, compname, year], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    # Run Mallet's LDA Version \n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10ks)\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['risk_document_no', 'risk_dominant_topic', 'risk_topic_perc_contrib', 'risk_keywords', 'fullrisktext' ,'compname', 'year']\n",
    "    # Merge with Original Data Frame \n",
    "    Final_LDA = df_dominant_topic.merge(comp10ks, left_on=['compname', 'year'], right_on=['compname', 'year'], how='left')\n",
    "    # Drop Storage Heavy Text Column \n",
    "    Final_LDA = Final_LDA[['risk_document_no', 'risk_dominant_topic', 'risk_topic_perc_contrib', 'risk_keywords', 'compname', 'year']]\n",
    "    # Merge all together\n",
    "    # Merge counts summaries and risk words\n",
    "    Words_10k = counts_summary.merge(word_risk_wide, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    # Merge back with original info \n",
    "    comp10ks_2 = comp10ks[['year', 'compname', 'bus_address', 'mail_address']]\n",
    "    Words_10k_2 = comp10ks_2.merge(Words_10k, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    Text_10k_analysis = Words_10k_2.merge(Final_LDA, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    # Merge counts summaries and risk words\n",
    "    del [[comp10ks, counts_full, counts_summary, words, word_risk, word_risk_wide, df_dominant_topic, data, opt, texts_clean, s, comp10ks_2, Words_10k_2]]\n",
    "    gc.collect()\n",
    "    return Text_10k_analysis\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/pipe_bring_your_own/pipe_bring_your_own.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import re\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "comp10ks = []\n",
    "for i in range(0,len(data_list)):\n",
    "    key = data_list[i].split('/')[2]\n",
    "    data_location = 's3://{}/{}/{}'.format(bucket, data_key, key)\n",
    "    try:\n",
    "        s = pd.read_csv(data_location, delimiter = None, error_bad_lines = False, header = None, warn_bad_lines=False)\n",
    "        text_file = s[0].str.cat(sep=' ')\n",
    "        comp10ks.append([data_list[i].split('/')[2].split('_')[0], ' '.join(data_list[i].split('/')[2].split('_')[3:]),text_file])\n",
    "    except:\n",
    "        pass  \n",
    "# Convert All to Pandas Dataframe \n",
    "cols = ['year', 'compname', \"text\"]\n",
    "comp10ks = pd.DataFrame(comp10ks, columns=cols)\n",
    "comp10ks.Year = pd.to_datetime(comp10ks.year).dt.to_period('y')\n",
    "\n",
    "#Need to look into how to best structure the hyperparameters\n",
    "# Preprocesing\n",
    "comp10ks['text'] = comp10ks['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "comp10ks['text'] = comp10ks.text.str.replace(r'<.*?>', '', regex= True)\n",
    "comp10ks['text'] = comp10ks.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "# Convert the titles to lowercase\n",
    "comp10ks['text'] = comp10ks['text'].map(lambda x: x.lower())\n",
    "comp10ks['text'] = comp10ks.text.str.replace('\\n', '')\n",
    "comp10ks['text'] = comp10ks.text.str.replace('\\t', '')\n",
    "comp10ks['text'] = comp10ks.text.str.replace('&nbsp;','')\n",
    "# Pull out fields\n",
    "comp10ks['bus_address'] = comp10ks.text.str.extract('business address: street 1:(.*?)mail address')\n",
    "comp10ks['mail_address'] = comp10ks.text.str.extract('mail address: street 1:(.*?)10-k')\n",
    "# Scrape out risk text using url\n",
    "comp10ks['riskfac'] = comp10ks.text.str.extract('item 1a(.*?)item 2')\n",
    "comp10ks['mandisc'] = comp10ks.text.str.extract('item 7(.*?)item 8')\n",
    "comp10ks['fullrisktext'] = comp10ks['riskfac'] + comp10ks['mandisc']\n",
    "\n",
    "# Tokenize and Preprocess\n",
    "comp10ks['words'] = comp10ks['text'].map(preprocess)\n",
    "# Create Bag of Words and convert to Pandas DataFrame\n",
    "rows = list()\n",
    "for row in comp10ks[['compname', 'year', 'words']].iterrows():\n",
    "    r = row[1]\n",
    "    for word in r.words:\n",
    "        rows.append((r.compname, r.year, word))\n",
    "words = pd.DataFrame(rows, columns=['compname', 'year', 'word'])\n",
    "##### Use Case 1: Total Word Counts by Doc\n",
    "# Calculate total number of words by doc\n",
    "counts_summary = words.groupby(['compname','year']).size().reset_index(name='count')\n",
    "# Total Document Word Counts \n",
    "counts_full = words.groupby(['compname','year', 'word']).size().reset_index(name='count')\n",
    "### Use Case 2: Word Counts for Risk/Uncertainty Language\n",
    "# Lay out the list of the words that we're looking for \n",
    "wordslist = ['anticipate', 'believe', 'depend', 'fluctuate', 'indefinite', 'likelihood', 'possible', 'predict', 'risk', 'uncertain']\n",
    "# Subset bag of words by risk/uncertainty words\n",
    "word_risk = counts_full[counts_full['word'].isin(wordslist)]\n",
    "word_risk_wide = word_risk.groupby(['compname', 'year', 'word'])['count'].sum().unstack('word')\n",
    "word_risk_wide = word_risk_wide[['anticipate', 'believe', 'depend', 'fluctuate', 'likelihood', 'possible', 'predict', 'risk', \n",
    "        'uncertain']].fillna(value=0)\n",
    "# Lemmatize - Come back and fix\n",
    "# comp10kbizdes['Text'] = map(lemma.lemmatize, comp10kbizdes['Text']\n",
    "# Tokenize and Preprocess\n",
    "doc_set = comp10ks['fullrisktext']\n",
    "token_text = list(build_text(doc_set.dropna()))\n",
    "# Create Bigrams \n",
    "bigram = gensim.models.Phrases(token_text)\n",
    "# Process Texts\n",
    "texts_clean = process_texts(token_text)\n",
    "# Create Dictionary and Corpus \n",
    "dictionary = Dictionary(texts_clean)\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000) # 15 might be prob with only 116\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "# Run the LDA Model with the optimal number of topics\n",
    "lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=10)\n",
    "opt = pd.DataFrame(numtopics, c_v)\n",
    "opt = opt.reset_index()\n",
    "s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "# Prep for merging LDA models into broader dataframe \n",
    "doc_lda = ldamodel[corpus]\n",
    "# Prep for Mallet's LDA Version \n",
    "\n",
    "# Create function for Mallet's\n",
    "def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10ks):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['risk_dominant_topic', 'risk_perc_contribution', 'risk_topic_keywords']\n",
    "    # Add original text to the end of the output\n",
    "    data = pd.Series(comp10ks.text.values.tolist())\n",
    "    compname = pd.Series(comp10ks.compname.values.tolist())\n",
    "    year = pd.Series(comp10ks.year.values.tolist())\n",
    "    sent_topics_df = pd.concat([sent_topics_df, data, compname, year], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Run Mallet's LDA Version \n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10ks)\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['risk_document_no', 'risk_dominant_topic', 'risk_topic_perc_contrib', 'risk_keywords', 'fullrisktext' ,'compname', 'year']\n",
    "# Merge with Original Data Frame \n",
    "Final_LDA = df_dominant_topic.merge(comp10ks, left_on=['compname', 'year'], right_on=['compname', 'year'], how='left')\n",
    "# Drop Storage Heavy Text Column \n",
    "Final_LDA = Final_LDA[['risk_document_no', 'risk_dominant_topic', 'risk_topic_perc_contrib', 'risk_keywords', 'compname', 'year']]\n",
    "# Merge all together\n",
    "# Merge counts summaries and risk words\n",
    "Words_10k = counts_summary.merge(word_risk_wide, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                          how='right')\n",
    "# Merge back with original info \n",
    "comp10ks_2 = comp10ks[['year', 'compname', 'bus_address', 'mail_address']]\n",
    "Words_10k_2 = comp10ks_2.merge(Words_10k, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                          how='right')\n",
    "Text_10k_analysis = Words_10k_2.merge(Final_LDA, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                          how='right')\n",
    "# Merge counts summaries and risk words\n",
    "del [[comp10ks, counts_full, counts_summary, words, word_risk, word_risk_wide, df_dominant_topic, data, opt, texts_clean, s, comp10ks_2, Words_10k_2]]\n",
    "gc.collect()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/scikit_learn_iris/scikit_learn_iris.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #### Parse Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    ###### All operations start here\n",
    "    start_time = time.time()\n",
    "    #### Bring in the data\n",
    "    data_list = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    comp10ks = []\n",
    "    for i in range(0,len(data_list)):\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    else: \n",
    "        try:\n",
    "        s = pd.read_csv(data_location, delimiter = None, error_bad_lines = False, header = None, warn_bad_lines=False)\n",
    "        text_file = s[0].str.cat(sep=' ')\n",
    "        comp10ks.append([data_list[i].split('/')[2].split('_')[0], ' '.join(data_list[i].split('/')[2].split('_')[3:]),text_file])\n",
    "        except:\n",
    "            pass  \n",
    "# Convert All to Pandas Dataframe \n",
    "cols = ['year', 'compname', \"text\"]\n",
    "comp10kbizdes = pd.DataFrame(comp10kbizdes, columns=cols)\n",
    "comp10kbizdes.year = pd.to_datetime(comp10kbizdes.year).dt.to_period('y')\n",
    "# Preprocesing\n",
    "comp10kbizdes['text'] = comp10kbizdes['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace(r'<.*?>', '', regex= True)\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "# Convert the titles to lowercase\n",
    "comp10kbizdes['text'] = comp10kbizdes['text'].map(lambda x: x.lower())\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace('\\n', '')\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace('\\t', '')\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace('&nbsp;','')\n",
    "# Lemmatize - Come back and fix\n",
    "# comp10kbizdes['Text'] = map(lemma.lemmatize, comp10kbizdes['Text']\n",
    "# Tokenize and Preprocess\n",
    "doc_set = comp10kbizdes['text']\n",
    "token_text = list(build_text(doc_set.dropna()))\n",
    "# Create Bigrams \n",
    "bigram = gensim.models.Phrases(token_text)\n",
    "# Process Texts\n",
    "texts_clean = process_texts(token_text)\n",
    "# Create Dictionary and Corpus \n",
    "dictionary = Dictionary(texts_clean)\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000) # 15 might be prob with only 116\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "# Run the LDA Model with the optimal number of topics\n",
    "lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=10)\n",
    "opt = pd.DataFrame(numtopics, c_v)\n",
    "opt = opt.reset_index()\n",
    "s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "# Prep for merging LDA models into broader dataframe \n",
    "doc_lda = ldamodel[corpus]\n",
    "# Prep for Mallet's LDA Version \n",
    "\n",
    "# Create function for Mallet's\n",
    "def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10kbizdes):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['bizdes_dominant_topic', 'bizdes_perc_contribution', 'bizdes_topic_keywords']\n",
    "    # Add original text to the end of the output\n",
    "    data = pd.Series(comp10kbizdes.text.values.tolist())\n",
    "    compname = pd.Series(comp10kbizdes.compname.values.tolist())\n",
    "    year = pd.Series(comp10kbizdes.year.values.tolist())\n",
    "    sent_topics_df = pd.concat([sent_topics_df, data, compname, year], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Run Mallet's LDA Version \n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10kbizdes)\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['bizdes_document_no', 'bizdes_dominant_topic', 'bizdes_topic_perc_contrib', 'bizdes_keywords' ,'compname', 'year']\n",
    "# Merge with Original Data Frame \n",
    "Final_LDA = df_dominant_topic.merge(comp10kbizdes, left_on=['compname', 'year'], right_on=['compname', 'year'], how='left')\n",
    "# Drop Storage Heavy Text Column \n",
    "Final_LDA_bt = Final_LDA[['bizdes_document_no', 'bizdes_dominant_topic', 'bizdes_topic_perc_contrib', 'bizdes_keywords', 'compname', 'year']]\n",
    "\n",
    "Text_bizdes_analysis = Text_10k_analysis.merge(Final_LDA_bt, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                          how='right')\n",
    "del [[comp10kbizdes, counts_summary, words, word_risk, word_risk_wide, df_dominant_topic, opt, texts_clean, s, Final_LDA_bt]]\n",
    "gc.collect()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# Print the coefficients of the trained classifier, and save the coefficients\n",
    "joblib.dump(Text_bizdes_analysis, os.path.join(args.model_dir, \"Text_bizdes_analysis.joblib\"))\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return results\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    Text_bizdes_analysis = joblib.load(os.path.join(model_dir, \"Text_bizdes_analysis.joblib\"))\n",
    "    return Text_bizdes_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import re\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import time\n",
    "\n",
    "# The method below collects the data from S3 along with a dictionary of \n",
    "# hyperparameters which only contains learning rate, and other parameters. (Figure out)\n",
    "\n",
    "def train(data_list, hyperparameters= {'learning_rate': 0.11}, num_cpus=0, num_gpus =1 , **kwargs):\n",
    "    start_time = time.time()\n",
    "    comp10ks = []\n",
    "    for i in range(0,len(data_list)):\n",
    "        key = data_list[i].split('/')[2]\n",
    "        data_location = 's3://{}/{}/{}'.format(bucket, data_key, key)\n",
    "        try:\n",
    "            s = pd.read_csv(data_location, delimiter = None, error_bad_lines = False, header = None, warn_bad_lines=False)\n",
    "            text_file = s[0].str.cat(sep=' ')\n",
    "            comp10ks.append([data_list[i].split('/')[2].split('_')[0], ' '.join(data_list[i].split('/')[2].split('_')[3:]),text_file])\n",
    "        except:\n",
    "            pass  \n",
    "        # Convert All to Pandas Dataframe \n",
    "    cols = ['year', 'compname', \"text\"]\n",
    "    comp10kbizdes = pd.DataFrame(comp10kbizdes, columns=cols)\n",
    "    comp10kbizdes.year = pd.to_datetime(comp10kbizdes.year).dt.to_period('y')\n",
    "    # Preprocesing\n",
    "    comp10kbizdes['text'] = comp10kbizdes['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "    comp10kbizdes['text'] = comp10kbizdes.text.str.replace(r'<.*?>', '', regex= True)\n",
    "    comp10kbizdes['text'] = comp10kbizdes.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "    # Convert the titles to lowercase\n",
    "    comp10kbizdes['text'] = comp10kbizdes['text'].map(lambda x: x.lower())\n",
    "    comp10kbizdes['text'] = comp10kbizdes.text.str.replace('\\n', '')\n",
    "    comp10kbizdes['text'] = comp10kbizdes.text.str.replace('\\t', '')\n",
    "    comp10kbizdes['text'] = comp10kbizdes.text.str.replace('&nbsp;','')\n",
    "    # Lemmatize - Come back and fix\n",
    "    # comp10kbizdes['Text'] = map(lemma.lemmatize, comp10kbizdes['Text']\n",
    "    # Tokenize and Preprocess\n",
    "    doc_set = comp10kbizdes['text']\n",
    "    token_text = list(build_text(doc_set.dropna()))\n",
    "    # Create Bigrams \n",
    "    bigram = gensim.models.Phrases(token_text)\n",
    "    # Process Texts\n",
    "    texts_clean = process_texts(token_text)\n",
    "    # Create Dictionary and Corpus \n",
    "    dictionary = Dictionary(texts_clean)\n",
    "    #dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000) # 15 might be prob with only 116\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "    # Run the LDA Model with the optimal number of topics\n",
    "    lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=10)\n",
    "    opt = pd.DataFrame(numtopics, c_v)\n",
    "    opt = opt.reset_index()\n",
    "    s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "    lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "    # Prep for merging LDA models into broader dataframe \n",
    "    doc_lda = ldamodel[corpus]\n",
    "    # Prep for Mallet's LDA Version \n",
    "\n",
    "    # Create function for Mallet's\n",
    "    def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10kbizdes):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        # Get main topic in each document\n",
    "        for i, row in enumerate(ldamodel[corpus]):\n",
    "            row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['bizdes_dominant_topic', 'bizdes_perc_contribution', 'bizdes_topic_keywords']\n",
    "        # Add original text to the end of the output\n",
    "        data = pd.Series(comp10kbizdes.text.values.tolist())\n",
    "        compname = pd.Series(comp10kbizdes.compname.values.tolist())\n",
    "        year = pd.Series(comp10kbizdes.year.values.tolist())\n",
    "        sent_topics_df = pd.concat([sent_topics_df, data, compname, year], axis=1)\n",
    "        return(sent_topics_df)\n",
    "\n",
    "    # Run Mallet's LDA Version \n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10kbizdes)\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['bizdes_document_no', 'bizdes_dominant_topic', 'bizdes_topic_perc_contrib', 'bizdes_keywords' ,'compname', 'year']\n",
    "    # Merge with Original Data Frame \n",
    "    Final_LDA = df_dominant_topic.merge(comp10kbizdes, left_on=['compname', 'year'], right_on=['compname', 'year'], how='left')\n",
    "    # Drop Storage Heavy Text Column \n",
    "    Final_LDA_bt = Final_LDA[['bizdes_document_no', 'bizdes_dominant_topic', 'bizdes_topic_perc_contrib', 'bizdes_keywords', 'compname', 'year']]\n",
    "\n",
    "    Text_bizdes_analysis = Text_10k_analysis.merge(Final_LDA_bt, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                              how='right')\n",
    "    del [[comp10kbizdes, counts_summary, words, word_risk, word_risk_wide, df_dominant_topic, opt, texts_clean, s, Final_LDA_bt]]\n",
    "    gc.collect()\n",
    "    return Text_bizdes_analysis\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import without parsing first\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "\n",
    "comp10kbizdes = []\n",
    "\n",
    "for i in range(0,len(data_list)):\n",
    "    key = data_list[i].split('/')[2]\n",
    "    data_location = 's3://{}/{}/{}'.format(bucket, data_key, key)\n",
    "    try:\n",
    "        s = pd.read_csv(data_location, delimiter = None, error_bad_lines = False, header = None, warn_bad_lines=False)\n",
    "        text_file = s[0].str.cat(sep=' ')\n",
    "        comp10ks.append([data_list[i].split('/')[2].split('_')[0], ' '.join(data_list[i].split('/')[2].split('_')[3:]),text_file])\n",
    "    except:\n",
    "        pass   \n",
    "# Convert All to Pandas Dataframe \n",
    "cols = ['year', 'compname', \"text\"]\n",
    "comp10kbizdes = pd.DataFrame(comp10kbizdes, columns=cols)\n",
    "comp10kbizdes.year = pd.to_datetime(comp10kbizdes.year).dt.to_period('y')\n",
    "# Preprocesing\n",
    "comp10kbizdes['text'] = comp10kbizdes['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace(r'<.*?>', '', regex= True)\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "# Convert the titles to lowercase\n",
    "comp10kbizdes['text'] = comp10kbizdes['text'].map(lambda x: x.lower())\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace('\\n', '')\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace('\\t', '')\n",
    "comp10kbizdes['text'] = comp10kbizdes.text.str.replace('&nbsp;','')\n",
    "# Lemmatize - Come back and fix\n",
    "# comp10kbizdes['Text'] = map(lemma.lemmatize, comp10kbizdes['Text']\n",
    "# Tokenize and Preprocess\n",
    "doc_set = comp10kbizdes['text']\n",
    "token_text = list(build_text(doc_set.dropna()))\n",
    "# Create Bigrams \n",
    "bigram = gensim.models.Phrases(token_text)\n",
    "# Process Texts\n",
    "texts_clean = process_texts(token_text)\n",
    "# Create Dictionary and Corpus \n",
    "dictionary = Dictionary(texts_clean)\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000) # 15 might be prob with only 116\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "# Run the LDA Model with the optimal number of topics\n",
    "lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=10)\n",
    "opt = pd.DataFrame(numtopics, c_v)\n",
    "opt = opt.reset_index()\n",
    "s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "# Prep for merging LDA models into broader dataframe \n",
    "doc_lda = ldamodel[corpus]\n",
    "# Prep for Mallet's LDA Version \n",
    "\n",
    "# Create function for Mallet's\n",
    "def format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10kbizdes):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['bizdes_dominant_topic', 'bizdes_perc_contribution', 'bizdes_topic_keywords']\n",
    "    # Add original text to the end of the output\n",
    "    data = pd.Series(comp10kbizdes.text.values.tolist())\n",
    "    compname = pd.Series(comp10kbizdes.compname.values.tolist())\n",
    "    year = pd.Series(comp10kbizdes.year.values.tolist())\n",
    "    sent_topics_df = pd.concat([sent_topics_df, data, compname, year], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "# Run Mallet's LDA Version \n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, data=comp10kbizdes)\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['bizdes_document_no', 'bizdes_dominant_topic', 'bizdes_topic_perc_contrib', 'bizdes_keywords' ,'compname', 'year']\n",
    "# Merge with Original Data Frame \n",
    "Final_LDA = df_dominant_topic.merge(comp10kbizdes, left_on=['compname', 'year'], right_on=['compname', 'year'], how='left')\n",
    "# Drop Storage Heavy Text Column \n",
    "Final_LDA_bt = Final_LDA[['bizdes_document_no', 'bizdes_dominant_topic', 'bizdes_topic_perc_contrib', 'bizdes_keywords', 'compname', 'year']]\n",
    "\n",
    "Words_10k_3 = Text_10k_analysis.merge(Final_LDA_bt, left_on=['compname', 'year'], right_on=['compname', 'year'], \n",
    "                          how='right')\n",
    "\n",
    "del [[comp10kbizdes, counts_summary, words, word_risk, word_risk_wide, df_dominant_topic, opt, texts_clean, s, Final_LDA_bt]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_10K_Analysis = wordrisk.merge(Final_LDA_bt, left_on=['CIK', 'Year'], right_on=['CIK', 'Year'], \n",
    "                                      how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Geo Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy \n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "Master_10K_Analysis['Coordinates'] = Master_10K_Analysis['Address'].apply(geolocator.geocode)\n",
    "\n",
    "Master_10K_Analysis['Latitude'] = Master_10K_Analysis.Coordinates.apply(lambda x: (x.latitude) if x is not None else x)\n",
    "Master_10K_Analysis['Longitude'] = Master_10K_Analysis.Coordinates.apply(lambda x: (x.longitude) if x is not None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_10K_Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='sec-edw-dev-landing-zone'\n",
    "data_key = 'TBD'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "# code you already have, saving the file locally to whatever directory you wish\n",
    "file_name = \"mydata.csv\" \n",
    "Master_10K_Analysis.to_csv(file_name)\n",
    "# instantiate S3 client and upload to s3\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(file_name, 'YOUR_S3_BUCKET_NAME', 'DESIRED_S3_OBJECT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Saving an ML Model to an S3 Bucket Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This signifies that print_function is from a future build \n",
    "# It's basically the equivalent of \"I guess you're not ready for this, but your kids are gonna love it\". \n",
    "#will allow you to use print as a function:\n",
    "#print('# of entries', len(dictionary), file=sys.stderr)\n",
    "from __future__ import print_function\n",
    "\n",
    "#The argparse module makes it easy to write user-friendly command-line interfaces.\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# This is just about ML here, so we probably don't need\n",
    "#from sklearn import tree\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    \n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "    # So they are passing parameters here instead of within the function itself?\n",
    "    parser.add_argument('--max_leaf_nodes', type=int, default=-1)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    # We're continuing to add_arumgnets to the parser - here we're just saying where it comes from and where it \n",
    "    # should go to\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    # So we're activating here \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    # Get all the file names from the appropriate directory locations \n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    # Ensure that a rogue 0 length file doesn't screw it up\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    # Read in a bunch of files together\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    # Bind output into a dataframe \n",
    "    train_data = pd.concat(raw_data)\n",
    "    \n",
    "    # labels are in the first column - Get target variable and predictors \n",
    "    train_y = train_data.ix[:,0]\n",
    "    train_X = train_data.ix[:,1:]\n",
    "    \n",
    "    # Here we support a single hyperparameter, 'max_leaf_nodes'. Note that you can add as many\n",
    "    # as your training my require in the ArgumentParser above.\n",
    "    #max_leaf_nodesint, default=None\n",
    "    #Grow a tree with max_leaf_nodes in best-first fashion. \n",
    "    #Best nodes are defined as relative reduction in impurity. \n",
    "    #If None then unlimited number of leaf nodes.\n",
    "    max_leaf_nodes = args.max_leaf_nodes\n",
    "    \n",
    "    # Now use scikit-learn's decision tree classifier to train the model.\n",
    "    # Seems redundent, but here is how you specify the parameters inside of the ML function\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "    \n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    # joblib.dump: Persist an arbitrary Python object into one file.\n",
    "    # We aee taking the results of the model and saving them to the model directory we set above. \n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "    \n",
    "    # This is just if we want to return a model \n",
    "    def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "# So get that this is a way to save a model to an S3 bucket in a traditional way. \n",
    "# Don't get what to do when preprocessing is involved or when you need it in a certain format. \n",
    "# Also don't get where --output-data-dir and --train play a role here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dockerizing an R Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "role = get_execution_role()\n",
    "# So the below would be the 10k-full-text prefix for me\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/DEMO-r-byo'\n",
    "\n",
    "# For this example, we'll need 3 supporting code files.\n",
    "# mars.R creates functions to fit and serve our model\n",
    "    # The algorithm we've chosen to use is Multivariate Adaptive Regression Splines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building out the R file and the docker file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The above means we are in command-line mode\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-rmars\n",
    "\n",
    "#set -e # stop if anything fails\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "# We'd use us-east-1\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "# Pulling out the ECR here \n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: Then to actually run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically the goal is to predict one of three flower species based on various measurements of the flowers' \n",
    "# attributes.\n",
    "\n",
    "# This oversimplifies because we have one file\n",
    "train_file = 'iris.csv'\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_file(train_file)\n",
    "\n",
    "# Now, let's setup the information needed to train a Multivariate Adaptive Regression Splines (MARS) model on iris data.\n",
    "\n",
    "# First, we'll get our region and account information so that we can point to the ECR container we just created.\n",
    "region = boto3.Session().region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify the role to use\n",
    "- Give the training job a name\n",
    "- Point the algorithm to the container we created\n",
    "- Specify training instance resources (in this case our algorithm is only single-threaded so stick to 1 instance)\n",
    "- Point to the S3 location of our input data and the train channel expected by our algorithm\n",
    "- Point to the S3 location for output\n",
    "- Provide hyperparamters (keeping it simple)\n",
    "- Maximum run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_job = 'DEMO-r-byo-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "print(\"Training job\", r_job)\n",
    "\n",
    "r_training_params = {\n",
    "    \"RoleArn\": role,\n",
    "    \"TrainingJobName\": r_job,\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": '{}.dkr.ecr.{}.amazonaws.com/sagemaker-rmars:latest'.format(account, region),\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.xlarge\",\n",
    "        \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"target\": \"Sepal.Length\",\n",
    "        \"degree\": \"2\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 60\n",
    "    }\n",
    "}\n",
    "\n",
    "## DON'T REALLY NEED TO CODE THIS BECAUSE YOU CAN CREATE JOBS IN SAGEMAKER IF YOU WANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's kick off our training job on Amazon SageMaker Training, using the parameters we just created. Because training is managed (AWS takes care of spinning up and spinning down the hardware), we don't have to wait for our job to finish to continue, but for this case, let's setup a waiter so we can monitor the status of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "sm.create_training_job(**r_training_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=r_job)['TrainingJobStatus']\n",
    "print(status)\n",
    "sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=r_job)\n",
    "status = sm.describe_training_job(TrainingJobName=r_job)['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "if status == 'Failed':\n",
    "    message = sm.describe_training_job(TrainingJobName=r_job)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hosting the model we just trained takes three steps in Amazon SageMaker. First, we define the model we want to host, pointing the service to the model artifact our training job just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hosting_container = {\n",
    "    'Image': '{}.dkr.ecr.{}.amazonaws.com/sagemaker-rmars:latest'.format(account, region),\n",
    "    'ModelDataUrl': sm.describe_training_job(TrainingJobName=r_job)['ModelArtifacts']['S3ModelArtifacts']\n",
    "}\n",
    "\n",
    "# I.e. once we already have the model and want to predict\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=r_job,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=r_hosting_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an endpoing configuration, passing in the model we just registered. In this case, we'll only use a few c4.xlarges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_endpoint_config = 'DEMO-r-byo-config-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(r_endpoint_config)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=r_endpoint_config,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.m4.xlarge',\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': r_job,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create the endpoints using our endpoint configuration from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r_endpoint = 'DEMO-r-endpoint-' + time.strftime(\"%Y%m%d%H%M\", time.gmtime())\n",
    "print(r_endpoint)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=r_endpoint,\n",
    "    EndpointConfigName=r_endpoint_config)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=r_endpoint)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sm.get_waiter('endpoint_in_service').wait(EndpointName=r_endpoint)\n",
    "finally:\n",
    "    resp = sm.describe_endpoint(EndpointName=r_endpoint)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict: To confirm our endpoints are working properly, let's try to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = pd.read_csv('iris.csv')\n",
    "\n",
    "runtime = boto3.Session().client('runtime.sagemaker')\n",
    "\n",
    "payload = iris.drop(['Sepal.Length'], axis=1).to_csv(index=False)\n",
    "response = runtime.invoke_endpoint(EndpointName=r_endpoint,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=payload)\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result\n",
    "\n",
    "\n",
    "plt.scatter(iris['Sepal.Length'], np.fromstring(result[0], sep=','))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build your own algorihm container (Does not show you the script of what goes in)\n",
    "<br/>\n",
    "<br/>\n",
    "Because you can run the same image in training or hosting, Amazon SageMaker runs your container with the argument train or serve. How your container processes this argument depends on the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is the optimal file structure. \n",
    "# If we want test, we would just add to the data folder \n",
    "# resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training\n",
    "# Since scikit-learn doesn't support distributed training, we'll ignore it here.\n",
    "    # GOOD TO KNOW!\n",
    "# The channels are created based on the call to CreateTrainingJob but it's generally important that channels \n",
    "# match what the algorithm expects.\n",
    "# Hosting has a very different model than training because hosting is reponding to inference requests that \n",
    "# come in via HTTP. \n",
    "\n",
    "# Amazon SageMaker uses two URLs in the container:\n",
    "# /ping will receive GET requests from the infrastructure. \n",
    "# Your program returns 200 if the container is up and accepting requests.\n",
    "# /invocations is the endpoint that receives client inference POST requests.\n",
    "\n",
    "/opt/ml\n",
    "|-- input\n",
    "|   |-- config\n",
    "|   |   |-- hyperparameters.json\n",
    "|   |   `-- resourceConfig.json\n",
    "|   `-- data\n",
    "|       `-- <channel_name>\n",
    "|           `-- <input data>\n",
    "|-- model\n",
    "|   `-- <model files>\n",
    "`-- output\n",
    "    `-- failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the container directory are all the components you need to package the sample algorithm for Amazon SageMager:\n",
    "\n",
    ".\n",
    "|-- Dockerfile\n",
    "|-- build_and_push.sh\n",
    "`-- decision_trees\n",
    "    |-- nginx.conf\n",
    "    |-- predictor.py\n",
    "    |-- serve\n",
    "    |-- train\n",
    "    `-- wsgi.py\n",
    "    \n",
    "# nginx.conf is the configuration file for the nginx front-end. Generally, you should be able to take this file as-is.\n",
    "# predictor.py is the program that actually implements the Flask web server and the decision tree predictions for this app. \n",
    "    # You'll want to customize the actual prediction parts to your application. \n",
    "    # Since this algorithm is simple, we do all the processing here in this file, but you may choose to have separate \n",
    "    # files for implementing your custom logic.\n",
    "# serve is the program started when the container is started for hosting. It simply launches the gunicorn server \n",
    "    # which runs multiple instances of the Flask app defined in predictor.py. \n",
    "    # You should be able to take this file as-is.\n",
    "# train is the program that is invoked when the container is run for training. \n",
    "    # You will modify this program to implement your training algorithm.\n",
    "# wsgi.py is a small wrapper used to invoke the Flask app. You should be able to take this file as-is.\n",
    "\n",
    "# In summary, the two files you will probably want to change for your application are train and predictor.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Python science stack, we will start from a standard Ubuntu installation and run the normal tools to install the things needed by scikit-learn. Finally, we add the code that implements our specific algorithm to the container and set up the right environment to run under."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shell code shows how to build the container image using docker build and push the container image to ECR using docker push. This code is also available as the shell script container/build-and-push.sh, which you can run as build-and-push.sh decision_trees_sample to build the image decision_trees_sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-decision-trees\n",
    "\n",
    "cd container\n",
    "\n",
    "# In Unix and Unix-like operating systems, chmod is the command and system call which is used to change the \n",
    "# access permissions of file system objects.\n",
    "# So x here I think is the user getting permission\n",
    "chmod +x decision_trees/train\n",
    "chmod +x decision_trees/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "\n",
    "# Looks like this is fairly standard and is grabbing from places withoiut me needing to custom code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you're first packaging an algorithm use with Amazon SageMaker, you probably want to test it yourself to make sure it's working right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts are:\n",
    "\n",
    "- train_local.sh: Run this with the name of the image and it will run training on the local tree. For example, you can run $ ./train_local.sh sagemaker-decision-trees. It will generate a model under the /test_dir/model directory. You'll want to modify the directory test_dir/input/data/... to be set up with the correct channels and data for your algorithm. Also, you'll want to modify the file input/config/hyperparameters.json to have the hyperparameter settings that you want to test (as strings).\n",
    "- serve_local.sh: Run this with the name of the image once you've trained the model and it should serve the model. For example, you can run $ ./serve_local.sh sagemaker-decision-trees. It will run and wait for requests. Simply use the keyboard interrupt to stop it.\n",
    "- predict.sh: Run this with the name of a payload file and (optionally) the HTTP content type you want. The content type will default to text/csv. For example, you can run $ ./predict.sh payload.csv text/csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your container packaged, you can use it to train models and use the model for hosting or batch transforms. Let's do that with the algorithm we made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Environment \n",
    "# S3 prefix\n",
    "prefix = 'DEMO-scikit-byo-iris'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create the Session\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to default bucket\n",
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an estimator and fit the model\n",
    "<br/>\n",
    "In order to use SageMaker to fit our algorithm, we'll create an Estimator that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "<br/>\n",
    "- The container name. This is constructed as in the shell commands above.\n",
    "- The role. As defined above.\n",
    "- The instance count which is the number of machines to use for training.\n",
    "- The instance type which is the type of machine to use for training.\n",
    "- The output path determines where the model artifact will be written.\n",
    "- The session is the SageMaker session object that we defined above.\n",
    "<br/><br/>\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-decision-trees:latest'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c4.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tree.fit(data_location)\n",
    "\n",
    "# This is for a tree obviously\n",
    "\n",
    "## DON'T REALLY NEED TO CODE THIS BECAUSE YOU CAN CREATE JOBS IN SAGEMAKER IF YOU WANT\n",
    "## But you can (SDK is code way and Training Job is Console way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model\n",
    "Deploying the model to SageMaker hosting just requires a deploy call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose some data and use it for a prediction\n",
    "shape=pd.read_csv(\"data/iris.csv\", header=None)\n",
    "shape.sample(3)\n",
    "# drop the label column in the training set\n",
    "shape.drop(shape.columns[[0]],axis=1,inplace=True)\n",
    "shape.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "a = [50*i for i in range(3)]\n",
    "b = [40+i for i in range(10)]\n",
    "indices = [i+j for i,j in itertools.product(a,b)]\n",
    "\n",
    "test_data=shape.iloc[indices[:-1]]\n",
    "\n",
    "# Just creating a dataset \n",
    "\n",
    "# Prediction is as easy as calling predict with the predictor we got back from deploy and the data we want to do \n",
    "# predictions with. The serializers take care of doing the data conversions for us. \n",
    "\n",
    "print(predictor.predict(test_data.values).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Building a Tensorflow Container\n",
    "<br/>\n",
    "<br/>\n",
    "When you are using a framework such as Apache MXNet or TensorFlow that has direct support in SageMaker, you can simply supply the Python code that implements your algorithm using the SDK entry points for that framework. \n",
    "<br/><br/>\n",
    "Won't repeat what was already shown in the previous example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Python SDK Local Training\n",
    "To represent our training, we use the Estimator class, which needs to be configured in five steps.\n",
    "\n",
    "1. IAM role - our AWS execution role\n",
    "2. train_instance_count - number of instances to use for training.\n",
    "3. train_instance_type - type of instance to use for training. For training locally, we specify local.\n",
    "4. image_name - our custom TensorFlow Docker image we created.\n",
    "5. hyperparameters - hyperparameters we want to pass.\n",
    "<br/><br/>\n",
    "Let's start with setting up our IAM role. We make use of a helper function within the Python SDK. This function throw an exception if run outside of a SageMaker notebook instance, as it gets metadata from the notebook instance. If running outside, you must provide an IAM role with proper access stated above in Permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit, Deploy, Predict\n",
    "Now that the rest of our estimator is configured, we can call fit() with the path to our local CIFAR10 dataset prefixed with file://. This invokes our TensorFlow container with 'train' and passes in our hyperparameters and other metadata as json files in /opt/ml/input/config within the container.\n",
    "<br/><br/>\n",
    "After our training has succeeded, our training algorithm outputs our trained model within the /opt/ml/model directory, which is used to handle predictions.\n",
    "<br/><br/>\n",
    "We can then call deploy() with an instance_count and instance_type, which is 1 and local. This invokes our Tensorflow container with 'serve', which setups our container to handle prediction requests through TensorFlow Serving. What is returned is a predictor, which is used to make inferences against our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets set up our SageMaker notebook instance for local mode.\n",
    "!/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# So they do necessarily need a whole new file. Can set up right here like this. \n",
    "hyperparameters = {'train-steps': 100}\n",
    "## DON'T REALLY NEED TO CODE THIS BECAUSE YOU CAN CREATE JOBS IN SAGEMAKER IF YOU WANT\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "# Setting up the image that you already dockerized\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name='sagemaker-tf-cifar10-example:latest',\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Grabbing the data\n",
    "estimator.fit('file:///tmp/cifar-10-data')\n",
    "\n",
    "# Used data for train, and use instance count and type for deployment of the predictor\n",
    "predictor = estimator.deploy(1, instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions using Python SDK\n",
    "To make predictions, we use an image that is converted using OpenCV into a json format to send as an inference request. We need to install OpenCV to deserialize the image that is used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install opencv-python\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "# Serialization is the process of translating data structures or object state into a format that can be \n",
    "# stored (for example, in a file or memory buffer) or transmitted and reconstructed later.\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "# Test data\n",
    "image = cv2.imread(\"data/cat.png\", 1)\n",
    "\n",
    "# resize, as our model is expecting images in 32x32.\n",
    "image = cv2.resize(image, (32, 32))\n",
    "\n",
    "# put into right format for analysis\n",
    "data = {'instances': numpy.asarray(image).astype(float).tolist()}\n",
    "\n",
    "# The request and response format is JSON for TensorFlow Serving.\n",
    "# For more information: https://www.tensorflow.org/serving/api_rest#predict_api\n",
    "# Note we made this predictor previosly\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "# For more information on the predictor class.\n",
    "# https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py\n",
    "predictor.predict(data)\n",
    "\n",
    "# Metrics that your algorithm sends to Amazon CloudWatch during training jobs. (Keep in mind that we can use this \n",
    "# for the comparison we need from SEC\n",
    "\n",
    "# Once it's containerized, you can also use the console or the Python SDK\n",
    "# For Hyperparameter specification, specify the hyperparameters that your algorithm supports by editing the \n",
    "# JSON object. For each hyperparameter that your algorithm supports, construct a JSON block. \n",
    "    # These are just the parameters you would usually specify "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a Model Package Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model artifacts can either be packaged in the same Docker container as the inference code or stored in Amazon S3.\n",
    "# Validation profiles, which are batch transform jobs that Amazon SageMaker runs to test your model package's inference code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use Amazon SageMaker Containers to Run a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample uses the kernel: conda_tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the editor of your choice, create the following Dockerfile text file locally and save it with the file name \n",
    "# \"Dockerfile\" without an extension. \n",
    "\n",
    "# Note the freeze thing Jeremy was talking about might make this easier because I won't \n",
    "# have to load in the package individually like this\n",
    "\n",
    "# Add and install Python modules\n",
    "ADD requirements.txt \"~/OneDrive - Accenture Federal Services/Documents/SEC/requirements.txt\"\n",
    "RUN cd /src; pip install -r requirements.txt\n",
    "\n",
    "RUN pip install sagemaker-containers # Amazon SageMaker Containers contains the common functionality \n",
    "                                     #necessary to create a container compatible with Amazon SageMaker.\n",
    "\n",
    "# Copies the training code inside the container\n",
    "COPY train.py /opt/ml/code/train.py # copies the script to the location inside the container that is \n",
    "                                    # expected by Amazon SageMaker. The script must be located in this folder.\n",
    "\n",
    "# Defines train.py as script entrypoint\n",
    "ENV SAGEMAKER_PROGRAM train.py # defines train.py as the name of the entrypoint script that is located in \n",
    "                               #the /opt/ml/code folder for the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the actual script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.models import Phrases \n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.test.utils import datapath\n",
    "from pprint import pprint\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gc\n",
    "\n",
    "# Define Functions \n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def build_text(doc_set):\n",
    "    for line in doc_set:\n",
    "        yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "        \n",
    "def process_texts(texts):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "    \n",
    "    1. Stopword Removal.\n",
    "    2. Collocation detection.\n",
    "    3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    texts: Tokenized texts.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    texts: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [bigram[line] for line in texts]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    texts = [[lemmatizer.lemmatize(word) for word in line] for line in texts]\n",
    "    return texts\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, texts, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    numtopics = []\n",
    "    for num_topics in range(1, limit):\n",
    "        numtopics.append(num_topics)\n",
    "        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, per_word_topics = True, random_state=100)\n",
    "        lm_list.append(lm)\n",
    "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "    \n",
    "    return lm_list, c_v, numtopics\n",
    "\n",
    "# The actual function \n",
    "# Convert All to Pandas Dataframe \n",
    "cols = ['year', 'compname', \"text\"]\n",
    "comp10ks = pd.DataFrame(data, columns=cols)\n",
    "comp10ks.Year = pd.to_datetime(comp10ks.year).dt.to_period('y')\n",
    "# Preprocesing\n",
    "comp10ks['text'] = comp10ks['text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "comp10ks['text'] = comp10ks.text.str.replace(r'<.*?>', '', regex= True)\n",
    "comp10ks['text'] = comp10ks.text.str.replace(r'&#\\d+;', '', regex= True)\n",
    "# Convert the titles to lowercase\n",
    "comp10ks['text'] = comp10ks['text'].map(lambda x: x.lower())\n",
    "comp10ks['text'] = comp10ks.text.str.replace('\\n', '')\n",
    "comp10ks['text'] = comp10ks.text.str.replace('\\t', '')\n",
    "comp10ks['text'] = comp10ks.text.str.replace('&nbsp;','')\n",
    "# Pull out fields\n",
    "comp10ks['bus_address'] = comp10ks.text.str.extract('business address: street 1:(.*?)mail address')\n",
    "comp10ks['mail_address'] = comp10ks.text.str.extract('mail address: street 1:(.*?)10-k')\n",
    "# Scrape out risk text using url\n",
    "comp10ks['riskfac'] = comp10ks.text.str.extract('item 1a(.*?)item 2')\n",
    "comp10ks['mandisc'] = comp10ks.text.str.extract('item 7(.*?)item 8')\n",
    "comp10ks['fullrisktext'] = comp10ks['riskfac'] + comp10ks['mandisc']\n",
    "# Tokenize and Preprocess\n",
    "doc_set = comp10ks['fullrisktext']\n",
    "token_text = list(build_text(doc_set.dropna()))\n",
    "# Create Bigrams \n",
    "bigram = gensim.models.Phrases(token_text)\n",
    "# Process Texts\n",
    "texts_clean = process_texts(token_text)\n",
    "# Create Dictionary and Corpus \n",
    "dictionary = Dictionary(texts_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_clean]\n",
    "# Run the LDA Model with the optimal number of topics\n",
    "lmlist, c_v, numtopics = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts_clean, limit=10)\n",
    "opt = pd.DataFrame(numtopics, c_v)\n",
    "opt = opt.reset_index()\n",
    "s = opt[opt['index'] == opt['index'].max()].iloc[0,1]\n",
    "lmtopics = lmlist[s].show_topics(formatted=False)\n",
    "LdaModel(corpus=corpus, num_topics=s, id2word=dictionary, per_word_topics = True, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENDED UP USING THE SCRIPT ABOVE TO FULLY DOCKERIZE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using AWS LDA with Business Text Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input/Output Interface for the LDA Algorithm\n",
    "<br/><br/>\n",
    "- LDA expects data to be provided on the train channel, and optionally supports a test channel, which is scored by the final model. LDA supports both recordIO-wrapped-protobuf (dense and sparse) and CSV file formats\n",
    "- For inference, text/csv, application/json, and application/x-recordio-protobuf content types are supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os, re\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# some helpful utility functions are defined in the Python module\n",
    "# \"generate_example_data\" located in the same directory as this\n",
    "# notebook\n",
    "from generate_example_data import generate_griffiths_data, plot_lda, match_estimated_topics\n",
    "\n",
    "# accessing the SageMaker Python SDK\n",
    "import sagemaker\n",
    "from sagemaker.amazon.common import numpy_to_record_serializer\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup AWS Credentials\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/DEMO-lda-introduction'\n",
    "\n",
    "print('Training input/output will be stored in {}/{}'.format(bucket, prefix))\n",
    "print('\\nIAM Role: {}'.format(role))\n",
    "\n",
    "# Setting it up - This is where we'd pass the list of files in S3, right? \n",
    "\n",
    "# separate the generated data into training and tests subsets\n",
    "num_documents_training = int(0.9*num_documents)\n",
    "num_documents_test = num_documents - num_documents_training\n",
    "\n",
    "documents_training = documents[:num_documents_training]\n",
    "documents_test = documents[num_documents_training:]\n",
    "\n",
    "topic_mixtures_training = topic_mixtures[:num_documents_training]\n",
    "topic_mixtures_test = topic_mixtures[num_documents_training:]\n",
    "\n",
    "print('documents_training.shape = {}'.format(documents_training.shape))\n",
    "print('documents_test.shape = {}'.format(documents_test.shape))\n",
    "\n",
    "# convert documents_training to Protobuf RecordIO format\n",
    "recordio_protobuf_serializer = numpy_to_record_serializer()\n",
    "fbuffer = recordio_protobuf_serializer(documents_training)\n",
    "\n",
    "# upload to S3 in bucket/prefix/train\n",
    "# We obv don't need to upload from here since it is already up - but can we convert via the console \n",
    "# (Something Sam or David might be able to do)\n",
    "\n",
    "# Do we need to pay attention to the filepath (train) here? \n",
    "fname = 'lda.data'\n",
    "s3_object = os.path.join(prefix, 'train', fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(fbuffer)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, s3_object)\n",
    "print('Uploaded data to S3: {}'.format(s3_train_data))\n",
    "\n",
    "sec-edw-dev-landing-zone/10k-forms-desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by SageMaker LDA configuring the model and defining the computational environment in which training will take place.\n",
    "<br/><br/>\n",
    "First, we specify a Docker container containing the SageMaker LDA algorithm. For your convenience, a region-specific container is automatically chosen for you to minimize cross-region data communication. Information about the locations of each SageMaker algorithm is available in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to how we would bring in our own custom\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "# select the algorithm container based on this notebook's current location\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = get_image_uri(region_name, 'lda')\n",
    "\n",
    "print('Using SageMaker LDA container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particular to a SageMaker LDA training job are the following hyperparameters:\n",
    "<br/><br/>\n",
    "- num_topics - The number of topics or categories in the LDA model.\n",
    "    - Usually, this is not known a priori.\n",
    "    -In this example, howevever, we know that the data is generated by five topics.\n",
    "- feature_dim - The size of the \"vocabulary\", in LDA parlance.\n",
    "    - In this example, this is equal 25.\n",
    "- mini_batch_size - The number of input training documents.\n",
    "- alpha0 - (optional) a measurement of how \"mixed\" are the topic-mixtures.\n",
    "    - When alpha0 is small the data tends to be represented by one or few topics.\n",
    "    - When alpha0 is large the data tends to be an even combination of several or many topics.\n",
    "    - The default value is alpha0 = 1.0.\n",
    "<br/><br/>\n",
    "In addition to these LDA model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role. Note that,\n",
    "<br/><br/>\n",
    "- Recommended instance type: ml.c4\n",
    "- Current limitations:\n",
    "    - SageMaker LDA training can only run on a single instance.(Could be major drawback)\n",
    "    - SageMaker LDA does not take advantage of GPU hardware.\n",
    "    - (The Amazon AI Algorithms team is working hard to provide these capabilities in a future release!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might be able to start here, but we might have to convert everything to recordIO-wrapped-protobuf first\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "# set algorithm-specific hyperparameters\n",
    "lda.set_hyperparameters(\n",
    "    num_topics=num_topics,\n",
    "    feature_dim=vocabulary_size,\n",
    "    mini_batch_size=num_documents_training,\n",
    "    alpha0=1.0,\n",
    ")\n",
    "\n",
    "# run the training job on input data stored in S3\n",
    "lda.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
