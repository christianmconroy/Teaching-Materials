---
title: "NLP Tutorial 01: Preprocessing"
author: "Ben Ortiz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
description: "The purpose of this book is to provide R code supplementing the NLP tutorial covering *preprocessing*."
output:
  bookdown::gitbook: default
---

# Intro

*The purpose of this book is to provide R code supplementing the NLP tutorial covering *preprocessing*

## Prerequisites 

The following packages are necessary to follow the material in this book.

```{r, eval=F}
install.packages(c(
  'dplyr', #utilities for data cleaning
  'fastNaiveBayes', #NB implementation
  'gmodels', #display results of model
  'magrittr', #allow piping
  'Matrix', #create sparse matrices
  'MLmetrics', #get F1 score
  'readr', #read in data
  'stringr', #utilities for string manipulation
  'tidytext', #TIDY version of text mining
  'tm' #text mining
))
```

## Package usage

We will explicitly refer to packages when we use a function. This takes the form of `package::function()`. The reason we do this is to be clear where functions are coming from.

The only exception to this is **magrittr**'s `%>%` (pipe) function. This is to allow some of our code to be more legible  

```{r}
library(magrittr)
```

If you haven't used `%>% before, then think of it as using the output of a previous function as the input of the next function.

That is rather than nesting functions:

```{r}
sqrt(sum(log(c(1, 10, 100))))
```

We can focus on functions in the order they are called

```{r}
c(1,10,100) %>%
  log() %>%
  sum() %>%
  sqrt()
```

<!--chapter:end:index.Rmd-->

# Data

## History of data

As per the [Kaggle repo](https://www.kaggle.com/crowdflower/twitter-airline-sentiment), this data is described as *A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as "late flight" or "rude service").*

## Load Data

You can find the data online at [https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv](https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv)

```{r, message=F, warning=F}
data_url <- "https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv"
tweets <- readr::read_csv(data_url)
```

Alternatively, if the data is not available, we made a trimmed down version of the data available in this tutorial folder.    

```{r, eval = F}
tweets <- readr::read_csv('us_airlines_tweets.csv')
```


## Quick Look

```{r}
dim(tweets)
```

```{r}
names(tweets)
```

```{r, eval = F}
table(tweets$airline_sentiment, tweets$airline)
```

```{r, echo = F}
knitr::kable(table(tweets$airline_sentiment, tweets$airline))
```

```{r, eval = F}
set.seed(4321) #consistent random numbers
sample(tweets$text, 5)
```

```{r, echo = F}
set.seed(4321) #consistent random numbers
knitr::kable(sample(tweets$text, 5))
```


<!--chapter:end:01-Data.Rmd-->

# tm: Traditional Text Preprocessing

## Columns of importance

```{r}
tweets_clean <- dplyr::select(tweets, text, airline_sentiment)
```

## Create a corpus

```{r}
tweet_corpus <- tweets_clean$text %>%
  tm::VectorSource() %>%  
  tm::Corpus() 
```

```{r}
tweet_corpus
```

A **Corpus** is simply a vector of documents that includes additional metadata.

```{r}
tm::inspect(tweet_corpus[1:6])
```

## Clean Corpus

There are a number of steps you can use to clean a corpus for text mining purposes. **tm** provides some helper functions to accomplish this.

**tm::tm_map** iterates over all the documents in a corpus and performs a function. It accepts two parameters: a corpus and a function to iterate with. The returned value is another corpus. This is a perfect candidate for piping.

```{r, message=F,warning=F}
corpus_clean <- tweet_corpus %>%
  tm::tm_map(tolower) %>%
  tm::tm_map(tm::removeNumbers) %>%
  tm::tm_map(tm::removeWords, tm::stopwords()) %>%
  tm::tm_map(tm::removePunctuation) %>%
  tm::tm_map(tm::stripWhitespace)
```

```{r}
tm::inspect(corpus_clean[1:6])
```

You can even create your own function if you think it will help. Let's do one that removes the names of the airlines.

```{r}
airlines <- c("virginamerica", 
              "jetblue", 
              "americanair", 
              "united", 
              "usairways", 
              "soutwestair") %>%
  paste0(collapse = '|') #creates a regex to remove any airline from text

rmAirlines <- function(txt){
  stringr::str_remove_all(txt, airlines) %>%
    tm::stripWhitespace()
}

```

```{r, warning=F}
corpus_clean <- corpus_clean %>%
  tm::tm_map(rmAirlines)

tm::inspect(corpus_clean[1:6])
```

## Create **Document-Term Matrix**

Before we perform any models, we need to create a **Document Term Matrix**

```{r}
tm_dtm <- tm::DocumentTermMatrix(corpus_clean)
```

```{r}
tm_dtm
```
You may notice that there are `r ncol(tm_dtm)` terms in the DTM, they might not all be important. We might want to remove terms that only show up in very few documents.

```{r}
tm_dict <- tm::findFreqTerms(tm_dtm, 5)
```

**tm_dict** contains terms that appear in at least 5 documents. Let's create anew DTM that only includes these terms.

```{r}
tm_dtm_trimmed <- tm::DocumentTermMatrix(corpus_clean,list(dictionary = tm_dict))
 
tm_dtm_trimmed
```

Now we have a smaller document term matrix with only `r ncol(tm_dtm_trimmed)` with the same number of rows [documents]. This step is less important for performance as it is for compute power.

And, to aid us in the modeling portion, it will be important to convert the DTM into a sparse matrix. Sparse matrices are much more space efficient and can be processed quicker.

```{r}
tm_sparse <- tm_dtm_trimmed %>% 
  as.matrix() %>%
  Matrix::Matrix(sparse = T)
```

```{r}
tm_sparse[1:6, 1:6]
```

Before we move onto the next section, it's important that we ensure that the sparse matrix is in line with our original data. We can do this by comparing the number of rows in the matrix with the number of sentiments available to us in the original data.

```{r}
nrow(tm_sparse) == length(tweets_clean$airline_sentiment)
```

Since they match we can store the sentiments for training purposes in a vector.

```{r}
tm_y <- tweets_clean$airline_sentiment
```

<!--chapter:end:02-tm.Rmd-->

# tidytext: A tidy way to text mine

## Columns of importance

Numbering the rows allows us to reference them later when we create a document term matrix.

```{r}
tweets_clean <- tweets %>%
  dplyr::select(text, airline_sentiment) %>%
  dplyr::mutate(id = 1:nrow(.))
```

```{r, echo = F}
knitr::kable(head(tweets_clean))
```

## Provide structure to the data

The tidy mantra is:

- Each variable is a column
- Each observation is a row
- Each type of observational unit is a table. 

This translates to text format as a **table with one token per row**

```{r}
tidy_tweets <- tweets_clean %>%
  tidytext::unnest_tokens(word, text ) %>% #tokenizes text
                                           #automatically lowercase and symbols
  dplyr::count(id, word, sort = T) #get counts of words in each tweet
```

```{r, echo = F}
knitr::kable(head(tidy_tweets))
```

Let's take a look at words that are common to the corpus

```{r, eval = F}
tidy_tweets %>%
  dplyr::count(word, sort = T) %>%
  head()
```

```{r, echo = F}
tidy_tweets %>%
  dplyr::count(word, sort = T) %>%
  head() %>%
  knitr::kable()
```

## Clean the Data

There's a lot of cleaning we can do. We can remove *stop words*, we can remove words that are too common or not common enough.

```{r}
uncommon <- tidy_tweets %>%
  dplyr::count(word) %>%
  dplyr::filter(n <= 5)

common_names <- c('united', 
                  'usairways', 
                  'americanair', 
                  'southwestair', 
                  'jetblue',
                  'virginamerica')
```

```{r, message = F}
tidy_tweets <- tidy_tweets %>%
  dplyr::anti_join(tidytext::stop_words) %>%
  dplyr::anti_join(uncommon) %>%
  dplyr::filter(!word %in% common_names) %>%
  dplyr::arrange(desc(n))
  
```


## Create a **Document Term Matrix**

```{r}
tidy_dtm <- tidytext::cast_dtm(tidy_tweets, id, word, n)

# id is document, word is term, and n are just the counts
```

```{r}
tidy_dtm
```

For computational reasons, it's good to convert the DTM into a sparse matrix.

```{r}
tidy_sparse <- tidy_dtm %>%
  as.matrix() %>%
  Matrix::Matrix(sparse = T)

# We want to do this for computational reasons
```

```{r}
tidy_sparse[1:6, 1:6]
```

Now, let's confirm that the rows of the matrix are associated with the number of sentiments we have.

```{r}
nrow(tidy_sparse) == length(tweets_clean$airline_sentiment)
```

This might be shocking, but this is a natural side effect of what happens during the cleaning process. If a document only contained stop words and other words that we filtered out, then it would be removed from the tidy data frame. We can use the **rownames** of the matrix, which are associated with the **id**s of the data frame to pull this information.


```{r}
tidy_y <- tidy_sparse %>%
  rownames() %>%
  as.numeric() %>%
  {tweets_clean$airline_sentiment[.]}

# Trying to figure out sentiment associated with each of the texts
    # Get rownames of the tidysparse matrix 
    # Convert the names into numerics 
    # Use numbers to pull out y values associated with the rownames
```

```{r}
nrow(tidy_sparse) == length(tidy_y)
```

<!--chapter:end:03-tidytext.Rmd-->

# fastNaiveBayes: a Naive Package

## Training & Test Sets 

Regardless of which route you took [tm or tidytext] you should have a sparse matrix available to model with. We will need to separate these matrices into **training** and **testing** sets.

Let's first determine how large our training set will be. 

```{r}
perc_train <- .75
```

### tm sets

Let us get a random sample of rows to use for training

```{r}
set.seed(4321) 
tm_train <- sample(1:nrow(tm_sparse), perc_train * nrow(tm_sparse))
```

Now let's grab the associated rows in the sparse matrix and the associated sentiments.

```{r}
# Create train sparse matrix (x and y)
tm_sparse_train <- tm_sparse[tm_train, ]
tm_y_train <- tm_y[tm_train]

tm_sparse_test <- tm_sparse[-tm_train, ]
tm_y_test <- tm_y[-tm_train]
```

### tidy sets

Let us get a random sample of rows to use for training

```{r}
set.seed(4321) 
tidy_train <- sample(col, perc_train * nrow(tidy_sparse))
```

Now let's grab the associated rows in the sparse matrix and the associated sentiments.

```{r}
tidy_sparse_train <- tidy_sparse[tidy_train, ]
tidy_y_train <- tidy_y[tidy_train]

tidy_sparse_test <- tidy_sparse[-tidy_train, ]
tidy_y_test <- tidy_y[-tidy_train]
```


## NaiveBayes

### tm

```{r}
tm_nb <- fastNaiveBayes::fastNaiveBayes(tm_sparse_train, tm_y_train)

tm_pred <- predict(tm_nb, tm_sparse_test)

gmodels::CrossTable(tm_pred, tm_y_test,
                    prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
                    dnn = c('predicted', 'actual'))

# Create a confusion matrix with predicted and test
# dnn is used just to name it
# Get accuracy on the diagonals
```

Let's get he F1 score.

```{r}
tm_f1 <- MLmetrics::F1_Score(tm_y_test, tm_pred)
tm_f1

# Goal is to get the best F1 score possible
# Don't need to build your own function!
```

### tidy

```{r}
tidy_nb <- fastNaiveBayes::fastNaiveBayes(tidy_sparse_train, tidy_y_train)

tidy_pred <- predict(tidy_nb, tidy_sparse_test)

gmodels::CrossTable(tidy_pred, tidy_y_test,
                    prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
                    dnn = c('predicted', 'actual'))
```

Let's get he F1 score.

```{r}
tidy_f1 <- MLmetrics::F1_Score(tidy_y_test, tidy_pred )
tidy_f1
```

<!--chapter:end:04-fastNaiveBayes.Rmd-->

